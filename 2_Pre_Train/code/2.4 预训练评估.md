> pretrain 的评估相对来说是 llm 全链路评估工作中最简单的环节，因为它并**不需要考察模型的指令 follow 能力、安全能力、幻觉现象**等，只需要看**模型整体的知识掌握程度即可**

# **2.4.1 PPL困惑度**

> 准备一些百科、逻辑、code 等数据，每天观察模型在这些测试集合上的 loss 表现，偶尔浮动是正常的，但整体趋势肯定是持续下降，最后趋于稳定的
>
> 特别地、ppl 只能是自己的模型和自己比，前面说过，由于 tokenizer 的压缩率不同，不同模型的 loss 没有明确可比性，全是字没有词的 tokenizer，loss 一定是最低的。不过另一方面，不管你的 tokenizer 压缩率多少，在通用知识测试集上的 loss 也是应该能降低到 2 以下的，否则就说明有点训练不充分

> 困惑度在1.5.5章[ 1.5 Positional Encoding 位置编码](https://kcnd4kn8i6ap.feishu.cn/wiki/UWrpwhKWhigc5DkOqRGcBRLInzb?fromScene=spaceOverview#share-EsQsdiXg1ojeHmxAGNucDUhrn5e)中讲过，这里再复述一遍
>
> **困惑度**（Perplexity）是自然语言处理中常用的一个评价指标，用于衡量语言模型的好坏。语言模型Model在测试集数据 $$T=\left\{w_{1}, w_{2}, \ldots, w_{N}\right\}$$上的困惑度计算，困惑度越低，说明模型对下一个单词的预测越准确，模型性能越好：
>
> &#x20;                    $$    \text{Perplexity}(Model)=\exp\left(-\frac{1}{N}\sum_{i = 1}^{N}\log P\left(w_{i}\mid w_{1}, \ldots, w_{i-1}\right)\right)$$

# **2.4.2 Benchmark**

> 在一些知识性的开源benchmark评测
>
> **推荐一个评估平台：`OpenCompass`**&#x68;ttps://github.com/open-compass/opencompass

| **开源Benchmark** | 用途                                  | 原文链接                                                                                                                                  | 数据地址                                                              |
| --------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| MMLU            | 知识与语言理解评估 LLM 在广泛主题领域的理解和推理能力       | [《Measuring Massive Multitask Language Understanding》](https://arxiv.org/abs/2009.03300)                                              | https://github.com/hendrycks/test                                 |
| GLUE            | 知识与语言理解对不同语境下的语言理解能力进行全面评估          | [《GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding》](https://arxiv.org/abs/1804.07461)           | https://huggingface.co/datasets/nyu-mll/glue                      |
| MultiNLI        | 知识与语言理解评估 LLM 根据陈述推理正确类别的能力         | [《A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference》](https://arxiv.org/abs/1704.05426)                  | https://huggingface.co/datasets/multi\_nli                        |
| SuperGLUE       | 知识与语言理解评估语言理解和推理的更深层次               | [SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537)                | https://huggingface.co/datasets/super\_glue                       |
| GSM8K           | 推理能力中小学题，测试 LLM 解决多步数学问题的能力         | [《Training Verifiers to Solve Math Word Problems》](https://arxiv.org/abs/2110.14168)                                                  | https://huggingface.co/datasets/gsm8k                             |
| CRASS           | 推理能力评估模型根据给定数据理解和推理备选场景的能力          | [《CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models》](https://arxiv.org/abs/2112.11941) | https://github.com/apergo-ai/CRASS-data-set/tree/main             |
|  MT-bench       | 多轮开放式对话测试模型在多个回合中进行连贯和上下文相关对话的能力    | [《Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena》](https://arxiv.org/abs/2306.05685)                                          | https://huggingface.co/datasets/lmsys/mt\_bench\_human\_judgments |
| QuAC            | 多轮开放式对话在对话中用上下文相关的、有时无法回答的问题来挑战 LLM | [《QuAC : Question Answering in Context》](https://arxiv.org/abs/1808.07036)                                                            | https://quac.ai/                                                  |
| HHH             | 评估道德评估模型在交互场景中的道德反应                 | [《A General Language Assistant as a Laboratory for Alignment》](https://arxiv.org/abs/2112.00861)                                      | https://github.com/anthropics/hh-rlhf                             |
| TruthfulQA      | 幻觉测试模型提供准确无偏信息的能力                   | [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958v2)                                         | https://github.com/sylinrl/TruthfulQA                             |

> 如果 pretrain 阶段全程是自己在负责，那么 benchmark 还是有一定可信程度的。但无论是继承同事的 checkpoint，还是下载开源模型的 checkpoint，一律视为它们刷过榜。除了模型完全给自己用的公司，只要是有宣发需求的 pretrain 团队就一定会刷榜，否则根本和别的模型比不了。这种情况下，benchmark 考察的就是谁遗忘的少，而不是谁具有的知识多，训的越多模型得分就越低
>
> 即使排除了刷榜问题，现在最主流的 benchmark，形式也都实在是有点单一了。全是选择题，而且是没有 cot 环节的选择题，衡量模型的能力肯定是有偏颇的。cot 就是模型的草稿纸，就算是我们人，拿到一个选择题，不去演算一下就直接说出 A、B、C、D 该选哪一个，也是很匪夷所思的
>
> 可是话说回来，benchmark 毕竟是现成的具有高密度知识的 question-answer 的高质量语料，不使用着实可惜。这里我的建议是改造 benchmark，无论是评估自己的模型还是评估别人的刷过榜的模型，尽量用生成式的方法去使用 benchmark，而不是看 A、B、C、D 哪个概率高
>
> 改造可以是任意形式的，举几个例子：
>
> * Question + Answer\_A，Question + Answer\_B，Question + Answer\_C，Question + Answer\_D。请结合上述文本，回答 Question
>
> * 把 Question 的正确答案丢弃，假设 B 选项是正确的答案，那就把它改成“B. 其他答案全错”，看模型还能不能把 B 选出来
>
> * 把原始 benchmark 的 A / B / C / D 改成 一 / 二 / 三 / 四
>
> * 把多选题改造成单选题
>
> * Question + A / B / C / D，先把这个知识让模型不知道答案的情况下训一遍，然后让模型直接说出 Question 的正确答案
>
> 如果担心模型不 follow 格式，那就用 few\_shot 的方式让模型去抄袭格式，通常都能 follow 住；如果担心模型停止不了，就限制 max\_new\_token，或者是使用 StoppingCriteria，让模型见到换行符就停止

# **2.4.3 大海捞针**

> 通过在长文本中随机插入关键信息，形成大型语言模型(LLM)的Prompt。该测试旨在检测大型模型是否能从长文本中提取出这些关键信息，从而评估模型处理长文本信息提取的能力，这可以反映LLM对长文本的理解基础能力。
>
> * **单一信息检索任务**：评估LLM在长文本中提取单一关键信息的能力，测试其对广泛叙述中特定细节的精确回忆能力。这对应于**原始的大海捞针测试**任务设定。
>
> * **多信息检索任务**：探讨LLM从长文本中检索多个相关信息的能力，模拟实际场景中对综合文档的复杂查询。
>
> * **多信息推理任务**：通过提取并利用长文本中的多个关键信息来评估LLM的长文本能力，要求模型对各关键信息片段有综合理解。
>
> * **祖先追溯挑战**：通过设计“亲属关系针”，测试LLM处理真实长文本中多层逻辑挑战的能力。在ATC任务中，通过一系列逻辑推理问题，检验模型对长文本中每个细节的记忆和分析能力，在此任务中，我们去掉了无关文本(Haystack)的设定，而是将所有文本设计为关键信息，LLM必须综合运用长文本中的所有内容和推理才能准确回答问题

# **2.4.4 概率探针**

> 从概率的角度来监控模型的知识能力有没有遗忘或者提升，适用于要观察模型的某一项具体能力。思路也非常简单，就观察某个 token 的概率是否增加，某个句子的概率是否增加，唯一麻烦的地方是探针测试集往往需要训练者一条一条亲自去构造，而不能批量生成，比如
>
> 1. Prob('北京'**｜**'中国的首都是')，就看一下这个概率值随着 pretrain 推进是否持续在增大
>
> 2. PPL('台湾属于中国')，这个句子的 ppl 是否持续在增大；PPL('台湾不属于中国')，这个句子的 ppl 是否持续在减小（这里意在问题监控，第一个句子PPL增大以及第二个句子PPL下降都意味着有问题，需要介入处理，绝无台独含义，希望大家不要过度联想。示例不会删除）
>
> 3. 对比探针，PPL('尊重同性恋') > PPL('反对同性恋') 是否成立
>
> 4. 指令 follow 能力，Prob( '{ ' **|** '以 json输出')
>
> 和 benchmark 改造类似，概率探针是可以根据自己的喜好任意构造的。我们重点观察的也是指标的变化趋势，而不是指标的绝对大小

# **2.4.5 续写能力**

> 准备好Prompt和ground truth文本，用Base模型续写Prompt，利用**Rouge-L、BLEU和Bertscore**等指标做相似度计算，作为评估续写能力的指标
