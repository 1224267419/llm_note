RAG通过在上下文和知识库中搜索用户prompt最相关的内容 , 优化用户的prompt,从而指导模型生成更准确的输出 , **底层原理还是 In-Context Learning**

**LLM本身的局限性：**

- **知识的局限性：**&#x6A21;型自身的知识完全源于它的训练数据，而现有的主流大模型的训练集基本都是构建于网络公开的数据，对于一些**实时性的、非公开的或离线的数据**是无法获取到的
- **幻觉问题：**&#x6240;有的AI模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。而这种**幻觉问题的区分是比较困难**的，因为它要求**使用者自身具备相应领域的知识**
- **数据安全性：完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍**



**RAG的特点：**

- **依赖LLM来强化信息检索和输出：**&#x52;AG需要结合LLM来进行信息的检索和生成，但如果单独使用RAG，它的能力会受到限制。也就是说，RAG需要依赖强大的语言模型支持，才能更有效地生成和提供信息
- **能与外部数据有效集成：**&#x52;AG能够很好地接入和利用外部数据库的数据资源。这一特性弥补了通用大模型在某些垂直或专业领域的知识不足或者数据时效问题，比如行业特定的术语和深度知识，能提供更精确的答案
- **数据隐私和安全保障：**&#x901A;常RAG所连接的私有数据库不会参与到大模型的数据集中训练。因此，RAG既能提升模型的输出表现，又能有效地保护这些私有数据的隐私性和安全性，不会将敏感信息暴露给大模型的训练过程
- **表现效果因多方面因素而异：**&#x52;AG的效果受多个因素影响，比如所使用的语言模型的性能、输入数据的质量、算法以及检索系统的设计等。这意味着不同的RAG系统之间效果差异较大，不能一概而论

## **RAG整体思路**

可以分为五个基本流程：**知识文档的准备、Embedding模型、向量数据库、查询检索和生成回答**

![](3_RAG.assets/image.png)

- **知识文档的准备：**在构建一个高效的RAG系统时，首要步骤是准备知识文档。现实场景中，我们面对的知识源可能包括多种格式，如**Word文档、TXT文件、CSV数据表、Excel表格，甚至是PDF文件、图片和视频**等。因此，第一步需要使用**专门的文档加载器（例如PDF提取器）或多模态模型（如OCR技术）**，将这些丰富的知识源转换为**大语言模型可理解的纯文本数据**。例如，处理PDF文件时，可以利用PDF提取器抽取文本内容；对于图片和视频，OCR技术能够识别并转换其中的文字信息。此外，鉴于文档可能存在过长的问题，我们还需执行一项关键步骤：**文档切片。我们需要将长篇文档分割成多个文本块，以便更高效地处理和检索信息**。这不仅有助于减轻模型的负担，还能提高信息检索的准确性
- **Embedding模型：核心任务是将文本转换为向量形式**，的日常语言中充满歧义和对表达词意无用的助词，而**向量表示则更加密集、精确，能够捕捉到句子的上下文关系和核心含义，**来识别语义上相似的句子。Word2Vec，BERT，GPT系列，BGE系列等等Embedding模型
- **向量数据库：**专门设计用于存储和检索向量数据的数据库系统。在RAG系统中，通过嵌入模型生成的所有向量都会被存储在这样的数据库中。这种数据库优化了处理和存储大规模向量数据的效率，使得在面对海量知识向量时，我们能够迅速检索出与用户查询最相关的信息
- **查询检索：**用户的问题会被输入到嵌入模型中进行向量化处理。然后，系统会在向量数据库中搜索与该问题向量语义上相似的知识文本或历史对话记录并返回
- **生成回答：**将用户提问和上一步中检索到的信息结合，构建出一个提示模版，输入到大语言模型中，静待模型输出答案即可

## **RAG分类**

**三个阶段Naive RAG，Advanced RAG、Modular RAG**

**综述论文：[Retrieval-Augmented Generation for Large Language Models: A Survey](https://www.semanticscholar.org/paper/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5)**

![](3_RAG.assets/image-1.png)

- **Naive RAG：**&#x7ECF;典的RAG，主要涉及“检索-阅读”过程。主要包括包括三个基本步骤：
  - **索引：**&#x5C06;文档库分割成较短的 Chunk，并通过编码器构建向量索引
  - **检索：**&#x6839;据问题和 chunks 的相似度检索相关文档片段
  - **生成：** 以检索到的上下文为条件，生成问题的回答
- **Advanced RAG**：Naive RAG 在检索质量、响应生成质量以及增强过程中存在多个挑战。Advanced RAG在**数据索引、检索前和检索后都进行了额外处理**。通过更精细的数据清洗、设计文档结构和添加元数据等方法提升文本的一致性、准确性和检索效率。在**检索前**阶段则可以使用**问题的重写**、**路由和扩充等方式对齐问题和文档块之间的语义差异**。在**检索后**阶段则可以通过将检索出来的文档库进行**重排序**避免 “Lost in the Middle ” 现象的发生，或是通过**上下文筛选与压缩**的方式缩短窗口长度
- **Modular RAG：**&#x5728;结构上更加自由的和灵活，引入了更多的**具体功能模块**，例如**查询搜索引擎、融合多个回答**。技术上将**检索与微调、强化学习等技术融合**。流程上也对 RAG 模块之间进行设计和编排，出现了多种的 RAG 模式

RAG评估和RAG优化详见code中的文档,这里先不赘述,

### **开源RAG评估框架**

- **[Ragas](https://github.com/nayeon7lee/FactualityPrompt)：主要评估忠实性、答案相关性和上下文相关性。**
  - **忠实性**。答案应基于给定的上下文。这个可以确保检索到的上下文可以作为生成答案的理由
  - **答案相关性**。生成的答案应针对所提供的实际问题
  - **上下文相关性**。检索的上下文应重点突出，尽可能少地包含无关信息，因为向LLM处理长篇上下文信息的成本很高，当上下文段落过长时，LLMs在利用上下文方面的效率往往较低，尤其是对于上下文段落中间提供的信息
- **LangSmith：**&#x5141;许调试、测试、评估和监控基于任何 LLM 框架构建的链和智能代理，并无缝集成 LangChain。

## **项目参考**

> 1. Langchain-Chatchat 入坑RAG的第一个项目，教程详细，上手友好，检索部分可优化点多，可自行魔改。
>    1. 可以优化的点：加一个结合历史信息改写当前Query的模块；引入重排模块
> 2. QAnyThing 网易有道的RAG开源项目，拿实验室的文件数据实测效果不错。
> 3. [JARVIS](https://github.com/microsoft/JARVIS) HuggingGPT，single-agent从此大火。
> 4. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) agent项目的必看系列。
> 5. [generative\_agents](https://github.com/joonspk-research/generative_agents) 斯坦福小镇