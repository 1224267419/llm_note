# 强化学习

这部分内容主要参考 [动手学强化学习](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)，可以通过该书详细入门强化学习，这里做一些**简要但是也比较全面**的介绍。**主要是想通过对强化学习有一定了解之后，看大模型强化学习会得心应手很多**

## 1. 强化学习问题、流程以及独特性

### **强化学习解决的问题**

在机器学习领域，有一类重要的任务和人生选择很相似，即序贯决策（sequential decision making）任务。决策和预测任务不同，**决策往往会带来“后果”，因此决策者需要为未来负责**，在未来的时间点做出进一步的决策**。实现序贯决策的机器学习方法就是强化学习（reinforcement learning），**预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变

强化学习（Reinforcement Learning，RL）是一种机器学习方法，用于解决需要在一定环境下**通过与环境交互来学习最优行为策略的问题**。其核心思想是**通过试错（Trial and Error）和奖励机制来指导智能体（Agent）学习如何在不同情境下采取行动，以最大化长期累积奖励**

**应用场景：**控制问题、游戏、资源管理优化、金融风险控制、推荐算法

### 强化学习流程

![](note2.assets/image.png)

智能体在这个过程中学习，它的最终目标是：**找到一个策略，这个策略根据当前观测到的环境状态和奖励反馈，来选择最佳的动作**

**强化学习的独特性**

对于一般的有监督学习任务，**目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数**。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差，用简要的公式可以概括为：

$$最优模型 = \arg\min_{模型} \mathbb{E}_{(特征, 标签)\sim 数据分布}[损失函数(标签, 模型(特征))]$$

相比之下，强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。**策略的价值可以等价转换成奖励函数在策略的占用度量（这里简单理解策略的占用度量是策略的分布即可）上的期望**，即：

$$最优策略 = \arg\max_{策略} \mathbb{E}_{(状态, 动作)\sim 策略的占用度量}[奖励函数(状态, 动作)]$$

- 有监督学习和强化学习的优化目标相似，即都是在**优化某个数据分布下的一个分数值的期望**。
- 二者优化的途径是不同的，**有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变**；**强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变**

综上所述，一般有监督学习和强化学习的范式之间的区别为：

- 一般的**有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小**
- **强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望**

#### **强化学习与有监督学习的其他区别**

强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器**不但可以感知周围的环境信息，还可以通过做决策来直接改变环境**，而不只是给出一些预测信号

> 强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号

##### **数据类型与来源**

- 有监督学习：依赖于**标注好的数据集**，每个数据**样本**都**带有明确的标签**（目标值）
- **强化学习：不依赖预先标注的数据集，而是通过与环境交互产生数据**。智能体在每一步行动后会得到环境的反馈（奖励或惩罚），这个反馈用于指导学习

##### **学习方式**

- **有监督学习：**基于**静态数据集进行训练，学习过程通常是一次性的，即通过一个固定的数据集训练完模型**
- **强化学习：**基于动态数据进行训练，学习过程是持续的，**通过与环境不断交互、试错来更新策略****

##### **反馈机制**

- **有监督学习：**每个训练样本都有明确的标签，模型可以直接计算误差
- **强化学习：**没有明确的标签，模型通过从环境中获得的奖励信号来评估行动的好坏。奖励通常是延迟的，**不是每个行动都能立即得到反馈**



## 2.多臂老虎机[代码](code\BernoulliBandit.py) 

多臂老虎机**不存在状态信息，只有动作和奖励**，算是最简单的“和环境交互中的学习”的一种形式

#### 定义 

我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布 $r$ 中获得一个奖励 。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作  次拉杆后获得尽可能高的累积奖励。由于**奖励的概率分布是未知**的，因此我们需要**在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡**。“采用怎样的操作策略才能使获得的**累积奖励最高**”便是多臂老虎机问题

regret:可以理解为懊悔定义为拉动当前拉杆的动作$a$与最优拉杆的期望奖励差,即$R(a)=Q^*-Q(a)$

累积懊悔cumulative regret:即操作\( T \)次拉杆后累积的懊悔总量，对于一次完整的 $T$ 步决策 $(a_1, a_2, \dots, a_T) $，累积懊悔为 $\sigma_R = \sum_{t=1}^T R(a_t)$ 。MAB问题的目标为最大化累积奖励，**等价于最小化cumulative regret**。



1. 贪心算法 EpsilonGreedy

   随机选择or当前期望最大值,期望修正公式如下:
   假设第 k 根拉杆之前被拉了 $n$ 次，平均奖励估值是 \( $Q_n$ \)，现在又得到新的奖励 \( $r$ \)，那么新的估值更新公式为：
   $Q_{n+1} = Q_n + \frac{1}{n+1}(r - Q_n) $等价于$Q_{n+1} = \frac{1}{n+1} \sum_{i=1}^{n+1} R_i $

   贪心算法更换贪心概率 $\epsilon $可以调整regret的增长斜率,但结果仍为**线性增长**

2. 随时间降低$ \epsilon $的贪心算法

   前期收集的信息太少,使用随机算法有助于收集信息
   后期掌握了足够的信息,使用贪心算法有助于降低regret
   此时$\epsilon$=1/t

3. **上置信界**算法