> **Reinforcement Learning from Human Feedback / Preference-based Reinforcement Learning åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  / åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ **
>
> **æœ¬ç« ä¸»è¦åŒ…å«RLHF-PPOçš„è®²è§£åŠå…¶æ”¹è¿›ç®—æ³•GRPOç­‰ä»¥åŠGRPOçš„æ”¹è¿›ç®—æ³•çš„è¯¦è§£**
>
> å¯ä»¥å‚è€ƒçš„åšå®¢ï¼š çŸ¥ä¹ã€å°çº¢ä¹¦ @çŒ›çŒ¿

# **3.3.1 RLåœ¨NLPåœºæ™¯ä¸‹çš„æ‹“å±•**

## **NLP MDPå»ºæ¨¡**

> å¼ºåŒ–å­¦ä¹ è¦åº”ç”¨åˆ°NLPä»»åŠ¡ä¸­ï¼Œéœ€è¦è¿›è¡Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹MDPå»ºæ¨¡ï¼Œå¯¹åº”çš„**agentã€ç¯å¢ƒã€çŠ¶æ€å’ŒåŠ¨ä½œ**éƒ½éœ€è¦æ˜ç¡®çš„å®šä¹‰ï¼Œ**MDPè¯¦è§3.2.2ç« èŠ‚[ 3.2 RL å¼ºåŒ–å­¦ä¹ åŸºç¡€](https://kcnd4kn8i6ap.feishu.cn/wiki/Cz5YwDjdpiPbIFkiqWecXt0EnNb?fromScene=spaceOverview#share-XZw9drgXBoOnHxxLXOzcQpKYnNc)**

![](images/diagram.png)

> ç»™å®šä¸€ä¸ªpromptï¼Œå¤§æ¨¡å‹ä¼š**åœ¨ $$t$$æ—¶åˆ»ç”Ÿæˆä¸€ä¸ªtokenï¼Œç„¶åä¸‹ä¸€ä¸ªæ—¶åˆ»æ ¹æ®prompt+ä¸Šä¸€æ—¶åˆ»çš„tokenå†å»ç”Ÿæˆä¸‹ä¸€ä¸ªtoken**ï¼Œè¿›è¡Œè‡ªå›å½’çš„ç”Ÿæˆï¼Œæ‰€ä»¥å¯ä»¥å®šä¹‰å¼ºåŒ–å­¦ä¹ ä¸­çš„å„ä¸ªæ¦‚å¿µä¸ºï¼š
>
> * **åŠ¨ä½œ$$a_t$$ï¼š**&#x751F;æˆçš„ `token`ï¼ŒåŠ¨ä½œç©ºé—´å°±æ˜¯æ•´ä¸ªè¯è¡¨ï¼ŒåŠ¨ä½œç©ºé—´å¤§å°å°±æ˜¯è¯è¡¨å¤§å°$$|V|$$
>
> * **ç­–ç•¥$$\pi(a_t|s_t)$$ï¼š**&#x6839;æ®å½“å‰çŠ¶æ€$$s_t$$ç”ŸæˆåŠ¨ä½œ`token`$$a_t$$çš„æ¦‚ç‡&#x20;
>
> * **çŠ¶æ€$$s_t$$ï¼š**&#x4E0A;æ–‡ä»¥åŠ$$t$$æ—¶åˆ»å‰ç”Ÿæˆçš„**æ‰€æœ‰ token concatçš„ token åºåˆ—ï¼Œ**&#x521D;å§‹çŠ¶æ€ $$s_0$$å°±æ˜¯promptçš„tokenåºåˆ—
>
> * **çŠ¶æ€è½¬ç§»ï¼š**&#x8FD9;é‡Œçš„å¼ºåŒ–å­¦ä¹ çŠ¶æ€è½¬ç§»æ˜¯ç¡®å®šæ€§çš„ï¼Œå®šä¹‰ä¸º**å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„concatçš„tokenåºåˆ—ä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€**ï¼Œå³$$s_{t+1}=[s_t,a_t]$$
>
> * **å¥–åŠ±$$r_t$$å’Œä»·å€¼ $$V_t$$ï¼š**&#x8FD9;é‡Œçš„å®šä¹‰å°±æ˜¯ä¸€èˆ¬å¼ºåŒ–å­¦ä¹ çš„å®šä¹‰ï¼Œ**å³æ—¶å¥–åŠ±ä»¥åŠçŠ¶æ€ä»·å€¼å‡½æ•°**

## **NLP RL ä¼˜åŒ–ç›®æ ‡**

> **å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„æœŸæœ›å€¼ï¼š**
>
> $$\max _\pi \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi} [r(x, y)]$$
>
> å…¶ä¸­$$x,y$$åˆ†åˆ«è¡¨ç¤º prompt å’Œ responseï¼Œ$$r(x,y)$$å³ä»£è¡¨å¯¹äºpromptï¼Œå½“å‰responseçš„å¥–åŠ±å€¼
>
> **å¸¦è¡Œä¸ºçº¦æŸï¼ˆbehavior-regularizedï¼‰çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç›®æ ‡ç­‰åŒäºå¯¹å¥–åŠ±å€¼åšå‡ºäº†ä¿®æ”¹ï¼š**
>
> $$\max _\pi \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi} [r(x, y)] - \beta D_{\text{KL}} [\pi(y|x) \| \pi_{\text{ref}}(y|x)]\\ = \max _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y|x)} \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \right] \\$$
>
> ç›®çš„æ˜¯**çº¦æŸå¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨ä¸€ä¸ªè·ç¦»è¡Œä¸º/å‚è€ƒç­–ç•¥ä¸å¤ªè¿œçš„èŒƒå›´å†…è¿›è¡Œæ›´æ–°ï¼Œé˜²æ­¢ç­–ç•¥è·‘å**ï¼Œè¿™ä¸ª**è¡Œä¸ºç­–ç•¥é€šå¸¸æ˜¯offlineæ•°æ®é›†çš„è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œæˆ–è€…æ˜¯å¤§æ¨¡å‹ä¸­çš„SFTåˆå§‹åŒ–ç­–ç•¥ï¼ˆreference ç­–ç•¥ï¼‰**

# **3.3.2 RLHFæµç¨‹**

> æ·±åº¦å­¦ä¹ RLHFé¦–æ¬¡åœ¨2017å¹´çš„è®º&#x6587;***&#x20;Deep Reinforcement Learning from Human Preferences&#x20;***&#x4E2D;è¢«æå‡ºï¼Œä¸€å¼€å§‹æ˜¯ç”¨äºè§£å†³**å¤æ‚å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ç¯å¢ƒå¥–åŠ±å‡½æ•°è®¾è®¡é—®é¢˜**çš„
>
> **è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/1706.03741**

## **ç ”ç©¶èƒŒæ™¯**



* å¼ºåŒ–å­¦ä¹ åœ¨è®¸å¤šä»»åŠ¡ä¸­é¢ä¸´**ç›®æ ‡å¤æ‚ã€éš¾ä»¥å®šä¹‰å¥–åŠ±å‡½æ•°**çš„é—®é¢˜ï¼Œå¯¼è‡´**éš¾ä»¥å°†äººç±»å®é™…ç›®æ ‡ä¼ è¾¾ç»™æ™ºèƒ½ä½“**

* ä¸æ­£ç¡®çš„ã€æœ‰åçš„å¥–åŠ±å‡½æ•°ä¼šå¯¼è‡´**æ™ºèƒ½ä½“è¿‡åˆ†åˆ©ç”¨(exploit)å¥–åŠ±å‡½æ•°**ï¼Œäº§ç”Ÿ**reward hacking**é—®é¢˜ï¼Œå³**å®é™…å­¦åˆ°çš„è¡Œä¸ºä¸äººç±»æœŸæœ›ä¸ç¬¦åˆï¼Œç”šè‡³æœ‰å®³**

* å¥–åŠ±å‡½æ•°çš„è®¾è®¡å·¥ç¨‹éœ€è¦**å¤§é‡çš„ä¸“ä¸šäººå£«çš„ç²¾åŠ›**

* ç°æœ‰æ–¹æ³•å¦‚**é€†å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ åœ¨å¤„ç†å¤æ‚è¡Œä¸ºæ—¶å­˜åœ¨å±€é™æ€§ï¼Œç›´æ¥ä½¿ç”¨äººç±»åé¦ˆä½œä¸ºå¥–åŠ±å‡½æ•°æˆæœ¬è¿‡é«˜**

## **ç ”ç©¶ç›®æ ‡**

ç”¨äº**è§£å†³æ²¡æœ‰æ˜ç¡®å®šä¹‰å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ **é—®é¢˜ï¼Œéœ€è¦æ»¡è¶³ä»¥ä¸‹å‡ ç‚¹ï¼š

* èƒ½å¤Ÿè§£å†³é‚£äº›äººç±»**åªèƒ½è¯†åˆ«æœŸæœ›è¡Œä¸º**ï¼Œä½†**ä¸ä¸€å®šèƒ½æä¾›demonstrationçš„ä»»åŠ¡**

* å…è®¸**éä¸“å®¶ç”¨æˆ·å¯¹æ™ºèƒ½ä½“è¿›è¡Œæ•™å¯¼**

* èƒ½å¤Ÿ**æ‰©å±•åˆ°å¤§å‹é—®é¢˜**

* åœ¨**ç”¨æˆ·åé¦ˆæ–¹é¢ç»æµé«˜æ•ˆ**

## **ç ”ç©¶æ–¹æ³•**

> ç®—æ³•å°†**å¥–åŠ±å‡½æ•°ä¸äººç±»åå¥½è¿›è¡Œæ‹Ÿåˆ**ï¼ŒåŒæ—¶**ç”¨RLç®—æ³•è®­ç»ƒä¸€ä¸ªç­–ç•¥æ¥ä¼˜åŒ–å½“å‰é¢„æµ‹çš„å¥–åŠ±å‡½æ•°**ã€‚**ç»™äººç±»æä¾›ä¸¤ä¸ªæ™ºèƒ½ä½“è¡Œä¸ºè½¨è¿¹çš„ç‰‡æ®µ(ä¸€èˆ¬æ¥è¯´æ˜¯è§†é¢‘ã€åŠ¨å›¾)ï¼Œè®©äººç»™å‡ºè‡ªå·±çš„åå¥½æ ‡ç­¾(å°±æ˜¯é‚£ä¸ªç‰‡æ®µæ›´å¥½)ï¼Œè€Œä¸æ˜¯æä¾›ç»å¯¹æ•°å€¼åˆ†æ•°**ã€‚åœ¨æŸäº›é¢†åŸŸä¸­ï¼Œäººç±»æ›´å®¹æ˜“è¿›è¡Œæ¯”è¾ƒï¼Œè€Œä¸”è¿™ç§æ¯”è¾ƒå¯¹äºå­¦ä¹ äººç±»åå¥½åŒæ ·æœ‰ç”¨ã€‚æ¯”è¾ƒæ™ºèƒ½ä½“è½¨è¿¹ç‰‡æ®µä¸æ¯”è¾ƒå•ä¸ªçŠ¶æ€å‡ ä¹ä¸€æ ·å¿«ï¼Œç”±æ­¤äº§ç”Ÿçš„æ¯”è¾ƒç»“æœæ˜æ˜¾æ›´æœ‰å¸®åŠ©

![](images/image.png)



> * **å¯¹æ¯”æ ‡ç­¾ï¼š**&#x5BF9;äºæ™ºèƒ½ä½“è½¨è¿¹ç‰‡æ®µ $$\sigma^1$$å’Œ $$\sigma^2$$æ¥è¯´ï¼Œä¸‹é¢çš„å¼å­è¡¨ç¤º $$\sigma^1$$æ¯” $$\sigma^2$$æ›´è¢«äººåå¥½ï¼Œå¾—åˆ°çš„æ ‡ç­¾ $$y$$ä¹Ÿå¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼Œ0.5ä»£è¡¨åŒç­‰åå¥½ç¨‹åº¦ã€‚**&#x20;$$s,a$$åˆ†åˆ«è¡¨ç¤ºæ™ºèƒ½ä½“çš„è§‚æµ‹/çŠ¶æ€å’ŒåŠ¨ä½œ**
>
> **&#x20;**$$\sigma^1\succ\sigma^2=
> \left(\left(s_{0}^{1}, a_{0}^{1}\right), \ldots,\left(s_{k - 1}^{1}, a_{k - 1}^{1}\right)\right) \succ\left(\left(s_{0}^{2}, a_{0}^{2}\right), \ldots,\left(s_{k - 1}^{2}, a_{k - 1}^{2}\right)\right) 
> \\ \\
> y = \{0,1,0.5\} \text{  if  } \{\sigma^1\succ\sigma^2, \sigma^2\succ\sigma^1, \sigma^1=\sigma^2\}$$
>
> * **åå¥½å»ºæ¨¡ï¼š**&#x7531;äºRLHFçš„ä¸€ä¸ªç›®æ ‡æ˜¯å°†**å¥–åŠ±å‡½æ•°ä¸äººç±»åå¥½è¿›è¡Œæ‹Ÿåˆï¼Œ**&#x5C31;æ˜¯**åˆ©ç”¨äººç±»çš„æ¯”è¾ƒåå¥½æ ‡ç­¾æ¥å­¦å‡ºä¸€ä¸ªreward model**ï¼Œé‚£å°±æ¶‰åŠåˆ°äº†å¥–åŠ±å‡½æ•°å’Œåå¥½ä¹‹é—´çš„å…³è”é—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºçš„æ–¹æ³•æ˜¯ï¼Œå°†å¥–åŠ±å‡½æ•°è§†ä¸ºè§£é‡Šäººç±»åˆ¤æ–­çš„æ½œåœ¨å› ç´ ï¼Œå¹¶**å‡è®¾äººç±»åå¥½ä¸€ä¸ªç‰‡æ®µçš„æ¦‚ç‡ä¸æ½œåœ¨å¥–åŠ±åœ¨è¯¥ç‰‡æ®µé•¿åº¦ä¸Šçš„æ€»å’Œå‘ˆæŒ‡æ•°ç›¸å…³**ï¼ŒåŸºäº **Bradley-Terry æ¨¡å‹**ï¼Œå¯ä»¥ç»™å‡º**äººç±»åå¥½ç‰‡æ®µ $$\sigma^1$$è¶…è¿‡ $$\sigma^2$$çš„æ¦‚ç‡**ï¼š
>
> $$
> \hat{P}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum \hat{r}(s_{t}^{1}, a_{t}^{1})}{\exp \sum \hat{r}(s_{t}^{1}, a_{t}^{1}) + \exp \sum \hat{r}(s_{t}^{2}, a_{t}^{2})}$$
>
> * **å¥–åŠ±å­¦ä¹ ï¼š**&#x5F97;åˆ°è¿™ä¸ªåå¥½å»ºæ¨¡ä»¥åŠæ”¶é›†åˆ°çš„äººç±»åå¥½æ ‡ç­¾ä¹‹åï¼Œå°±å¯ä»¥ç®€å•çš„ä½¿ç”¨**äºŒåˆ†ç±»çš„æ€è·¯æ¥éšå¼çš„å­¦ä¹ æˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°**äº†ï¼ŒæŸå¤±å‡½æ•°æ˜¯åˆ†ç±»å¸¸ç”¨çš„**äº¤å‰ç†µï¼Œ**&#x7136;ååˆ©ç”¨è¿™ä¸ªlossè®­ç»ƒä¼˜åŒ–å¾—åˆ°æœ€åçš„ç¬¦åˆäººç±»åå¥½çš„å¥–åŠ±å‡½æ•°
>$$
>  $$ \mathrm{loss}(\hat{r}) = - \mathbb{E}_{(\sigma^{1}, \sigma^{2}, y) \in \mathcal{D}} \left[y(\sigma^1\succ\sigma^2) \log \hat{P}[\sigma^{1} \succ \sigma^{2}] + y(\sigma^2\succ\sigma^1) \log \hat{P}[\sigma^{2} \succ \sigma^{1}]\right]$$
> 
>  å¦‚æœå°†æ­£æ ·æœ¬ï¼ˆè¢«åå¥½ï¼‰å’Œè´Ÿæ ·æœ¬ï¼ˆä¸è¢«åå¥½ï¼‰è®°ä¸º $$\sigma^+,\sigma^-$$ï¼Œåˆ™ä¸Šè¿°losså¯ä»¥å†™æˆï¼š
> 
>  $$
>   \mathrm{loss}(\hat{r}) = - \mathbb{E}_{(\sigma^{+}, \sigma^{-}, y) \in \mathcal{D}} \left[ \log \hat{P}[\sigma^{+} \succ \sigma^{-}] \right] =- \mathbb{E}_{(\sigma^{+}, \sigma^{-}, y) \in \mathcal{D}} \left[ \log \frac{\exp \sum \hat{r}(s_{t}^{+}, a_{t}^{+})}{\exp \sum \hat{r}(s_{t}^{+}, a_{t}^{+}) + \exp \sum \hat{r}(s_{t}^{-}, a_{t}^{-})}\right]$$
> 
>* **ç­–ç•¥å­¦ä¹ ï¼š**&#x5F97;åˆ°å¥–åŠ±å‡½æ•°ä¹‹åå°±å¯ä»¥åº”ç”¨ä»»ä½•ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å»æœ€å¤§åŒ–å¥–åŠ±ç”¨æ¥äº§å‡ºç›¸åº”çš„ç­–ç•¥äº†
> 
>* **åœ¨çº¿å­¦ä¹ ï¼š**&#x8FD9;ç¯‡æ–‡ç« æå‡ºçš„æ–¹æ³•æ˜¯åœ¨çº¿RLHF(Online RLHF)ï¼Œå°±æ˜¯å¥–åŠ±å‡½æ•°å’Œç­–ç•¥å­¦ä¹ æ˜¯äº¤æ›¿åŒæ—¶è¿›è¡Œçš„ï¼Œä¼´éšç€æ™ºèƒ½ä½“ä¸æ–­çš„å’Œç¯å¢ƒäº¤äº’äº§ç”Ÿæ–°çš„è½¨è¿¹æ•°æ®ç”¨æ¥ç»™äººç±»æ‰“åé¦ˆæ ‡ç­¾

## ç ”ç©¶**æ€»ç»“**

> * **ç®—æ³•åŸç†ï¼š**&#x7B97;æ³•é€šè¿‡å°†å¥–åŠ±å‡½æ•°ä¸äººç±»åå¥½è¿›è¡Œæ‹Ÿåˆï¼Œ**ä½¿æ™ºèƒ½ä½“çš„è¡Œä¸ºæœç€ç¬¦åˆäººç±»æœŸæœ›çš„æ–¹å‘å‘å±•**ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–ç­–ç•¥ä»¥æœ€å¤§åŒ–é¢„æµ‹çš„å¥–åŠ±ï¼Œä»è€Œåœ¨æ²¡æœ‰æ˜ç¡®å¥–åŠ±å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œè®©æ™ºèƒ½ä½“å­¦ä¼šåšå‡ºç¬¦åˆäººç±»åå¥½çš„å†³ç­–
>
> * **åé¦ˆæ–¹å¼ä¼˜åŠ¿ï¼š**
>
>   * **æ˜“äºæä¾›ï¼š**&#x76F8;æ¯”æä¾›ç»å¯¹æ•°å€¼åˆ†æ•°ï¼Œ**äººç±»æ›´å®¹æ˜“å¯¹æ™ºèƒ½ä½“è½¨è¿¹ç‰‡æ®µè¿›è¡Œæ¯”è¾ƒ**ï¼Œé™ä½äº†åé¦ˆçš„éš¾åº¦ï¼Œä½¿å¾—éä¸“å®¶ç”¨æˆ·ä¹Ÿèƒ½æ›´è½»æ¾åœ°å‚ä¸åˆ°æ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹ä¸­
>
>   * **ä¿¡æ¯ä¸°å¯Œï¼š**&#x667A;èƒ½ä½“è½¨è¿¹ç‰‡æ®µåŒ…å«äº†ä¸€å®šçš„è¡Œä¸ºåºåˆ—ä¿¡æ¯ï¼Œæ¯”å•ä¸ªçŠ¶æ€æ›´èƒ½åæ˜ æ™ºèƒ½ä½“çš„è¡Œä¸ºç‰¹ç‚¹å’Œè¶‹åŠ¿ï¼Œå› æ­¤åœ¨å­¦ä¹ äººç±»åå¥½æ–¹é¢æ›´æœ‰å¸®åŠ©ï¼Œèƒ½å¤Ÿ**ä¸ºå¥–åŠ±å‡½æ•°çš„æ‹Ÿåˆæä¾›æ›´æœ‰ä»·å€¼çš„ä¿¡æ¯**
>
>   * **åœ¨çº¿åé¦ˆçš„å¥½å¤„ï¼š**&#x5728;çº¿æ”¶é›†åé¦ˆæ„å‘³ç€ç³»ç»Ÿå¯ä»¥**å®æ—¶è·å–äººç±»çš„åå¥½ä¿¡æ¯**ï¼Œå¹¶**æ ¹æ®æ–°çš„åé¦ˆåŠæ—¶è°ƒæ•´ç­–ç•¥å’Œå¥–åŠ±å‡½æ•°**ã€‚è¿™æ ·å¯ä»¥**é¿å…ç³»ç»Ÿè¿‡åº¦ä¾èµ–ä¹‹å‰å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°**ï¼Œ**é˜²æ­¢å› å¥–åŠ±å‡½æ•°çš„ä¸å‡†ç¡®æ€§æˆ–å±€é™æ€§è€Œå¯¼è‡´çš„ä¸è‰¯è¡Œä¸º**ï¼Œä»è€ŒæŒç»­æé«˜ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ä»»åŠ¡ç¯å¢ƒï¼›ç¼ºç‚¹å°±æ˜¯**å®æ—¶æ”¶é›†äººç±»åå¥½æ ‡ç­¾æˆæœ¬å¾ˆé«˜**ï¼Œæ‰€ä»¥ä¹‹ååœ¨LLMä¸­åº”ç”¨çš„æ—¶å€™ï¼Œå¾ˆå¤šå·¥ä½œéƒ½åœ¨ç ”ç©¶**è‡ªåŠ¨åŒ–åå¥½æ ‡ç­¾ï¼Œæ¯”å¦‚RLAIFï¼Œåˆ©ç”¨å¤§æ¨¡å‹ä»£æ›¿äººç±»ç»™åå¥½**

# **3.3.3 LLMå¯¹é½ä¸­çš„RLHF+PPO**

> è®­ç»ƒæ¡†æ¶ä»ç„¶é€‰ç”¨`OpenRLHF`æ¯”è¾ƒæ–¹ä¾¿å¥½ç”¨

## **InstructGPT**

> ç°åœ¨åˆçœ‹å›æ¥å…·ä½“åœ¨LLMçš„å¯¹é½ä¸­æ˜¯æ€ä¹ˆåº”ç”¨ä¸Šé¢é‚£ä¸€å¥—RLHFçš„æµç¨‹å’ŒPPOç®—æ³•çš„ï¼ŒOpenAIæœ€æ—©åœ¨**InstructGPT**ä¸­å°†RLHFæŠ€æœ¯åº”ç”¨åœ¨LLM post-trainingä¸­ï¼Œä½¿æ¨¡å‹çœŸæ­£ä¸äººç±»æ„å›¾å¯¹é½
>
> **è®ºæ–‡ï¼š*Training language models to follow instructions with human feedback***
>
> **é“¾æ¥ï¼šhttps://arxiv.org/pdf/2203.02155**

![](images/image-1.png)

> InstructGPTçš„æ•´ä¸ªæµç¨‹æ˜¯**å…ˆåŸºäºGPT-3è¿›è¡ŒSFTè®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œç„¶åäººåŠ›ç»™æ•°æ®æ‰“åå¥½æ ‡ç­¾ï¼Œå¾—åˆ°çš„åå¥½æ•°æ®åˆ©ç”¨å‰é¢æåˆ°çš„RLHFçš„åå¥½å»ºæ¨¡è®­ç»ƒä¸€ä¸ªReward Modelï¼Œæœ€åå†åº”ç”¨PPOç®—æ³•æœ€å¤§åŒ–è¿™ä¸ªReward Modelæä¾›çš„å¥–åŠ±å€¼æ¥åŸºäºSFTæ¨¡å‹è®­ç»ƒå¾—åˆ°InstructGPT**ï¼Œæ•´ä¸ªè¿‡ç¨‹æœ‰å‡ ä¸ªå€¼å¾—æ³¨æ„çš„åœ°æ–¹ï¼š
>
> * **Reward Modelæ˜¯åœ¨ç§»é™¤äº†æœ€åéåµŒå…¥å±‚(unembedding)çš„ SFT æ¨¡å‹çš„åŸºç¡€ä¸Šè®­ç»ƒçš„**ï¼Œä½¿å…¶æ¥æ”¶ä¸€ä¸ªpromptå’Œresponseï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡rewardå€¼ã€‚ç°åœ¨çš„åšæ³•é€šå¸¸æ˜¯åœ¨åŸæœ¬æ¨¡å‹åŸºç¡€ä¸Šå¢åŠ ä¸€ä¸ª`[hidden_size,1]` çš„çº¿æ€§å±‚è®­ç»ƒï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ä½œä¸ºå¥–åŠ±
>
> * å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œ**å¯¹äºåŒä¸€ä¸ªpromptï¼Œ äººå·¥å¯èƒ½éœ€è¦æ‰“æ ‡ç­¾çš„responseä¸æ­¢ä¸¤ä¸ª**ï¼Œä¹Ÿå°±æ˜¯éœ€è¦ç»™å¤šä¸ªå›ç­”æ’åºï¼Œç„¶åInstructGPTçš„åšæ³•æ˜¯é€šè¿‡æ’åºæ ‡ç­¾ï¼Œå†å°†å…¶è½¬æ¢ä¸º $$C_K^2$$ä¸ªä¸¤ä¸¤å¯¹æ¯”çš„åå¥½æ ‡ç­¾
>
> * InstructGPTåœ¨**å®ç°PPOçš„æ—¶å€™å¯¹å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡è¿›è¡Œäº†ä¿®æ”¹**ï¼ŒåŸæœ¬çš„å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯**æœ€å¤§åŒ–å¥–åŠ±**ï¼š
>
>   $$
>   \text{objective}(\phi) = \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{\text{RL}}}} \left[ r_{\phi}(x, y)  \right]
>   $$
>
>   InstructGPTåœ¨æ­¤åŸºç¡€ä¸Šï¼Œå‚è€ƒä¹‹å‰çš„ä¸€äº›æ–‡çŒ®ï¼Œå¯¹å¥–åŠ±å‡½æ•°åŠ ä¸Šäº†**RL policyå’Œ SFT policyçš„per-token level KLæ•£åº¦æ¥é˜²æ­¢RL policyçš„è¿‡ä¼˜åŒ–**ï¼Œå¯¼è‡´åç¦»SFTæ¨¡å‹å¤ªè¿œã€‚è¿™æ ·çš„æ“ä½œæœ€å¸¸è§åœ¨**Behavior-Regularized Offline RLï¼ˆè¡Œä¸ºçº¦æŸçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼‰**&#x4E2D;ï¼š
>
>   $$
>   \text{objective}(\phi) = \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{\text{RL}}}} \left[ r_{\phi}(x, y) - \beta \log \left( \frac{\pi^{\text{RL}}_{\phi}(y \mid x)}{\pi^{\text{SFT}}(y \mid x)} \right) \right]
>   $$
>
>   å½¢å¼ä¸Šç­‰åŒäºé‡æ–°è®¾è®¡äº†å¥–åŠ±å‡½æ•°ï¼Œå…¶ä¸­åŒ…å«äº†è¡Œä¸ºçº¦æŸé¡¹ï¼Œä½¿å¾—RLç­–ç•¥çš„è¡Œä¸ºä¸è¦å¤ªåç¦»SFTæ¨¡å‹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œ**InstructGPTè¿˜åŠ å…¥äº†pretrainæ•°æ®çš„æ¢¯åº¦ï¼ˆPPO-ptxï¼Œè¯¦è§3.3.4ç« PPOè®­ç»ƒçš„Trickå’Œé—®é¢˜ï¼‰ï¼Œæ¥ä¿®å¤åœ¨å…¬å¼€NLPæ•°æ®é›†ä¸Šçš„æ€§èƒ½å›å½’**ï¼š
>
>   $$
>   \text{objective}(\phi) = \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{\text{RL}}}} \left[ r_{\phi}(x, y) - \beta \log \left( \frac{\pi^{\text{RL}}_{\phi}(y \mid x)}{\pi^{\text{SFT}}(y \mid x)} \right) \right] + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log \left( \pi_{\text{RL}}^{\phi}(x) \right) \right]
>   $$

* **æ•´ä½“æµç¨‹å›¾ï¼š**&#x6574;ä½“çš„æµç¨‹ä¸­åŒ…å«**Actorã€Criticã€Rewardå’ŒReference(SFT)å››ä¸ªæ¨¡å‹**

![](images/image-2.png)

## **Actor Model**

> inputä¸€æ¡prompt ï¼ˆbatch\_size = 1ï¼‰ï¼Œoutputå¯¹åº”çš„responseï¼Œå†å°†prompt + responseå»è®¡ç®—å¾—åˆ°lossï¼Œç”¨äºæ›´æ–°actorï¼Œå…·ä½“&#x7684;**&#x20;PPO loss å’Œ GAE** è®¡ç®—&#x89C1;**&#x20;3.2.11 PPOç®—æ³•[ 3.2 RL å¼ºåŒ–å­¦ä¹ åŸºç¡€](https://kcnd4kn8i6ap.feishu.cn/wiki/Cz5YwDjdpiPbIFkiqWecXt0EnNb?fromScene=spaceOverview#share-VRhadlhMUoxfSzxH3u0cfZXvnY2)**ï¼Œ $$\epsilon$$å–0\~1ä¹‹é—´çš„å€¼
>
> $$
> \text{Actor\_loss} = -\min \left(   \frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}\cdot A_t(s_t,a_t), \text{clip} \left( \frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}, 1-\epsilon, 1+\epsilon \right) \cdot A_t(s_t,a_t)\right)
>  $$
>
> å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è®¡ç®—çš„æ˜¯token-levelçš„ä¼˜åŠ¿å‡½æ•°ï¼Œæ¯ä¸€ä¸ªè¾“å‡ºtokenå¤„éƒ½è®¡ç®—loss

![](images/diagram-1.png)

```python
class PolicyLoss(nn.Module):
    """
    Policy Loss for PPO
    """

    def __init__(self, clip_eps: float = 0.2) -> None:
        super().__init__()
        self.clip_eps = clip_eps

    def forward(
        self,
        log_probs: torch.Tensor,
        old_log_probs: torch.Tensor,
        advantages: torch.Tensor,
        action_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        ratio = (log_probs - old_log_probs).exp()
        surr1 = ratio * advantages
        surr2 = ratio.clamp(1 - self.clip_eps, 1 + self.clip_eps) * advantages
        loss = -torch.min(surr1, surr2)
        loss = masked_mean(loss, action_mask, dim=-1).mean()
        return loss

def masked_mean(tensor, mask, dim):
    if mask is None:
        return tensor.mean(axis=dim)
    return (tensor * mask).sum(axis=dim) / mask.sum(axis=dim)
```

## **Critic Model**

> ç”¨äºé¢„æµ‹æœŸæœ›æ€»æ”¶ç›Š $$V_t$$ ï¼Œåˆå§‹åŒ–æ–¹å¼æœ‰å¾ˆå¤šç§ï¼Œä¾‹å¦‚**å’ŒActorå…±äº«éƒ¨åˆ†å‚æ•°ã€ä»Reward Modelåˆå§‹åŒ–è€Œæ¥**ã€‚Value Headå±‚æ˜¯ä¸ªç®€å•çš„çº¿å½¢å±‚ï¼Œç”¨äºå°†åŸå§‹è¾“å‡ºç»“æœæ˜ å°„æˆå•ä¸€çš„å€¼ã€‚**Critic Modelæˆ–è€…è¯´Value Modelç”¨Vå€¼çš„TD-erroræ›´æ–°ï¼Œå½“å‰çŠ¶æ€çš„é¢„ä¼°æ”¶ç›Šå¯¹é½å³æ—¶å¥–åŠ±åŠ ä¸Šä¸‹ä¸€æ­¥çŠ¶æ€é¢„ä¼°æ”¶ç›Šï¼ŒTD-errorè¯¦è§3.2.6**
>
> $$  \text{Critic\_loss} = (r_t + \gamma \cdot V_{t+1} - V_t)^2  $$
>
> å®é™…çš„ä»£ç ä¸­ä¼šç”¨åˆ°GAE+value=returnsï¼Œç„¶åcriticå½“å‰çš„è¾“å‡ºvalueså»å¯¹é½returnsï¼Œ $$Critic\_loss = (returns-values)^2$$
>
> åœ¨RLHFä¸­ï¼Œæˆ‘ä»¬ä¸ä»…è¦è®­ç»ƒæ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»å–œå¥½çš„å†…å®¹çš„èƒ½åŠ›ï¼ˆActorï¼‰ï¼Œä¹Ÿè¦æå‡æ¨¡å‹å¯¹äººç±»å–œå¥½é‡åŒ–åˆ¤æ–­çš„èƒ½åŠ›ï¼ˆCriticï¼‰ã€‚è¿™å°±æ˜¯Criticæ¨¡å‹å­˜åœ¨çš„æ„ä¹‰ï¼ˆä¸Šè¿°æ˜¯é€šä¿—è§£é‡Šï¼Œå…·ä½“å¯ä»¥å›é¡¾ **3.2.10 Actor-Criticç®—æ³•[ 3.2 RL å¼ºåŒ–å­¦ä¹ åŸºç¡€](https://kcnd4kn8i6ap.feishu.cn/wiki/Cz5YwDjdpiPbIFkiqWecXt0EnNb?fromScene=spaceOverview#share-SAZRdtSAQomwSlxOKMTcDXRpnqe)**ï¼‰
>
> **çŠ¶æ€ä»·å€¼å‡½æ•°Væ˜¯é’ˆå¯¹çŠ¶æ€æ¥è¯´çš„ï¼Œæ‰€ä»¥V0å¯¹åº”S0ï¼Œå³promptï¼Œæ‰€ä»¥å–promptæœ€åä¸€ä¸ªtokenä½ç½®çš„è¾“å‡ºä½œä¸ºV0**

![](images/diagram-2.png)

```python
class ValueLoss(nn.Module):
    """
    values: å®æ—¶criticè·‘å‡ºæ¥çš„é¢„ä¼°é¢„æœŸæ”¶ç›Š
    old_valuesï¼šè€criticè·‘å‡ºæ¥çš„é¢„ä¼°é¢„æœŸæ”¶ç›Š
    returnsï¼šå®é™…é¢„æœŸæ”¶ç›Š
    maskï¼šresponseéƒ¨åˆ†çš„mask
    """

    def __init__(self, clip_eps: float = None) -> None:
        super().__init__()
        self.clip_eps = clip_eps

    def forward(
        self,
        values: torch.Tensor,
        old_values: torch.Tensor,
        returns: torch.Tensor,
        action_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        # è¿™é‡Œå¯¹Valueä¹Ÿè¿›è¡Œäº†clipæ“ä½œï¼Œé“ç†å’Œactor lossé‚£ä¸ªclipä¸€æ ·
        if self.clip_eps is not None:
            values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)
            surr1 = (values_clipped - returns) ** 2
            surr2 = (values - returns) ** 2
            loss = torch.max(surr1, surr2)
        else:
            loss = (values - returns) ** 2

        loss = masked_mean(loss, action_mask, dim=-1).mean()
        return 0.5 * loss
```

## **Reward Model**

> åœ¨**PPOè®­ç»ƒè¿‡ç¨‹ä¸­Reward Modelå‚æ•°å†»ç»“ï¼Œåªç”¨æ¥æä¾›å¥–åŠ±å€¼**ï¼Œåªå–æœ€åtokenå¤„çš„å¥–åŠ±å€¼ï¼Œä½œä¸ºæ•´ä¸ªå¥å­çš„å¥–åŠ± $$r(x,y)$$ã€‚Reward modelä¸€èˆ¬æ˜¯åœ¨SFT modelçš„åŸºç¡€ä¸Šæ·»åŠ Value Headè®­ç»ƒçš„ï¼Œå…·ä½“reward modelè®­ç»ƒæµç¨‹å’Œä»£ç è§ **ï¼ˆ9.2.4ç« RMè®­ç»ƒï¼‰**

![](images/diagram-3.png)

> **Reward Modelè®­ç»ƒçš„losså¦‚ä¸‹ï¼š**
>
> $$\text{Reward\_loss}=- \mathbb{E}_{(x,y_w,y_l)\sim D} \left[\log\left(\sigma\left(r(x,y_w)-r(x,y_l)\right)\right)\right]$$
>
> å…¶ä¸­$$x,y_w,y_l$$åˆ†åˆ«è¡¨ç¤º`prompt`ã€`chosen response` å’Œ `rejected response`
>
> sigmoidå‡½æ•°ï¼š $$\sigma(x)=\frac{1}{1+\exp(-x)}$$ï¼Œæ‰€ä»¥ $$\sigma(r(x,y_w)-r(x,y_r))=\frac{\exp(r(x,y_w))}{\exp(r(x,y_w))+\exp(r(x,y_l))}$$
>
> æœ€åçš„reward lossä¸º
>
> $$\text{Reward\_loss}=- \mathbb{E}_{(x,y_w,y_l)\sim D} \left[\log\frac{\exp(r(x,y_w))}{\exp(r(x,y_w))+\exp(r(x,y_l))}\right]$$

```python
class PairWiseLoss(nn.Module):
    """
    Pairwise Loss for Reward Model
    """
    def forward(self, chosen_reward, reject_reward, margin):
        if margin is not None:
            loss = -F.logsigmoid(chosen_reward - reject_reward - margin)
        else:
            loss = -F.logsigmoid(chosen_reward - reject_reward)
        return loss.mean()
```

> ### **ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ RLHFçš„å¯¹æ¯”**
>
> å¯¹æ¯”ä¸€ä¸‹æˆ‘ä»¬å‰é¢**3.3.2ç« èŠ‚[ 3.3 RLHF åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ](https://kcnd4kn8i6ap.feishu.cn/wiki/TQqTwh2uwiSrqYktIPccTQOcn0g?fromScene=spaceOverview#share-Wq9IdaBcnoFbVNxMffzcxW2xnMd)**&#x4F20;ç»Ÿå¼ºåŒ–å­¦ä¹ çš„RLHFçš„lossï¼š
>
> $$
> \text{Reward\_loss}  =- \mathbb{E}_{(\sigma^{+}, \sigma^{-}, y) \in \mathcal{D}} \left[ \log \frac{\exp \sum r(s_{t}^{+}, a_{t}^{+})}{\exp \sum r(s_{t}^{+}, a_{t}^{+}) + \exp \sum r(s_{t}^{-}, a_{t}^{-})}\right]$$
>
> **ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ çš„RLHFæ˜¯å¯¹ä¸€æ¡è½¨è¿¹é‡Œçš„æ‰€æœ‰ $$(s,a)$$çŠ¶æ€åŠ¨ä½œå¯¹çš„å¥–åŠ±è¿›è¡Œäº†åŠ å’Œï¼Œè€Œå¤§æ¨¡å‹Reward modelè¿™é‡Œåˆ™åªæœ‰ä¸€ä¸ªé’ˆå¯¹æ•´ä¸ªresponseçš„å¥–åŠ±å€¼ï¼›ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å¯¹æ¯”çš„ä¸¤ä¸ªç‰‡æ®µè½¨è¿¹åˆå§‹çŠ¶æ€ $$s^+_0,s^-_0$$ä¸ä¸€å®šæ˜¯ç›¸åŒçš„ï¼Œè€Œå¤§æ¨¡å‹çš„åå¥½æ•°æ®è¿™é‡Œåˆå§‹çŠ¶æ€prompt$$x$$æ˜¯ç›¸åŒçš„**
>
> æ›´ä¸€èˆ¬çš„ï¼Œæˆ‘ä»¬å¯ä»¥**æŠŠä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ RLHF lossä¸­çš„å¥–åŠ±åŠ å’Œæ¢æˆä¸€ä¸ªèšåˆæ“ä½œ `AGG`**ï¼Œå…¶ä¸­èšåˆæ“ä½œå¯ä»¥å–å¤šç§ $$\text{AGG}=[\sum,\sum\beta,-1,\text{Transformer()}]$$ï¼Œè¿™é‡Œåˆ—ä¸¾çš„èšåˆæ“ä½œåˆ†åˆ«æ˜¯åŠ å’Œã€åŠ æƒå’Œã€å–æœ€åå’ŒTransformerèšåˆ
>
> **ä»è¿™é‡Œå¯ä»¥å»¶ä¼¸å‡ºå¯¹å¤§æ¨¡å‹Reward modelè®­ç»ƒlossçš„ä¸¤ä¸ªç†è§£**ï¼Œé¦–å…ˆæˆ‘ä»¬çœ‹ä¸€ä¸‹Reward Modelè®­ç»ƒçš„æ—¶å€™çš„æ“ä½œå¦‚ä¸‹å›¾ï¼Œå¯¹äº**æ¯ä¸€ä¸ªresponseåªå–æœ€åä¸€ä¸ªtokenä½ç½®å¯¹åº”çš„valueä½œä¸ºæ•´ä¸ªresponseçš„rewardå€¼å³ $$r(x,y)$$**

![](images/diagram-4.png)

> ### **MDPè§’åº¦**
>
> * **ç¬¬ä¸€ä¸ªè§’åº¦ï¼Œå°†promptåˆ°responseçš„è¿‡ç¨‹çœ‹ä½œæ˜¯å•æ­¥MDP**ã€‚å³åˆå§‹çŠ¶æ€ $$s_0=\text{prompt}$$ï¼ŒåŠ¨ä½œ $$a_0=\text{response}$$ï¼ŒäºŒè€…éƒ½æ˜¯tokenåºåˆ—ï¼ŒåŒºåˆ«æ˜¯ä¹‹å‰å®šä¹‰çš„NLP RL MDPé‡Œé¢åŠ¨ä½œæ˜¯ä¸€ä¸ªtokenï¼Œè¿™é‡Œæ˜¯tokenåºåˆ—ã€‚è¿™æ ·å°±æ˜¯ä¸€ä¸ªå•æ­¥MDPï¼Œæ²¡æœ‰è½¬ç§»åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ï¼Œç„¶åæ ¹æ®å½“å‰çŠ¶æ€åŠ¨ä½œç»™å‡ºå¥–åŠ± $$r(s_0,a_0)$$ä¹Ÿå°±æ˜¯ $$r(x,y)$$ã€‚ä»è¿™ä¸ªè§’åº¦çœ‹ï¼Œå¯ä»¥ç†è§£é‡‡ç”¨äº†`AGG`èšåˆæ“ä½œä¸­çš„`-1`æ“ä½œï¼Œå³æœ€åä¸€æ­¥çŠ¶æ€åŠ¨ä½œå¯¹çš„å¥–åŠ±
>
> * **ç¬¬äºŒä¸ªè§’åº¦ï¼Œå°†promptåˆ°responseçš„è¿‡ç¨‹çœ‹ä½œå¤šæ­¥MDP**ã€‚å³`[prompt,response]`ä¸­æ¯ä¸€ä¸ªtokenéƒ½æ˜¯ä¸€ä¸ªåŠ¨ä½œï¼ŒçŠ¶æ€åˆ™æ˜¯å½“å‰åŠ¨ä½œå’Œä¹‹å‰æ‰€æœ‰tokenæ‹¼æ¥çš„åºåˆ—ã€‚æ­¤æ—¶å°†æ•´ä¸ªCasual LLMçœ‹ä½œæ•´ä½“ï¼Œä¸­é—´ç»“æœçœ‹ä½œ $$r(s_t,a_t)$$ï¼Œé‚£ä¹ˆæœ€åä¸€ä¸ªtokenä½ç½®è¾“å‡ºçš„rewardå€¼å¯ä»¥çœ‹ä½œæ˜¯é‡‡ç”¨äº†`AGG`èšåˆæ“ä½œä¸­çš„`Transformer`æ“ä½œï¼Œå³èšåˆäº†æ•´ä¸ªå¥å­ä¿¡æ¯çš„å¥–åŠ±å€¼ $$r(x,y)=\text{Transformer}(\mathbb{I(s_0,a_0)},\mathbb{I(s_1,a_1),\cdots,\mathbb{I}}(s_T,a_T))$$ï¼Œ$$\mathbb{I}(s,a)$$ä»£è¡¨ä¸­é—´ç»“æœ
>
> **ä¸ºä»€ä¹ˆè¦ç”¨ä»£è¡¨æ•´ä¸ªå¥å­çš„å¥–åŠ±å€¼ï¼Ÿå› ä¸ºåå¥½æ ‡ç­¾æ˜¯å¥å­çº§åˆ«çš„**ï¼ˆå½“ç„¶åˆ°äº†PRMè¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ—¶å€™ä¹Ÿä¼šæœ‰ä¸­é—´è¿‡ç¨‹çš„åå¥½æ ‡ç­¾ï¼‰

## **Reference Model**

> ä¸€èˆ¬ä¹Ÿç”¨**SFTé˜¶æ®µå¾—åˆ°çš„SFTæ¨¡å‹åšåˆå§‹åŒ–**ï¼Œåœ¨PPOè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒçš„å‚æ•°æ˜¯å†»ç»“çš„ï¼Œç”¨æ¥äº§ç”Ÿper-token çš„KLçº¦æŸé¡¹ï¼Œé˜²æ­¢ç­–ç•¥å¯¼è‡´åç¦»SFTæ¨¡å‹å¤ªè¿œï¼Œä¹Ÿå°±æ˜¯è®­åäº†

![](images/diagram-5.png)

> ç„¶åæ ¹æ®ä¼˜åŒ–ç›®æ ‡$$\max _\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y|x)} \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \right] \\$$ï¼Œæ–°çš„å¥–åŠ±å¯ä»¥å†™æˆ $$r(x,y)-\beta KL\_reward$$
>
> æ‰€ä»¥æ–°çš„token-level reward å¯ä»¥è¡¨ç¤ºæˆ$$\left\{\begin{array}{l}
> r_{t}=-\beta *\left(\log \frac{\pi\left(a_{t} \mid s_{t}\right)}{\pi_{r e f}\left(a_{t} \mid s_{t}\right)}\right), t \neq T \\
> r_{t}=r(x,y) -\beta *\left(\log \frac{\pi\left(a_{t} \mid s_{t}\right)}{\pi_{r e f}\left(a_{t} \mid s_{t}\right)}\right), t=T
> \end{array}\right.$$ï¼Œå…¶ä¸­$$T$$è¡¨ç¤ºç»ˆæ­¢çŠ¶æ€æ—¶é—´ï¼Œä¹Ÿå°±æ˜¯å¥å­æœ«å°¾çš„tokenï¼Œæˆ–è€…è¡¨ç¤ºä¸º$$r(s_t, a_t) = \textbf{I}(s_t =[\text{EOS}])r(x,y)-\beta \text{KL}(t)$$ï¼Œå…·ä½“å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](images/diagram-6.png)

## **Online & Offline RLHF**

> **å‚è€ƒé“¾æ¥ï¼šhttps://www.zhihu.com/question/651021172/answer/3513159005**
>
> **Onlineå’Œ Offline**ä¹Ÿå¯ä»¥**å›é¡¾3.2.7ç« [ 3.2 RL å¼ºåŒ–å­¦ä¹ åŸºç¡€](https://kcnd4kn8i6ap.feishu.cn/wiki/Cz5YwDjdpiPbIFkiqWecXt0EnNb?fromScene=spaceOverview#share-QwGzdNFoxoeBOox3odmcUjhPnec)**
>
> * **Online çš„æ ¸å¿ƒæ€è·¯å°±æ˜¯ï¼šè®©æ¨¡å‹è‡ªå·±åšç”Ÿæˆï¼Œæˆ‘ä»¬æ ¹æ®æ¨¡å‹ç”Ÿæˆç»“æœçš„å¥½åæ¥æ‰“åˆ†ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹è¿›è¡Œæ›´æ–°**ã€‚Online éœ€è¦æ¨¡å‹äº²è‡ªè¾“å‡ºç­”æ¡ˆï¼Œç„¶åæ ¹æ®åé¦ˆå­¦ä¹ ï¼›
>
> * **Offline çš„æ–¹æ³•ï¼š**&#x6A21;å‹ä¸éœ€è¦äº²è‡ªè¾“å‡ºç­”æ¡ˆï¼Œæ ¹æ®æå‰æ”¶é›†å¥½çš„Offlineæ•°æ®é›†ä¸­çš„ç»™å®šçš„ã€Œå¥½åæ ·æœ¬ã€æ¥è¿›è¡Œæ¨¡æ‹Ÿå­¦ä¹ ã€‚Off Policy çš„è®­ç»ƒé€Ÿåº¦èƒ½å¤Ÿæ›´å¿«ï¼ˆåªç”¨forwardçœ‹å¤§é‡çš„æ ·æœ¬æ¥å­¦ä¹ ï¼Œä¸ç”¨generateï¼‰ï¼Œä½†éå¸¸ä¾èµ–ç»™å®šçš„æ•°æ®æ˜¯å¦å’Œã€Œæ¨¡å‹è‡ªèº«èƒ½åŠ›ã€è¶³å¤Ÿç›¸è¿‘ã€‚æœ€ç†æƒ³çš„æ•ˆæœå°±æ˜¯ï¼Œæ‰¾åˆ°å¤§é‡å’Œä½ è‡ªèº«æ°´å¹³å·®ä¸å¤šçš„ç©å®¶çš„å¯¹å±€èµ„æ–™ç»™ä½ å­¦ä¹ ï¼Œè¿™äº›è®­ç»ƒæ ·æœ¬çš„åˆ©ç”¨ç‡æ‰æ˜¯æœ€é«˜çš„ã€‚åä¹‹ï¼Œå¯¹äº Online æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬éƒ½æ˜¯æ¨¡å‹è‡ªå·±ç”Ÿæˆçš„

# **3.3.4 PPOè®­ç»ƒçš„Trickå’Œé—®é¢˜**

## **æ¨¡å‹å±‚é¢**

> 1. **Token Level KL-Penaltyï¼š**&#x52;Læ¨¡å‹å’ŒSFTæ¨¡å‹çš„å“åº”åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ä¼šå¯¹æ¯ä¸ªtokenè¿›è¡Œè®¡ç®—ã€‚è¿™ä¸ªæ•£åº¦éšååœ¨è®­ç»ƒæœŸé—´ä½œä¸ºå¥–åŠ±å‡½æ•°ä¸­çš„æƒ©ç½šé¡¹è¢«çº³å…¥ã€‚å…·ä½“æ¥è¯´ï¼ŒæŒ‰æ ‡è®°çš„å¥–åŠ±è¡¨ç¤ºå¦‚ä¸‹ï¼š
>
> $$r(s_t, a_t) = \textbf{I}(s_t =[\text{EOS}])r(x,y)-\beta \text{KL}(t)$$
>
> $$\text{KL}(t) = \log({\pi_{\theta_{\text{}}}(a_t|s_t)^{\text{RL}}}/{\pi^{\text{SFT}}(a_t|s_t)}ï¼‰$$
>
> * **Generalized Advantage Estimation ï¼š**&#x5C06;GAEç”¨äºåœ¨ PPO ä¸­ä¼°è®¡é€ä¸ª token çš„å¥–åŠ±ã€‚**é€šå¸¸è®¾ç½® Î»=1ï¼Œå°† GAE æ–¹æ³•è½¬å˜ä¸ºè’™ç‰¹å¡æ´›ä¼°è®¡æ–¹æ³•**
>
> * **Adding SFT Lossï¼š**&#x50;POä¸­**åŠ å…¥é¢å¤–çš„ç›‘ç£ä¸‹ä¸€ä¸ªtokené¢„æµ‹æŸå¤±ï¼Œä»¥åŠKLæ•£åº¦ï¼Œå¯ä»¥ä¿ç•™SFTæ¨¡å‹çš„æ—¢æœ‰èƒ½åŠ›**

## **PPOå±‚é¢**

4. **PPO-ptxï¼šè®ºæ–‡InstructGPTï¼Œ**&#x50;PO-ptxå°±æ˜¯åœ¨åŸæœ¬çš„PPOä¼˜åŒ–ç›®æ ‡ï¼ˆå¸¦KLè¡Œä¸ºçº¦æŸçš„æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼‰åŸºç¡€ä¸Šï¼Œ**å¢åŠ äº†ä¸€é¡¹å½“å‰policyåœ¨pretrainæ•°æ®é›†ä¸Šçš„ä¼˜åŒ–ç›®æ ‡**ï¼Œæˆ–è€…è¯´åŠ äº†åœ¨pertainæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒlossï¼Œå³ptx lossï¼Œ**ç”¨äºé¿å…ç­–ç•¥é—å¿˜é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„çŸ¥è¯†**ï¼š

   $$
   \mathrm{objective}(\phi) = E_{(x,y)\sim D_{\pi_{\phi}^{\mathrm{RL}}}} \left[r_{\theta}(x,y) - \beta \log \left(\pi_{\phi}^{\mathrm{RL}}(y|x)/\pi^{\mathrm{SFT}}(y|x)\right)\right] +  \gamma E_{x\sim D_{\mathrm{pretrain}}} \left[\log(\pi_{\phi}^{\mathrm{RL}}(x))\right]$$

   è¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†**å‡è½»å¯¹é½ç¨ï¼ˆAlignment Taxï¼‰**ï¼Œ&#x5373;**&#x20;RLHF è™½ç„¶æœ‰åŠ©äºå¯¹é½äººç±»åå¥½ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æŸäº› NLP åŸºå‡†ä¸Šçš„æ€§èƒ½ä¸‹é™**

   ![](images/image-3.png)

   **å¯¹é½ç¨ç¤ºæ„å›¾**

   > è®º&#x6587;***Training a helpful and harmless assistant with reinforcement learning from human feedback***&#x4E2D;ï¼ŒAnthropic çš„ç ”ç©¶è€…è¯„ä¼°äº†å¤§å°åœ¨ 13M åˆ° 52B ä¹‹é—´çš„ 7 ç§ä¸åŒæ¨¡å‹ï¼Œå¾—å‡ºçš„ç»“è®ºæ˜¯å¯¹è¾ƒå°çš„æ¨¡å‹æ¥è¯´ï¼Œä¼šæœ‰å¯¹é½ç¨ï¼Œä½†å¯¹è¾ƒå¤§æ¨¡å‹æ¥è¯´ï¼Œå¯¹é½åªæœ‰å¥½å¤„ï¼Œå°¤å…¶æ˜¯å‚æ•°é‡åœ¨ 13B åˆ° 52B ä¹‹é—´çš„æ¨¡å‹ï¼Œ**å³åªè¦æ¨¡å‹å¤Ÿå¤§ï¼ŒPPO æœ¬èº«å°±èƒ½åœ¨ NLP ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¸¦æ¥å¯¹é½çš„å¥½å¤„**ï¼Œä»–ä»¬è¿˜ç¡®å®šäº†å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒä¸­ KL æ•£åº¦ç³»æ•°çš„æœ€ä¼˜å‚æ•°ä¸º Î² = 0.001

5. **KL Rewardï¼š**&#x7B2C;äºŒé¡¹çš„ KL rewardå‰é¢æœ‰ä¸€ä¸ªç³»æ•° betaï¼Œä»å®é™…è®­ç»ƒçš„ä½“éªŒæ¥è¯´ betaçš„è®¾ç½®éå¸¸é‡è¦ï¼Œå¯ä»¥æœ‰æ•ˆé¿å…ç­–ç•¥èµ°çš„å¤ªè¿œï¼ˆèµ°å¤ªè¿œå®¹æ˜“å¯¼è‡´ç­–ç•¥è¿‡æ‹Ÿåˆå’Œåå¡Œï¼‰ï¼Œè¿™é‡Œbetaçš„è®¾å®šé€šå¸¸è¦ç»“åˆtarget KLçš„è®¾å®šï¼Œå³æˆ‘ä»¬å¯ä»¥é€šè¿‡å®éªŒç¡®å®šKLå˜åŒ–å¤šå¤§æ¨¡å‹çš„è¡¨ç°æ¯”è¾ƒå¥½ï¼Œç„¶åæ ¹æ®è¿™ä¸ª target KLæ¥å†³å®š betaçš„å¤§å°ï¼Œä½†æ˜¯è¿™ç§æ–¹å¼é€šå¸¸éœ€è¦å¤§é‡çš„å®éªŒæ¯”è¾ƒã€‚

   ![](images/image-4.png)

6. **PTX Lossï¼š**&#x6700;åä¸€é¡¹æ˜¯é¢„è®­ç»ƒçš„ Lossï¼ŒåŒæ ·è¿™ä¸€é¡¹æœ‰ä¸€ä¸ªç³»æ•° $$\gamma$$ï¼ŒInstructGPT ç§å°† $$\gamma$$è®¾ä¸º 27.8ï¼Œä½†åœ¨æˆ‘çš„å®éªŒç»å†ä¸­ï¼Œé€šå¸¸è¿™ä¸€é¡¹åº”ç»“åˆ policy loss å’Œ pretrain loss çš„å¤§å°ç»¼åˆè®¾å®šã€‚åœ¨æˆ‘çš„å®éªŒä¸­ï¼Œgamma < 1 æ¨¡å‹æ‰èƒ½æ¯”è¾ƒå¥½çš„æ”¶æ•›

7. **Reward Normalizationï¼š**&#x5728; RLHF çš„è®­ç»ƒ&#x4E2D;**&#x20;reward normalization éå¸¸æœ‰åŠ©äºè®­ç»ƒçš„ç¨³å®šæ€§**ï¼Œæ¯•ç«Ÿæˆ‘ä»¬çš„ reward ä¸åƒåœ¨æ¸¸æˆç¯å¢ƒä¸­é‚£ä¹ˆè§„åˆ™ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªæ¨¡å‹å­¦å‡ºæ¥çš„ä¸­é—´å±‚è¾“å‡ºï¼ˆè¿™å°±æ„å‘³ç€è¾“å‡ºèŒƒå›´å¯èƒ½ä¼šå¾ˆå¤§ï¼‰

8. **Distributed Advantage Normalizationï¼š**&#x540C;&#x6837;**&#x20;Advantage Normalization ä¹Ÿæ˜¯PPOè®­ç»ƒä¸­å¸¸ç”¨çš„ç¨³å®šè®­ç»ƒçš„æŠ€æœ¯**ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ DeepSpeed ç­‰ç±»ä¼¼ DDP çš„è®­ç»ƒæ‰‹æ®µæ—¶åº”æ³¨æ„åšå…¨å±€æ ·æœ¬çš„ Advantage Normalizationï¼Œè€Œä¸æ˜¯æŸä¸ªDDPè¿›ç¨‹åªé’ˆå¯¹è‡ªå·±çš„æ ·æœ¬åšå½’ä¸€åŒ–ã€‚è¿™ä¸€ç‚¹ç›®å‰çš„ RLHF å¼€æºæ¡†æ¶éƒ½æ²¡æœ‰å……åˆ†è€ƒè™‘è¿›æ¥

9. **Model Initializationï¼š**&#x5177;ä½“æ¥è¯´ï¼Œç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¨¡å‹åˆå§‹åŒ– Actor æ¨¡å‹ï¼Œç”¨å¥–åŠ±æ¨¡å‹åˆå§‹åŒ– Critic æ¨¡å‹ï¼Œä»¥ç¡®ä¿é«˜æ•ˆçš„PPOè®­ç»ƒã€‚ä¹Ÿæ˜¯ä¸€èˆ¬é»˜è®¤çš„æ“ä½œ

10. **Adam Learning Rateï¼š**&#x41;ctor modelçš„Adamå­¦ä¹ ç‡å¤§çº¦æ˜¯SFTæ¨¡å‹å­¦ä¹ ç‡çš„ååˆ†ä¹‹ä¸€ã€‚ä¾‹å¦‚ï¼Œåœ¨OpenRLHFä¸­ï¼ŒSFT modelçš„Adamå­¦ä¹ ç‡ä¸º5eâˆ’6ï¼Œè€Œactor modelçš„å­¦ä¹ ç‡ä¸º5eâˆ’7ã€‚æ­¤å¤–ï¼Œè¯„è®ºè€…æ¨¡å‹çš„Adamå­¦ä¹ ç‡å¤§çº¦æ˜¯SFT modelçš„ä¸¤å€ï¼Œä¸€èˆ¬è®¾ç½®å­¦ä¹ ç‡ä¸º9eâˆ’6

11. **Value Function Loss Clippingï¼š**$$Loss_v = \max[(V_{\theta_t} - V_{targ})^2, (\text{clip}(V_{\theta_t}, V_{\theta_{t-1}} - \epsilon,  V_{\theta_{t-1}} + \epsilon) - V_{targ})^2]$$ï¼Œè¿™ä¸ªåœ¨å‰é¢3.3.3ç« èŠ‚ä¸­çš„ä»£ç éƒ¨åˆ†å·²ç»æ¶‰åŠåˆ°äº†

12. **Advantage Normalizationï¼š**&#x5728;ä½¿ç”¨å‡æ–¹æŸå¤±è®­ç»ƒå€¼ç½‘ç»œçš„è¿‡ç¨‹ä¸­ï¼Œç®—æ³•å¯¹ä¸€äº›å¤§å€¼æ•æ„Ÿã€‚æ ‡å‡†åŒ–ä¼˜åŠ¿å¯ä»¥å‡è½»è¿™äº›å¤§å€¼çš„å½±å“ã€‚å®è·µä¸­ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨Zåˆ†æ•°æ ‡å‡†åŒ–æ–¹æ³•ï¼Œğ‘Ÿ=(ğ‘Ÿâˆ’ğœ‡)/ğ›¿ï¼Œå…¶ä¸­ğœ‡æ˜¯ä¸€ä¸ªæ‰¹æ¬¡æ ·æœ¬çš„å‡å€¼ï¼Œğ›¿æ˜¯æ ‡å‡†å·®

## **å¥–åŠ±åˆ©ç”¨å’Œæ³›åŒ–é—®é¢˜**

> æŒç»­åœ¨ä¸€ä¸ªè®­ç»ƒé›†åˆä¸ŠåšRLè®­ç»ƒï¼Œå‘ç°train rewardæŒç»­åœ¨æ¶¨ï¼Œä½†æ˜¯åœ¨æµ‹è¯•é›†åˆä¸Šäººå·¥æµ‹è¯•æ•ˆæœä¼šä¸‹è·Œã€‚æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯Reward hackingå’ŒGeneralizationé—®é¢˜å¯¼è‡´çš„
>
> * **Reward hackingé—®é¢˜ï¼š**&#x5F53;train rewardåœ¨å¢é•¿çš„æ—¶å€™ï¼Œä½†reward modelè¢«hackäº†ï¼Œå› æ­¤çœ‹ä¼¼train rewardå¢é•¿ï¼Œä½†å…¶å®äººå·¥è¯„ä¼°çš„æ—¶å€™æ•ˆæœåœ¨ä¸‹é™ã€‚**3.3.2ç« èŠ‚ä¹Ÿæœ‰è®²è¿‡è¿™ä¸ªé—®é¢˜[ 3.3 RLHF åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ](https://kcnd4kn8i6ap.feishu.cn/wiki/TQqTwh2uwiSrqYktIPccTQOcn0g?fromScene=spaceOverview#share-Hhi0dbTEVoQIH9xdgxAcv7R1nye)**
>
> * **Generalizationé—®é¢˜ï¼š**&#x5F53;train rewardåœ¨å¢é•¿çš„æ—¶å€™ï¼Œå‡å¦‚test datasetçš„äººå·¥è¯„ä¼°ä¾ç„¶åœ¨ä¸Šæ¶¨ï¼Œé‚£ä¹ˆreward hackingæ²¡æœ‰å‘ç”Ÿã€‚æ­¤æ—¶æ­¤åˆ»å¦‚æœæµ‹è¯•é›†åˆä¸Šæ•ˆæœå´åœ¨ä¸‹é™ï¼Œé‚£ä¹ˆå°±æ˜¯æ¨¡å‹overfitè®­ç»ƒé›†åˆï¼Œæ³›åŒ–é—®é¢˜å‘ç”Ÿ

# **3.3.5 SFT ä¸ RLHF çš„æœ¬è´¨åŒºåˆ«**

## åˆ†æ

> * **SFTä¼˜åŒ–ç›®æ ‡**ï¼šç»™å®špromptå’Œå¯¹åº”outputï¼Œæœ€å¤§åŒ–LLMç­–ç•¥ $$Ï€_Î¸$$è¾“å‡ºoutputä¸­æ¯ä¸ªtokençš„æ¡ä»¶æ¦‚ç‡ï¼Œ**outputæ˜¯æ•°æ®é›†å¸¦çš„**
>
> * **RLHFä¼˜åŒ–ç›®æ ‡**ï¼šPPOä¸ºä¾‹ï¼Œå¯ä»¥ç®€åŒ–ä¸ºç»™å®špromptç›®æ ‡ä¸ºæœ€å¤§åŒ–ä¼˜åŠ¿ $$A_t$$ï¼Œé‡‡ä»¥é‡è¦æ€§é‡‡æ ·æ¯”å€¼ $$\frac{\pi_\theta(o_t|p,o_1,...,o_{t-1})}{\pi_{\theta_{old}}(o_t|p,o_1,...,o_{t-1})}$$ï¼Œ**outputæ˜¯ $$\pi_{\theta_{old}}$$é‡‡æ ·å¾—æ¥**
>
> **æ¢¯åº¦å…¬å¼**
>
> $$\nabla_\theta J_{SFT}(\theta) = \mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(o_t|p,o_1,...,o_{t-1})\right]$$
>
> $$\nabla_\theta J_{PPO}(\theta) = \mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T} A_t \nabla_\theta \log \pi_\theta(o_t|p,o_1,...,o_{t-1})\right]$$

## ç»“è®º

> * SFTæœ¬è´¨ä¸Šåœ¨è¿›è¡Œ**æ¨¡ä»¿å­¦ä¹ **ï¼Œä¸”æ‰€æœ‰tokenå¯¹åº”çš„**æ¢¯åº¦ç³»æ•°ä¸º1**ï¼Œå¯¹ç­–ç•¥ä¼˜åŒ–çš„è´¡çŒ®ç›¸åŒ
>
> * RLHF-PPOæœ¬è´¨ä¸Šæ˜¯åœ¨è¿›è¡Œ**æ¢ç´¢å’Œåˆ©ç”¨**ï¼Œé€šè¿‡è‡ªèº«é‡‡æ ·å¾—åˆ°è¾“å‡ºæ ·æœ¬ï¼Œç„¶å**åˆ©ç”¨ä¼˜åŠ¿å‡½æ•°è¯„åˆ¤å½“å‰åŠ¨ä½œç›¸å¯¹å¹³å‡åŠ¨ä½œçš„ä»·å€¼,è°ƒæ•´ç­–ç•¥ä¼˜åŒ–çš„æ–¹å‘å’Œå¹…åº¦**ï¼Œä¼˜åŠ¿çš„æ­£è´Ÿä»£è¡¨æ–¹å‘ï¼Œç»å¯¹å€¼ä»£è¡¨å¹…åº¦ï¼Œæ¢¯åº¦ç³»æ•°ä¸º$$A_t$$
>
> **PPOæ¢¯åº¦æ¨å¯¼**
>
> å‚ç…§DeepSeekMathé™„å½•çš„å‡è®¾ï¼Œç®€åŒ–å»æ‰clipï¼Œå¹¶åªæ›´æ–°ä¸€è½®ï¼Œè¿™æ ·æœ‰$$\pi_{\theta_{old}} = \pi_\theta$$
>
> **PPOæ¢¯åº¦æ¨å¯¼è¿‡ç¨‹**
>
> $$\nabla_\theta J_{PPO}(\theta) = \nabla_\theta \left(\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T} A_t \frac{\pi_\theta(o_t|p,o_{<t})}{\pi_{\theta_{old}}(o_t|p,o_{<t})}\right]\right)$$
>
>
>
> $$= \mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T} A_t \frac{\nabla_\theta \pi_\theta(o_t|p,o_{<t})}{\pi_{\theta_{old}}(o_t|p,o_{<t})}\right]$$
>
>
>
> $$= \mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T} A_t \frac{\pi_\theta(o_t|p,o_{<t})}{\pi_{\theta_{old}}(o_t|p,o_{<t})} \cdot \frac{\nabla_\theta \pi_\theta(o_t|p,o_{<t})}{\pi_\theta(o_t|p,o_{<t})}\right]$$
>
>
>
> $$= \mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T} A_t \nabla_\theta \log \pi_\theta(o_t|p,o_{<t})\right]$$

# **3.3.6 RLAIF åŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ **

> åŸºäºAIåé¦ˆï¼ˆRLAIFï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ä½¿ç”¨LLMæ¥ç”Ÿæˆåé¦ˆä¿¡å·æ¥æ‰©å±•RLHFèŒƒå¼ã€‚è¿™ç§æ–¹æ³•å¯ä»¥è¡¥å……æˆ–æ›¿ä»£äººç±»åé¦ˆï¼Œåœ¨**äººç±»æ ‡æ³¨ç¨€ç¼ºã€æ˜‚è´µæˆ–ä¸ä¸€è‡´**çš„ä»»åŠ¡ä¸­æä¾›æ›´å¯æ‰©å±•çš„ä½æˆæœ¬åå¥½æ•°æ®

![](images/image-5.png)

> å¤§è§„æ¨¡åº”ç”¨RLHFçš„ä¸»è¦æŒ‘æˆ˜åœ¨äº**RLHFä¾èµ–äººç±»ç”Ÿæˆçš„åå¥½æ ‡ç­¾**ï¼Œè¿™éœ€è¦å¤§é‡èµ„æºã€‚æ ‡æ³¨æ•°æ®çš„è¿‡ç¨‹æ—¢**æ—¶é—´å¯†é›†å‹åˆæ˜‚è´µ**ï¼Œå¹¶ä¸”äººç±»è¯„ä¼°äººå‘˜å¯èƒ½ä¼šå¼•å…¥**ä¸ä¸€è‡´**çš„åœ°æ–¹ã€‚è¿™äº›çº¦æŸå¤§å¤§é™åˆ¶äº†RLHFçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚RLAIF åˆ©ç”¨LLMä½œä¸ºåé¦ˆçš„æ¥æºï¼Œå‡å°‘äº†å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ï¼Œæä¾›äº†ä¼ ç»ŸRLHFçš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚è¿™ç§æ–¹æ³•å¯å®ç°è¿ç»­çš„åé¦ˆç”Ÿæˆï¼Œå¯æ˜¾è‘—æé«˜å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶ä¿ç•™äººç±»å¼•å¯¼æ¨¡å‹ä¼˜åŒ–çš„çµæ´»æ€§
>
> **RLHFå’ŒRLAIFä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºåé¦ˆçš„æ¥æºï¼šRLHFå–å†³äºäººç±»ç”Ÿæˆçš„åå¥½ï¼Œè€ŒRLAIFä½¿ç”¨AIç”Ÿæˆçš„åé¦ˆæ¥æŒ‡å¯¼ç­–ç•¥æ›´æ–°**ã€‚RLAIFå¯ä»¥å®ç°ä¸RLHFç›¸å½“ç”šè‡³ä¼˜äºRLHFçš„æ€§èƒ½ï¼ˆ**å…¶å®ç°åœ¨å¤§éƒ¨åˆ†åå¥½æ ‡ç­¾éƒ½ä¼šé€šè¿‡è’¸é¦GPTã€Claudeã€Geminiç­‰å¼ºæ¨¡å‹æ¥ç”Ÿæˆ**ï¼‰
>
> **AI feedback collectï¼š**&#x4C;LMåŸºäºé¢„å…ˆå®šä¹‰çš„æ ‡å‡†ï¼ˆPromptï¼‰ç”Ÿæˆåé¦ˆæ ‡ç­¾ï¼Œæ ‡å‡†å¯ä»¥åŒ…æ‹¬**ç‰¹å®šäºä»»åŠ¡çš„æŒ‡æ ‡ã€responseçš„é•¿åº¦ç­‰**

> ### **æ€»ç»“**
>
> RLAIFçš„æ–¹æ³•åˆ©ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¿™ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰è¶£çš„æ–°é€”å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹æ ‡æ³¨äººåŠ›æˆ–è€…å¼€å‘æ—¶é—´æ¯”è¾ƒç´§çš„æƒ…å†µä¸‹ã€‚è¿™ç§æ–¹æ³•çš„æ½œåœ¨ä¼˜åŠ¿åœ¨äºå®ƒå¯èƒ½å‡å°‘äº†å¯¹å¤§é‡äººå·¥æ ‡æ³¨çš„éœ€æ±‚ï¼Œä»è€ŒåŠ é€Ÿäº†æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²ã€‚åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€äº›éœ€è¦æ³¨æ„çš„é—®é¢˜ï¼š
>
> * **æ•°æ®è´¨é‡ï¼š**&#x867D;ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿæˆçš„æ•°æ®è´¨é‡ä»ç„¶å¯èƒ½å—åˆ°é™åˆ¶ã€‚æ¯”å¦‚ï¼ŒLLMå¯èƒ½ä¼šäº§ç”Ÿä¸åŸå§‹å†…å®¹ä¸ä¸€è‡´çš„è¾“å‡ºæˆ–åŒ…å«é”™è¯¯çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œç¡® ä¿AIç”Ÿæˆçš„åå¥½æ•°æ®çš„è´¨é‡è‡³å…³é‡è¦
>
> * **è®¡ç®—æˆæœ¬ï¼š**&#x867D;ç„¶RLAIFæ–¹æ³•å¯èƒ½å‡å°‘äº†å¯¹äººå·¥æ ‡æ³¨çš„éœ€æ±‚ï¼Œä½†ä½¿ç”¨LLMç”Ÿæˆæ•°æ®ä»ç„¶å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚å¹¶ä¸”å¯èƒ½è¦æ±‚éå¸¸å¤§çš„æ•ˆæœéå¸¸å¥½çš„LLMæ¥è¿›è¡Œåå¥½æ•°æ®çš„æ ‡æ³¨ï¼Œæ¯”å¦‚ä½¿ç”¨gpt3.5æˆ–è€…gpt4æ¥å£è¿›è¡Œæ ‡æ³¨

# **3.3.7 GRPO**

> PPOä½œä¸ºä¸€ç§Onlineå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œéœ€**è¦å¤§æ¨¡å‹åœ¨ä¸ç¯å¢ƒï¼ˆä¸€äº›questionsï¼‰çš„äº¤äº’ï¼ˆæ ¹æ®questionè¾“å‡ºanswerï¼‰ä¸­è¿›è¡Œé‡‡æ ·å’Œç­–ç•¥æ›´æ–°**ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦**å››ä¸ªæ¨¡å‹Actorã€Criticã€Rewardå’ŒReference Modelï¼Œå…¶ä¸­Reward modelæ˜¯æå‰è®­ç»ƒå¥½çš„ï¼ŒPPOè¿‡ç¨‹ä¸­åªæœ‰Actorå’ŒCriticéœ€è¦è®­ç»ƒï¼ŒRewardå’ŒReferenceåªæ¨ç†**ã€‚**å››ä¸ªæ¨¡å‹å¸¦æ¥çš„è®­ç»ƒæ¨ç†èµ„æºéœ€æ±‚å·¨å¤§ï¼Œå†åŠ ä¸ŠPPOä½œä¸ºä¸€ç§on-policyå¼ºåŒ–å­¦ä¹ ç®—æ³•æœ¬èº«è®­ç»ƒå°±ä¸å¤ªç¨³å®šï¼Œä¸€äº›å·¥ä½œè¯•å›¾å¯¹å¤§æ¨¡å‹RLHF-PPOç®—æ³•è¿›è¡Œæ”¹è¿›**

## **å¯¹PPOä¼˜åŒ–çš„æ€è€ƒ**

> ### **Group Relative Policy Optimization**
>
> **è®ºæ–‡ï¼š*DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models***
>
> **é“¾æ¥ï¼šhttps://arxiv.org/pdf/2402.03300**
>
> å›é¡¾ä¸€ä¸‹PPOç®—æ³•ï¼Œæˆ‘ä»¬ä¸ºäº†è®¡ç®—`Actor_loss`è¦ç”¨åˆ°Referenceã€Rewardå’ŒValueä¸‰ä¸ªmodelï¼Œå…¶ä¸­**Reference modelå‚ä¸KL rewardçš„è®¡ç®—ï¼ŒRewardå’ŒValue modelå‚ä¸GAEçš„è®¡ç®—**
>
> $$
> \text{Actor\_loss} = -\min \left(   \frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}\cdot A_t(s_t,a_t), \text{clip} \left( \frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}, 1-\epsilon, 1+\epsilon \right) \cdot A_t(s_t,a_t)\right)
>  $$
>
> $$  \text{Critic\_loss} = (r_t + \gamma \cdot V_{t+1} - V_t)^2  $$
>
> $$
>   A_t^{\text{GAE}} = 
>
>  \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} =\sum_{l=0}^{\infty} (\gamma \lambda)^l \left( r_{t+l} + V_{t+l+1}-V_{t+l}\right)$$

> ### **PPOä¼˜åŒ–çš„æ€è€ƒ**
>
> åŠ ä¸ŠActor modelï¼Œ**PPOç®—æ³•ä¸€å…±éœ€è¦å››ä¸ªæ¨¡å‹**ï¼Œåœ¨**ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼ŒActorå’ŒCriticä»¥åŠReferenceéƒ½æ˜¯ç®€å•çš„ç½‘ç»œå®ç°ï¼ŒRewardæ˜¯ç¯å¢ƒè‡ªå¸¦çš„äººä¸ºè®¾è®¡å¥½çš„ï¼ˆå¯ä»¥ç†è§£ä¸ºrule-based rewardï¼‰**ï¼Œæ‰€ä»¥å¹¶ä¸å­˜åœ¨å¤§è§„æ¨¡çš„èµ„æºéœ€æ±‚ï¼Œè€Œåˆ°äº†LLMè¿™é‡Œï¼Œ**æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯åŸºäºLLMï¼ˆSFTï¼‰æ¨¡å‹åˆå§‹åŒ–æˆ–è€…æ”¹è¿›çš„ï¼Œé‚£ä¹ˆå³ä½¿åœ¨PPOè®­ç»ƒè¿‡ç¨‹ä¸­åªæœ‰Actorå’ŒCriticéœ€è¦æ›´æ–°å‚æ•°ï¼Œå››ä¸ªæ¨¡å‹çš„æ¨ç†å’Œè®­ç»ƒå°±éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä»¥åŠå››ä¸ªæ¨¡å‹ä¹Ÿä¼šå¸¦æ¥æ›´å¤šçš„ç´¯ç§¯è¯¯å·®**
>
> **ä»æ¨¡å‹æ•°é‡ä¸Šå…¥æ‰‹æ€è€ƒå“ªäº›æ¨¡å‹æ˜¯å¯ä»¥å»æ‰çš„**
>
> * é¦–å…ˆ**Actor modelæ˜¯ç­–ç•¥æœ¬èº«å¿…é¡»æœ‰**
>
> * å…¶æ¬¡Reference modelå¯ä»¥å»æ‰ï¼Œè¿™æ ·å°±é€€åŒ–ä¸ºæœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼Œä½†æ˜¯**Reference modelå¯ä»¥é˜²æ­¢æ¨¡å‹ç­–ç•¥å¤ªåç¦»referenceç­–ç•¥ï¼Œæ‰€ä»¥ä¸€èˆ¬è¦æœ‰**
>
> * ç„¶åå°±æ˜¯Reward modelï¼Œå¦‚æœæ›¿æ¢æˆrule-based rewardï¼Œé‚£å°±ä¸éœ€è¦å•ç‹¬è®­ç»ƒä¸€ä¸ªReward modelï¼Œä½†æ˜¯ä¸ç®¡rule-based è¿˜æ˜¯model-based rewardï¼Œ**rewardåœ¨å…¬å¼çš„è®¡ç®—ä¸­æ˜¯ä¸å¯æˆ–ç¼ºçš„**
>
> * æœ€åå°±æ˜¯Criticæˆ–è€…è¯´Value modelï¼Œæˆ‘ä»¬**èƒ½ä¸èƒ½é€šè¿‡æŸç§æ›¿æ¢ï¼Œå»æ‰Critic modelï¼Ÿ**
>
> å›é¡¾ç­–ç•¥æ¢¯åº¦ã€Actor-Criticä»¥åŠPPOç®—æ³•ï¼Œéƒ½æ˜¯åˆ©ç”¨ä¸€ä¸ª**åŸºäºå¥–åŠ±çš„å€¼æ¥å¼•å¯¼ç­–ç•¥çš„ä¼˜åŒ–æ–¹å‘**ï¼Œå¸¸ç”¨çš„ä¾¿æ˜¯ç”¨ä¼˜åŠ¿å‡½æ•°Advantageæ¥æŒ‡å¯¼ï¼Œ**ä¼˜åŠ¿å‡½æ•°åŸå§‹å®šä¹‰ä¸º $$A_t = Q(s_t,a_t)-V(s_t)$$å³å½“å‰åŠ¨ä½œçš„ä»·å€¼æ¯”å¹³å‡åŠ¨ä½œä»·å€¼é«˜å‡ºçš„éƒ¨åˆ†**ï¼Œä¸‹é¢çœ‹çœ‹GRPOå…·ä½“æ˜¯æ€ä¹ˆåšå»æ‰Critic modelçš„

## **GRPOçš„ä¼˜åŠ¿å‡½æ•°ä¼°è®¡**

> GRPOé€šè¿‡**å¯¹ç›¸åŒé—®é¢˜ $$q$$ç”¨ $$\pi_{\theta_{old}}$$é‡‡æ ·çš„å¤šä¸ªä¸€ç»„è¾“å‡º $$\{o_1, o_2, \ldots, o_G\}$$ï¼Œç„¶åReward modelå¯¹è¿™äº›å›ç­”éƒ½ç»™å‡ºå¥–åŠ±å€¼**ï¼Œç„¶åç»™å‡ºä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡

![](images/image-6.png)

![](images/image-7.png)

> ### **ORM ç»“æœå¥–åŠ±ç›‘ç£RL**
>
> **å³ä½¿ç”¨ä¼ ç»Ÿçš„ORMï¼ˆOutcome Reward Modelï¼‰æ¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæ¨¡å‹å¯¹ä¸€ç»„è¾“å‡ºç”Ÿæˆä¸€ç»„å¥–åŠ±å€¼ $$\bold{r}=\{r_1, r_2,\cdots,r_G\}$$ï¼Œç„¶åé€šè¿‡ä¸‹å¼å¯¹ä¼˜åŠ¿å‡½æ•°è¿›è¡Œä¼°è®¡ï¼š**
>
> **$$\hat{A}_{i,t}=\hat{r_i}=\frac{r_i-\text{mean}(\bold{r})}{\text{std}(\bold{r})}$$**
>
> **ä¸Šè¿°å¼å­å°±æ˜¯å¯¹é‡‡æ ·çš„è¿™ä¸€ç»„è¾“å‡ºçš„å¥–åŠ±è®¡ç®—å½’ä¸€åŒ–å¥–åŠ±ä½œä¸ºè¾“å‡ºçš„ä¼˜åŠ¿å‡½æ•°ï¼ŒåŒæ—¶èµ‹å€¼ç»™è¾“å‡ºä¸­æ¯ä¸€ä¸ªtokenä½œä¸ºè¯¥tokenå¤„çš„ä¼˜åŠ¿å‡½æ•°ã€‚ç®€å•ç†è§£å°±æ˜¯å½“å‰ç¬¬$$i$$ä¸ªè¾“å‡ºçš„å¥–åŠ± $$r_i(q,o_i)$$æ¯”æ‰€æœ‰è¾“å‡ºçš„å¥–åŠ±å¹³å‡å€¼é«˜å‡ºçš„ä¼˜åŠ¿ï¼ˆå¯ä»¥ä¸ºè´Ÿï¼‰**
>
> ### **ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆåšï¼Ÿ**
>
> å›é¡¾å¼ºåŒ–å­¦ä¹ ä¸­ä»·å€¼å‡½æ•°çš„å®šä¹‰ä¸ºå›æŠ¥ï¼ˆæŠ˜æ‰£ç´¯ç§¯å¥–åŠ±ï¼‰çš„æœŸæœ›ï¼š
>
> $$ V(s) = \mathbb{E}_\pi [G_t | s_t = s] = \mathbb{E}_\pi[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots|s_t=s]$$
>
> $$Q(s,a)=\mathbb{E}_\pi \left[G_t|s_t=s,a_t=a \right]=\mathbb{E}_\pi[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots|s_t=s,a_t=a]$$
>
> åœ¨ç»“æœå¥–åŠ±ç›‘ç£RLçš„è®¾å®šä¸‹ï¼Œå°±æ˜¯ä»é—®é¢˜$$q$$åˆ°è¾“å‡º $$o$$ï¼ŒçŠ¶æ€æ˜¯ $$q$$ï¼ŒåŠ¨ä½œæ˜¯ $$o$$ï¼Œåªæœ‰ä¸€æ­¥å¥–åŠ±å³ $$r(q,o)$$ï¼Œæ‰€ä»¥ä»·å€¼å‡½æ•°å¯ä»¥è¿›ä¸€æ­¥å†™æˆï¼š
>
> $$ V(s) = \mathbb{E}_\pi[R_t|s_t=s]$$
>
> $$Q(s,a)=\mathbb{E}_\pi[R_t|s_t=s,a_t=a]=R(s,a)$$
>
> æ‰€ä»¥ç”¨ $$r_i(q,o_i)$$ä¼°è®¡ $$Q(q,o_i)$$ï¼Œç”¨ $$\text{mean}(\bold{r})=\frac{1}{G}\sum_i^G r(q,o_i)$$æ¥ä¼°è®¡ $$V(q)$$æ˜¯åˆç†çš„ï¼Œå‰ææ˜¯groupé‡‡æ ·è¶³å¤Ÿå¤šï¼Œç„¶åå¾—åˆ°ä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡ $$A(q,o_i)=Q(q,o_i)-V(q)$$ã€‚é™¤ä»¥ $$\text{std}(\bold{r})$$æ˜¯ä¸ºäº†å½’ä¸€åŒ–
>
> ä½†æ˜¯è¿™æ ·å¾—åˆ°çš„ä¼˜åŠ¿æ˜¯æ•´ä¸ª $$(q,o_i)$$çš„ä¼˜åŠ¿å€¼ï¼Œ**GRPOåœ¨è¿™é‡Œåˆ™å°†æ•´ä¸ªä¼˜åŠ¿å€¼ç”¨åœ¨äº†è¾“å‡º$$o_i$$æ¯ä¸€ä¸ªçš„æ¯ä¸€ä¸ªtokenä¸Šï¼ˆå¹¿æ’­broadcastæ“ä½œï¼‰**ï¼Œç›¸å½“äºå½“å‰policyå»æ›´æ–°è¾“å‡º $$o_i$$ä¸­æ¯ä¸€ä¸ªtokençš„æ¡ä»¶è¾“å‡ºæ¦‚ç‡ $$\pi_\theta(a_t|s_t)$$çš„**æ›´æ–°å¹…åº¦å’Œæ–¹å‘æ˜¯ä¸€è‡´çš„**

![](images/diagram-7.png)

> ### **PRM è¿‡ç¨‹å¥–åŠ±ç›‘ç£RL**
>
> ä½¿ç”¨PRMæ¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒåŒæ ·é‡‡æ ·groupè¾“å‡ºï¼Œå¯¹åº”çš„ä¸€ç»„å¥–åŠ±å€¼ä¸º :
>
> $$
> \mathbf{R}=\left\{\left\{r_{1}^{\mathrm{index}(1)},\cdots,r_{1}^{\mathrm{index}(K_1)}\right\},\cdots,\left\{r_{G}^{\mathrm{index}(1)},\cdots,r_{G}^{\mathrm{index}(K_G)}\right\}\right\}
>  $$
>
> å…¶ä¸­ $$index(j)$$ä¸ºç¬¬ $$j$$ä¸ªæ­¥éª¤çš„ end tokençš„indexï¼Œ $$K_i$$æ˜¯ç¬¬ $$i$$ä¸ªè¾“å‡ºçš„æ­¥éª¤ä¸ªæ•°ï¼ŒGRPOè®¡ç®—ä¼˜åŠ¿å¦‚ä¸‹ï¼š
>
> $$
> \hat{A}_{i,t}=\sum_{\mathrm{index}(j)\geq t} \widetilde{r}_{i}^{\mathrm{index}(j)} =\sum_{\mathrm{index}(j)\geq t} \frac{{r}_{i}^{\mathrm{index}(j)}-\text{mean}(\bold{r})}{\text{std}(\bold{r})}$$
>
> å¯¹äºPRMå¼ºåŒ–å­¦ä¹ çš„æƒ…å†µï¼ŒGRPOå¹¶ä¸èƒ½ä¸¥è°¨çš„ä»å¼ºåŒ–å­¦ä¹ ä»·å€¼å‡½æ•°çš„å®šä¹‰æ¨å¯¼å‡ºï¼Œæ‰€ä»¥è¿˜æ˜¯æ ¹æ®å…¬å¼æ¥ç®€è¦ç†è§£ä¸€ä¸‹ã€‚å¯¹äºæ‰€æœ‰é‡‡æ ·çš„è¾“å‡ºä¸­çš„æ‰€æœ‰æ­¥éª¤ $$\{\text{step}_{i,j}\}, i\in G,j\in K_i $$ï¼Œæ ¹æ®PRMå¾—åˆ°å¥–åŠ±å€¼ $$r^{\text{index}(j)}_i$$ï¼Œç„¶åè®¡ç®—å¾—åˆ°æ¯ä¸€ä¸ªæ­¥éª¤ç›¸å¯¹äºæ‰€æœ‰æ­¥éª¤çš„é«˜å‡ºçš„ä¼˜åŠ¿ï¼ˆä¹Ÿå¯ä»¥åªç†è§£ä¸ºè¿›è¡Œäº†ä¸€æ­¥reward normalizationï¼‰ï¼Œä½†æ˜¯è¿™åªæ˜¯é’ˆå¯¹å•æ­¥æ­¥éª¤è®¡ç®—çš„ï¼Œå®é™…ä¸Šæˆ‘ä»¬çš„è¾“å‡ºæˆ–è€…è¯´responseæ˜¯ç”±å¤šä¸ªæ­¥éª¤æ‹¼æ¥çš„ï¼Œæ‰€ä»¥å½“å‰æ­¥éª¤çš„ä¼˜åŠ¿åº”å½“åŒ…å«åç»­çš„æœŸæœ›ï¼Œå³å½“å‰æ­¥éª¤çš„ä¼˜åŠ¿å€¼ä¸ºå½“å‰æ­¥éª¤ä»¥åŠåç»­æ­¥éª¤ä¼˜åŠ¿å€¼çš„å’Œ

![](images/diagram-8.png)

## **GRPOæ•´ä½“æ›´æ–°å…¬å¼å’Œä»£ç **

> æœ€åä¸ç®¡æ˜¯ORMè¿˜æ˜¯PRMè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œéƒ½ç”¨ä¸‹å¼è¿›è¡Œç­–ç•¥æ›´æ–°ï¼š
> $$
>\mathcal{J}_{GRPO}(\theta)=\mathbb{E}[q\sim P(Q),\{o_i\}_{i = 1}^{G}\sim\pi_{\theta_{old}}(O|q)]\\
> \quad\quad\frac{1}{G}\sum_{i = 1}^{G}\frac{1}{|o_i|}\sum_{t = 1}^{|o_i|}\left\{\min\left[\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}\hat{A}_{i,t},\mathrm{clip}\left(\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})},1 - \varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right]-\beta\mathbb{D}_{KL}[\pi_{\theta}||\pi_{ref}]\right\}
> $$
>
>  æ³¨æ„è¿™é‡Œçš„KLæ•£åº¦é‡‡ç”¨äº†ä¸€ä¸ªæ— åä¼°è®¡ç­‰å¼ï¼ŒåŠ åœ¨ç­–ç•¥ä¼˜åŒ–é¡¹åé¢ï¼ŒPPOåˆ™æ˜¯å°†KLæ•£åº¦çº¦æŸé¡¹åŠ åˆ°å¥–åŠ±å€¼å½“ä¸­ï¼š
>
> $$\text{GRPO}:\quad
>\mathbb{D}_{KL}[\pi_{\theta}||\pi_{ref}]=\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-\log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-1
> $$
> $$\text{PPO}:\quad
> r_t = r_{\varphi}(q, o_{\leq t}) - \beta \log \frac{\pi_{\theta}(o_t|q, o_{<t})}{\pi_{ref}(o_t|q, o_{<t})}
>$$
> 
> **GRPOä»£ç å®ç°ï¼Œtrlåº“çš„ç‰ˆæœ¬ï¼Œä»£ç ç»è¿‡ç²¾ç®€ï¼Œä¿ç•™æ ¸å¿ƒç®—æ³•å®ç°éƒ¨åˆ†ï¼Œå¸®åŠ©ç†è§£GRPOï¼Œå…·ä½“éœ€è¦è¿è¡Œä»£ç è¯·çœ‹trl/trainer/grpo\_trainer.py**

```python
def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
    if return_outputs:
        raise ValueError("The GRPOTrainer does not support returning outputs")
    # Compute the per-token log probabilities for the model

    prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
    completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
    input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
    attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
    logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens

    per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)

    # Compute the KL divergence between the model and the reference model
    ref_per_token_logps = inputs["ref_per_token_logps"]
    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1

    # x - x.detach() allows for preserving gradients from x
    advantages = inputs["advantages"]
    per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
    per_token_loss = -(per_token_loss - self.beta * per_token_kl)
    loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()

    # Log the metrics
    completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()
    self._metrics["completion_length"].append(completion_length)

    mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
    self._metrics["kl"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())

    return loss
def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
    # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
    logits = model(
        input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1
    ).logits  # (B, L, V)
    logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred

    # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.
    per_token_logps = []
    for logits_row, input_ids_row in zip(logits, input_ids[:, -logits_to_keep:]):
        log_probs = logits_row.log_softmax(dim=-1)
        token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)
        per_token_logps.append(token_log_prob)
    return torch.stack(per_token_logps)
def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[str, Union[torch.Tensor, Any]]:
    device = self.accelerator.device
    prompts = [x["prompt"] for x in inputs]
    prompts_text = [maybe_apply_chat_template(example, self.processing_class)["prompt"] for example in inputs]
    prompt_inputs = self.processing_class(
        prompts_text, return_tensors="pt", padding=True, padding_side="left", add_special_tokens=False
    )
    prompt_inputs = super()._prepare_inputs(prompt_inputs)
    prompt_ids, prompt_mask = prompt_inputs["input_ids"], prompt_inputs["attention_mask"]

    if self.max_prompt_length is not None:
        prompt_ids = prompt_ids[:, -self.max_prompt_length :]
        prompt_mask = prompt_mask[:, -self.max_prompt_length :]
        
    # Regular generation path
    with unwrap_model_for_generation(self.model, self.accelerator) as unwrapped_model:
        prompt_completion_ids = unwrapped_model.generate(
            prompt_ids, attention_mask=prompt_mask, generation_config=self.generation_config
        )

    # Compute prompt length and extract completion ids
    prompt_length = prompt_ids.size(1)
    prompt_ids = prompt_completion_ids[:, :prompt_length]
    completion_ids = prompt_completion_ids[:, prompt_length:]
    prompt_mask = prompt_mask.repeat_interleave(self.num_generations, dim=0)

    # Mask everything after the first EOS token
    is_eos = completion_ids == self.processing_class.eos_token_id
    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)
    eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]
    sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)
    completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()

    # Concatenate prompt_mask with completion_mask for logit computation
    attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)

    logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens

    with torch.inference_mode():
        ref_per_token_logps = self._get_per_token_logps(
            self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep
        )
    # Decode the generated completions
    completions = self.processing_class.batch_decode(completion_ids, skip_special_tokens=True)
    if is_conversational(inputs[0]):
        completions = [[{"role": "assistant", "content": completion}] for completion in completions]

    # Compute the rewards
    prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]  # repeat prompts

    rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)
    for i, (reward_func, reward_processing_class) in enumerate(
        zip(self.reward_funcs, self.reward_processing_classes)
    ):
        if isinstance(reward_func, nn.Module):  # Module instead of PretrainedModel for compat with compiled models
            if is_conversational(inputs[0]):
                messages = [{"messages": p + c} for p, c in zip(prompts, completions)]
                texts = [apply_chat_template(x, reward_processing_class)["text"] for x in messages]
            else:
                texts = [p + c for p, c in zip(prompts, completions)]
            reward_inputs = reward_processing_class(
                texts, return_tensors="pt", padding=True, padding_side="right", add_special_tokens=False
            )
            reward_inputs = super()._prepare_inputs(reward_inputs)
            with torch.inference_mode():
                rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]  # Shape (B*G,)
        else:
            # Repeat all input columns (but "prompt" and "completion") to match the number of generations
            reward_kwargs = {key: [] for key in inputs[0].keys() if key not in ["prompt", "completion"]}
            for key in reward_kwargs:
                for example in inputs:
                    # Repeat each value in the column for `num_generations` times
                    reward_kwargs[key].extend([example[key]] * self.num_generations)
            output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)
            rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)

    # Sum the rewards from all reward functions
    rewards = rewards_per_func.sum(dim=1)

    # Compute grouped-wise rewards
    mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
    std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)

    # Normalize the rewards to compute the advantages
    mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
    std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
    advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)
   
    return {"prompt_ids": prompt_ids,"prompt_mask": prompt_mask,"completion_ids": completion_ids,"completion_mask": completion_mask,"ref_per_token_logps": ref_per_token_logps,
        "advantages": advantages,
    }
```

## **GRPOä»£ç çš„å°é—®é¢˜**

> ä¸Šè¿°ä»£ç compute\_losså‡½æ•°ä¸­åœ¨è®¡ç®—é‡è¦æ€§é‡‡æ ·çš„æ—¶å€™ç›´æ¥é‡‡ç”¨äº†ä¸‹é¢çš„å¼å­è®¡ç®—ï¼Œå¹¶ä¸”ä¹Ÿæ²¡æœ‰è¿›è¡Œ`clip`æ“ä½œï¼š
>
> **`torch.exp(per_token_logps - per_token_logps.detach())`**
>
> æˆ‘ä»¬å›é¡¾GRPOå…¬å¼ä¸­çš„é‡è¦æ€§é‡‡æ ·ï¼š
> $$
>\mathcal{J}_{GRPO}(\theta)=\mathbb{E}[q\sim P(Q),\{o_i\}_{i = 1}^{G}\sim\pi_{\theta_{old}}(O|q)]\\
> \quad\quad\frac{1}{G}\sum_{i = 1}^{G}\frac{1}{|o_i|}\sum_{t = 1}^{|o_i|}\left\{\min\left[\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}\hat{A}_{i,t},\mathrm{clip}\left(\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})},1 - \varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right]-\beta\mathbb{D}_{KL}[\pi_{\theta}||\pi_{ref}]\right\}
> $$
>
>  TRLåœ¨å®ç°çš„æ—¶å€™ç›´æ¥ä½¿ç”¨äº†å½“å‰çš„$$\pi_\theta$$ä½œä¸º $$\pi_{\theta_{old}}$$ï¼Œå³æ–°æ—§ç­–ç•¥ä¸€æ ·äº†ï¼é‚£ä¹ˆè¿™æ ·åšæœ‰æ²¡æœ‰é—®é¢˜å‘¢ï¼Ÿ
>
> æˆ‘ä»¬å¯ä»¥å†å›é¡¾ä¸€ä¸‹ä¸€èˆ¬çš„PPOä»£ç ä¸­é‡è¦æ€§é‡‡æ ·çš„ç›¸å…³å¤„ç†ï¼Œè¿™é‡Œæˆ‘ç”¨ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœºæ™¯çš„ä»£ç ç¤ºä¾‹ä¸€ä¸‹ï¼š

```python
old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()
for _ in range(self.epochs):
    log_probs = torch.log(self.actor(states).gather(1, actions))
    ratio = torch.exp(log_probs - old_log_probs)
    surr1 = ratio * advantage
    surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # æˆªæ–­
    actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPOæŸå¤±å‡½æ•°
    critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))
    self.actor_optimizer.zero_grad()
    self.critic_optimizer.zero_grad()
    actor_loss.backward()
    critic_loss.backward()
    self.actor_optimizer.step()
    self.critic_optimizer.step()
```

> å¯ä»¥çœ‹åˆ°ï¼Œ**ä¸€èˆ¬çš„é‡è¦æ€§é‡‡æ ·å¤„ç†ï¼Œä¼šå¯¹ä¸€æ‰¹æ ·æœ¬æ›´æ–°å¤šä¸ªepoch**ï¼Œå…¶ä¸­åˆå§‹åŒ–ç¬¬ä¸€ä¸ªepochçš„æ—¶å€™æ–°æ—§ç­–ç•¥æ˜¯ä¸€æ ·çš„ï¼Œå¯ä»¥å¾—åˆ°é‡è¦æ€§é‡‡æ ·ä¸º $$\frac{\pi_\theta}{\pi_{\theta_{old}}}=1$$ï¼Œä½†æ˜¯åç»­ä¼šå¤šæ›´æ–°å‡ æ¬¡å½“å‰ç­–ç•¥ï¼Œæ—§ç­–ç•¥ä¸åŠ¨ï¼Œæ‰€ä»¥å°±ä¸ä¼šå‡ºç°æ–°æ—§ç­–ç•¥ä¸€æ ·çš„æƒ…å†µï¼Œè¿™æ ·å¯ä»¥æé«˜å½“å‰æ‰¹æ¬¡æ•°æ®çš„æ ·æœ¬æ•ˆç‡
>
> å›è¿‡å¤´æ¥å†çœ‹TRLçš„GRPOå®ç°ï¼Œç›¸å½“äºå°‘äº†æ›´æ–°å¤šä¸ªepochè¿™ä¸ªæ“ä½œï¼Œåªæ›´æ–°ä¸€æ¬¡ï¼Œæ ·æœ¬æ•ˆç‡ä½ã€‚ç”±äºå½“å‰ç­–ç•¥å’Œæ—§ç­–ç•¥ä¸€æ ·ï¼Œæ‰€ä»¥ä¹Ÿå°±ä¸éœ€è¦`clip`æ“ä½œäº†ï¼Œå› ä¸ºé‡è¦æ€§æƒé‡ä¸º1ã€‚ä½¿ç”¨detachæ“ä½œç›®çš„æ˜¯ä¸ºäº†ä½¿æ¢¯åº¦èƒ½å¤Ÿæ­£ç¡®å›ä¼ å›å½“å‰ç­–ç•¥ç½‘ç»œï¼ŒåŸå§‹ä»£ç ä¸­çš„æ³¨é‡Šï¼š
>
> > **x - x.detach() allows for preserving gradients from x**
>
> æ‰€ä»¥TRLçš„å®ç°ç­‰ä»·ä¸‹é¢çš„å¼å­ï¼Œåªé ä¼˜åŠ¿å‡½æ•°å’ŒKLé¡¹æä¾›æ¢¯åº¦ä¿¡å·
>
> $$
> \mathcal{J}_{GRPO}(\theta)=\mathbb{E}[q\sim P(Q),\{o_i\}_{i = 1}^{G}\sim\pi_{\theta_{old}}(O|q)]
> \frac{1}{G}\sum_{i = 1}^{G}\frac{1}{|o_i|}\sum_{t = 1}^{|o_i|}\left\{\hat{A}_{i,t}-\beta\mathbb{D}_{KL}[\pi_{\theta}||\pi_{ref}]\right\}
> $$

```python
def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
    # Compute the per-token log probabilities for the model
    prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
    completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
    input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
    attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
    logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens
    per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
    # Compute the KL divergence between the model and the reference model
    ref_per_token_logps = inputs["ref_per_token_logps"]
    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
    # x - x.detach() allows for preserving gradients from x
    advantages = inputs["advantages"]
    per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
    per_token_loss = -(per_token_loss - self.beta * per_token_kl)
    loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
```

> ä¸è¿‡è¿™ä¸ªé—®é¢˜æœ€è¿‘å¾—åˆ°äº†ä¸€äº›ä¿®æ­£ï¼Œç°åœ¨å»çœ‹TRLçš„æºç ï¼Œå·²ç»åŠ ä¸Šäº†`num_iterations`å‚æ•°å’Œä¸€äº›å…¶ä»–æ”¹åŠ¨ï¼Œä¸‹é¢åªçœ‹ä¸€ä¸‹è®¡ç®—lossçš„æ—¶å€™çš„æ”¹åŠ¨ï¼Œå…¶ä»–å…·ä½“æ”¹åŠ¨å‚è€ƒ TRL åº“ 2025.2.21å·çš„ `Commit e5ae703`
>
> https://github.com/huggingface/trl/commit/e5ae703d352b29537159180087ef8bd4b41bf625
>
> å¯ä»¥çœ‹åˆ°å¤šæ­¥æ›´æ–°å’Œ`clip`æ“ä½œéƒ½æ›´æ–°ä¸Šå»äº†

```python
# Compute the loss
advantages = inputs["advantages"]
# When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's computation (see
# _generate_and_score_completions) and use per_token_logps.detach() instead.
old_per_token_logps = inputs["old_per_token_logps"] if self.num_iterations > 1 else per_token_logps.detach()
coef_1 = torch.exp(per_token_logps - old_per_token_logps)
coef_2 = torch.clamp(coef_1, 1 - self.epsilon, 1 + self.epsilon)
per_token_loss1 = coef_1 * advantages.unsqueeze(1)
per_token_loss2 = coef_2 * advantages.unsqueeze(1)
per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
```

## å®é™…è®­ç»ƒä¸­çš„é—®é¢˜åŠè§£æ³•

### GRPOè®­ç»ƒæ—¶ä¸ºä»€ä¹ˆä¼šäº§ç”Ÿå¤§é‡çš„é‡å¤å†…å®¹

> #### èƒŒæ™¯
>
> LLMé€šè¿‡GRPO è¿™ç§ On-Policy æ–¹æ³•è¿›è¡ŒRLè®­ç»ƒæ—¶ï¼Œæ™®éå­˜åœ¨Policy Collapseé—®é¢˜ï¼Œå³ç­–ç•¥ç†µæ€¥å‰§ä¸‹é™ã€‚è¿™ä¸€ç°è±¡ç›´æ¥å¯¼è‡´æ¨¡å‹ç”Ÿæˆå†…å®¹çš„æ¢ç´¢æ€§ä¸§å¤±å’Œå¤šæ ·æ€§ä¸è¶³ï¼Œæœ€ç»ˆè¡¨ç°ä¸º**å¤§é‡é‡å¤æˆ–æ¨¡å¼åŒ–çš„è¾“å‡º**ã€‚å…·ä½“è¡¨ç°æ¥è¯´ï¼Œå¯èƒ½æœ‰ä¸‹é¢ä¸¤ç§æƒ…å†µï¼š
>
> **Entropy Collapse**ï¼šGRPOé€šå¸¸é‡‡ç”¨ Critic-Freeè®¾è®¡ï¼Œä½¿å…¶ç­–ç•¥æ¢¯åº¦å¯¹é•¿åºåˆ—ä»»åŠ¡ä¸­é«˜æ–¹å·®ã€ç¨€ç–çš„å¥–åŠ±ä¿¡å·æå…¶æ•æ„Ÿã€‚ç­–ç•¥ä¸ºè§„é¿è¿™ç§ä¸ç¡®å®šæ€§ï¼Œä¼šå€¾å‘äºå¿«é€Ÿæ”¶ç¼©è‡³ä½ç†µã€é«˜ç¡®å®šæ€§çš„æ¨¡å¼ï¼Œå³ç­–ç•¥ç†µå•è°ƒé€’å‡ã€‚  &#x20;
>
> **Reward Hacking**ï¼šé‡å¤æ€§å†…å®¹æ˜¯æ¨¡å‹æœ€å¤§åŒ–RMè¯„åˆ†çš„ä¸€ç§**ä½æˆæœ¬ä½œå¼Šç­–ç•¥**ã€‚è‹¥ RM éšæ€§åœ°å°†å›å¤é•¿åº¦æˆ–å†—ä½™ä¿¡æ¯ä¸é«˜å¥–åŠ±å…³è”ï¼ŒGRPOå°†ç²¾ç¡®éµå¾ªæ­¤ä¿¡å·ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆé‡å¤ä¸”æ›´é•¿çš„æ–‡æœ¬ã€‚
>
> **å…¶ä»–å¯èƒ½çš„åŸå› **ï¼šåœ¨åšRLVRæ—¶ï¼Œæ ‡æ³¨è€…å€¾å‘é€‰æ‹©å¸¸è§ç­”æ¡ˆï¼Œè¿™ä¼šåŠ å‰§æ¨¡å¼åå¡Œï¼Œå³ä½¿å¥–åŠ±æ¨¡å‹å®Œç¾ä¹Ÿæ— æ³•æ¶ˆé™¤ã€‚è¯¥åå·®åœ¨æœ€ä¼˜ç­–ç•¥ä¸­è¡¨ç°ä¸ºå¯¹å¸¸è§æ¨¡å¼çš„â€œæ¸©åº¦é”åŒ–â€ï¼Œä¿ƒä½¿æ¨¡å‹è¾“å‡ºå‘åå¥½çš„å•ä¸€æ¨¡å¼æ”¶æ•›ã€‚

> #### è§£æ³•
>
> ##### 1. ä»»æ„ç†µç­–ç•¥ä¼˜åŒ–
>
> AEPOæ—¨åœ¨å–ä»£ä¼ ç»Ÿçš„Entropy Bonusï¼Œé€šè¿‡åœ¨Temperature-adjusted distributionsä¸Šåº”ç”¨ REINFORCE ç­–ç•¥æ¢¯åº¦ï¼Œå®ç°å¯¹ç­–ç•¥ç†µçš„ç¨³å®šå’Œç²¾ç¡®æ§åˆ¶ã€‚
>
> ##### 2. åŸºäºåæ–¹å·®çš„å±€éƒ¨è£å‰ª
>
> ä¼ ç»Ÿçš„ PPO $$Ïµ-clip$$ä½œç”¨äºæ•´ä¸ªåºåˆ—çš„å¹³å‡ä¼˜åŠ¿å‡½æ•°ï¼Œæ— æ³•æœ‰æ•ˆç®¡ç† token çº§åˆ«é«˜æ–¹å·®ä¿¡å·ç§¯ç´¯å¯¼è‡´çš„å±€éƒ¨ä¸ç¨³å®šæ›´æ–°ã€‚ Clip-Cov æ˜¯ä¸€ç§ Token-Level çš„ PPO æ”¹è¿›ï¼Œé€šè¿‡è£å‰ªé‚£äº›å…·æœ‰**é«˜åæ–¹å·®**çš„ Tokenï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚ å¯¹äºGRPOï¼ŒClip-Cov èƒ½å¤Ÿé™åˆ¶å±€éƒ¨ç­–ç•¥çš„å‰§çƒˆçªå˜ï¼Œæœ‰æ•ˆé˜²æ­¢æ¨¡å‹è¿‡åº¦ä¼˜åŒ–å°‘æ•° Token è€Œå½¢æˆå±€éƒ¨é‡å¤æ¨¡å¼ã€‚&#x20;
>
> ##### 3. ç­–ç•¥è¿‡æ»¤
>
> PF-PPOåœ¨ PPO é‡‡æ ·åï¼Œä¼šè¿‡æ»¤æ‰è¢«è®¤ä¸º**å¥–åŠ±ä¸å¯é çš„æ ·æœ¬**ï¼ˆé€šå¸¸æ˜¯ä¸­ç­‰å¥–åŠ±æ ·æœ¬ï¼‰ï¼Œä»…ä½¿ç”¨é«˜ä¿¡å™ªæ¯”å­é›†è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚é€šè¿‡æé«˜è®­ç»ƒä¿¡å·çš„ SNRï¼Œé™ä½äº†ç­–ç•¥å¯¹å«å™ªå¥–åŠ±ä¿¡å·**è¿‡åº¦æ‹Ÿåˆ**çš„é£é™©ï¼Œé¼“åŠ±ç­–ç•¥æ¢ç´¢çœŸæ­£é«˜è´¨é‡çš„åŒºåŸŸï¼Œä»æ ¹æœ¬ä¸Šç¼“è§£ç­–ç•¥æ”¶ç¼©å’Œé‡å¤æ¨¡å¼ã€‚
>
> ##### 4. è°ƒå‚æ•°
>
> æœ€æœ´å®æ— åçš„æ–¹æ³•ï¼ŒåŒæ—¶ä¹Ÿå¤§æ¦‚ç‡æ˜¯å¯ä»¥å¸¦æ¥æ”¶ç›Šã€‚
>
> * Rollout é‡‡æ ·æ¸©åº¦ï¼šå³ä½¿policyæœ¬èº«å¼€å§‹æ”¶ç¼©ï¼Œå¦‚æœåœ¨ Rollout é˜¶æ®µä½¿ç”¨è¾ƒé«˜çš„é‡‡æ ·æ¸©åº¦ï¼Œä»èƒ½ç¡®ä¿ RL Buffer ä¸­åŒ…å«æ›´å¤šé«˜ç†µã€å·®å¼‚åŒ–çš„æ ·æœ¬ã€‚
>
> * KLæ•£åº¦æƒ©ç½šç³»æ•° $$Î²_{KL}$$è¿‡ä½ä¼šå¯¼è‡´ç­–ç•¥æ¿€è¿›è¿½æ±‚ RM å¥–åŠ±ï¼Œææ˜“å¼•å‘ç­–ç•¥å¿«é€Ÿæ”¶ç¼©å’Œè¿‡æ‹Ÿåˆï¼ˆé‡å¤å†…å®¹ï¼‰ã€‚å¿…é¡»ä»¥**åŠ¨æ€ç›‘æµ‹ KL æ•£åº¦å’Œå¤šæ ·æ€§æŒ‡æ ‡**ï¼ˆå¦‚ Distinct-Nï¼‰ä¸ºæŒ‡å¯¼ï¼Œè¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚
>
> * è£å‰ªæ¯”ï¼šåœ¨ GRPO/DAPO å®è·µä¸­ï¼Œé‡‡ç”¨ç›¸å¯¹è¾ƒé«˜çš„è£å‰ªé˜ˆå€¼ï¼ˆå¦‚ **Clip-Higher Ïµ=0.28ï¼‰**ï¼Œå…è®¸ç­–ç•¥è¿›è¡Œæ›´æ¿€è¿›çš„æ›´æ–°ï¼Œæœ‰åŠ©äºå¯¹æŠ—æ—©æœŸç­–ç•¥æ”¶ç¼©ç°è±¡ã€‚



# **3.3.7 ReMax**

> **è®ºæ–‡ï¼šReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models**

## **ç­–ç•¥æ¢¯åº¦ä¸PPOå›é¡¾**

> ### **ç­–ç•¥æ¢¯åº¦**
>
> **å…¶ä¸­$$\psi_t$$æ˜¯ä¸€ä¸ªåŸºäºå¥–åŠ±çš„ä¼°è®¡å€¼ï¼Œç”¨äºç»™åé¢çš„ç­–ç•¥æ¢¯åº¦$$\nabla_\theta \log \pi_\theta(a_t|s_t )$$æä¾›æ›´æ–°å¹…åº¦(å¤§å°)å’Œæ–¹å‘(æ­£è´Ÿ)**
>
> $$g=\mathbb{E}_{\pi_\theta}\left[ \sum^T_{t=0}\psi_t\nabla_\theta \log \pi_\theta(a_t|s_t )\right]$$
>
> åŸºäº**å¤šæ¡æ ·æœ¬è®¡ç®—æœŸæœ›çš„è¯å°±æ˜¯ï¼š**
>
> $$g= \frac{1}{N}\sum_{i=1}^N\sum^T_{t=0}\psi_t^i\nabla_\theta \log \pi_\theta(a_t^i|s_t^i )$$

> ### **Actor-Criticç±»æ–¹æ³•å¯¹$$\psi_t$$çš„ä¼°è®¡**
>
> 1. **è½¨è¿¹çš„æ€»å›æŠ¥ï¼š**$$\sum_{t' = 0}^{T} \gamma^{t'} r_{t'}$$
>
> 2. **åŠ¨ä½œ$$a_t$$ä¹‹åçš„å›æŠ¥ï¼š**$$\sum_{t' = t}^{T} \gamma^{t' - t} r_{t'}$$ï¼šè¿™ä¸ªå°±æ˜¯è’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œä¹Ÿå°±æ˜¯REINFORCEæ–¹æ³•ç”¨åˆ°çš„æ–¹æ³•ï¼Œæ–¹å·®å¤§
>
> 3. **åŠ å…¥åŸºçº¿çš„æ”¹è¿›ç‰ˆæœ¬ï¼š**$$\sum_{t' = t}^{T} \gamma^{t' - t} r_{t'} - b(s_t)$$ï¼šé€šè¿‡åŠ å…¥åŸºçº¿ $$b(s_t)$$æ¥é™ä½æ–¹å·®ï¼Œä¸€ä¸ªæ¯”è¾ƒå¸¸ç”¨çš„åŸºçº¿å°±æ˜¯ä»·å€¼å‡½æ•°$$V(s_t)$$
>
> 4. **åŠ¨ä½œä»·å€¼å‡½æ•°ï¼š**$$Q^{\pi_{\theta}}(s_t, a_t)$$
>
> 5. **ä¼˜åŠ¿å‡½æ•°ï¼š**$$A^{\pi_{\theta}}(s_t, a_t)$$
>
> 6. **æ—¶åºå·®åˆ†ï¼š**$$r_t + \gamma V^{\pi_{\theta}}(s_{t + 1}) - V^{\pi_{\theta}}(s_t)$$ï¼šå¯¹ä¼˜åŠ¿å‡½æ•°çš„ä¸€æ­¥ä¼°è®¡

> ### **PPOç®—æ³•**
>
> $$
> \arg \max_{\theta} \mathbb{E}_{s \sim \nu^{\beta}, a \sim \pi_{\theta_k}(\cdot \mid s)} \left[ \min \left( \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_k}(a \mid s)} A^{\pi_{\theta_k}}(s, a), \text{clip} \left( \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_k}(a \mid s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}}(s, a) \right) \right]
>  $$
>
> å¯¹PPOè¿›è¡Œä¸€äº›ç®€åŒ–ï¼Œæå–æ ¸å¿ƒï¼Œå‘ç°**PPOå°±æ˜¯ç”¨ä¼˜åŠ¿å‡½æ•°æŒ‡å¯¼ç­–ç•¥æ›´æ–°çš„ç­–ç•¥æ¢¯åº¦ç±»æ–¹æ³•ï¼Œåªä¸è¿‡åœ¨å®ç°çš„æ—¶å€™lossè®¡ç®—PPOæ˜¯ä¹˜ä»¥ $$\pi$$ï¼Œç­–ç•¥æ¢¯åº¦æ˜¯ä¹˜ä»¥ $$\log\pi$$ï¼Œéƒ½ä¼šåå‘ä¼ æ’­æ›´æ–°policyçš„å‚æ•°ï¼š**
>
> $$\arg \max_{\theta} \mathbb{E}_{s \sim \nu^{\beta}, a \sim \pi_{\theta}(\cdot \mid s)} \left[ {\pi_{\theta}(a \mid s)} Adv^{\pi_{\theta}}(s, a)  \right]$$

![](images/image-8.png)

![](images/image-9.png)

## **ReMaxå…·ä½“ç®—æ³•**

![](images/image-10.png)

> ### **ReMax**
>
> ReMaxçš„**å‡ºå‘ç‚¹å’ŒGRPOä¹Ÿæ˜¯ä¸€è‡´çš„ï¼Œæƒ³è¦å»æ‰Critic Model**ï¼Œ**GRPOæ˜¯åœ¨PPOçš„å½¢å¼åŸºç¡€ä¸Šå¯¹ä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡è¿›è¡Œé‡æ–°è®¡ç®—ï¼ŒReMaxåˆ™æ˜¯å®Œå…¨å€’é€€å›äº†ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¹¶å¯¹å…¶ä¸­çš„baselineè®¡ç®—åšå‡ºäº†ä¿®æ”¹**ï¼ˆä½†å®é™…ä¸ŠPPOä¹Ÿå±äºç­–ç•¥æ¢¯åº¦ä¸€ç±»çš„ç®—æ³•ï¼Œå‰é¢ä¹Ÿæåˆ°äº†ï¼‰
>
> $$\text{ReMax = REINFORCE + Max}$$
>
> ä»ç­–ç•¥æ¢¯åº¦ä¸­çš„REINFORCEæ–¹æ³•å‡ºå‘ï¼Œå‰é¢æåˆ°**REINFORCEç®—æ³•å°±æ˜¯å¯¹ $$\psi_t$$é‡‡ç”¨äº†è’™ç‰¹å¡æ´›ä¼°è®¡åç»­å›æŠ¥ï¼š**
>
> $$g= \frac{1}{N}\sum_{i=1}^N\sum^T_{t=0}(\sum_{t' = t}^{T} \gamma^{t' - t} r_{t'}^i)\nabla_\theta \log \pi_\theta(a_t^i|s_t^i )$$
>
> ä½†æ˜¯**è€ƒè™‘åˆ°å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­åªåœ¨æœ€åä¸€ä¸ªtoken(åŠ¨ä½œ)å¤„æœ‰å¥–åŠ± $$r(x,y)$$ï¼Œè¿™ä¸ªå¥–åŠ±èƒ½å¤Ÿä»£è¡¨æ•´ä¸ªå¥å­çš„æ€»å›æŠ¥ï¼Œä¸­é—´token(åŠ¨ä½œ)å¤„çš„å¥–åŠ±çœ‹ä½œ0**ï¼ŒåŒæ—¶é’ˆå¯¹å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¿®æ”¹è¡¨è¿°ï¼Œé‚£ä¹ˆREINFORCEçš„å½¢å¼å°±å˜æˆäº†ï¼š
>
> $$\begin{aligned}
> g= \frac{1}{N}\sum_{i=1}^N\sum^T_{t=0}r(x^i,y^i)\nabla_\theta \log \pi_\theta(a_t^i|[x^i, a^i_{1:t-1}] )\\ =\frac{1}{N}\sum_{i=1}^N\sum^T_{t=0}r(x^i,a^i_{1:T})\nabla_\theta \log \pi_\theta(a_t^i|[x^i, a^i_{1:t-1}] )
> \end{aligned}
> $$
>å…¶ä¸­ $$x^i$$ä¸º`prompt`ï¼Œ $$y^i = [a^i_1,a^i_2,\cdots,a^i_T]=a_{1:T}^i$$ä¸º`response`ï¼Œ $$a_t^i$$ä¸ºè¾“å‡ºçš„`token`ä¹Ÿæ˜¯`action`ï¼ŒçŠ¶æ€åˆ™æ˜¯é€šè¿‡æ‹¼æ¥è¿™äº›`token`å½¢æˆçš„
> 
>**ä½†æ˜¯ä¹‹å‰å‡ æœŸæˆ‘ä»¬è®²è¿‡ï¼ŒREINFORCEç”¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¼°è®¡å›æŠ¥æœ‰æ–¹å·®å¤§çš„é—®é¢˜ï¼Œæ‰€ä»¥ReMaxå®é™…ç”¨åˆ°çš„æ˜¯å‡å»åŸºçº¿baselineçš„ç‰ˆæœ¬ï¼š**
> 
>$$g=\frac{1}{N}\sum_{i=1}^N\sum^T_{t=0}(r(x^i,a^i_{1:T})-b_\theta(x^i))\nabla_\theta \log \pi_\theta(a_t^i|[x^i, a^i_{1:t-1}] )$$
> 
>å…¶ä¸­åŸºçº¿å…·ä½“è®¡ç®—ä¸ºï¼š
> $$
>b_{\theta}(x^{i}) = r(x^{i},\bar{a}_{1:T}^{i}), \bar{a}_{t}^{i} \in \underset{}{\operatorname{argmax}} \pi_{\theta}(\cdot|x^{i},\bar{a}_{1:t - 1}^{i})
> $$
> 
>  å³ä½¿ç”¨åœ¨**æ¯ä¸€ä¸ªè¾“å‡º`token`å¤„ç”¨ `greedy` è§£ç æ‰€äº§ç”Ÿçš„`response`çš„å¥–åŠ±å€¼ä½œä¸ºåŸºçº¿å€¼**ï¼Œå‡å»åŸºçº¿å€¼èƒ½å¤Ÿé™ä½æ–¹å·®ï¼Œè¿™ä¸€ç‚¹åœ¨ä¹‹å‰å‡ æœŸå¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸“é¡¹ä¹Ÿæœ‰è®²è¿‡
>
> é€šå¸¸æƒ…å†µä¸‹æˆ‘ä»¬åœ¨ On Policy è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒLLM åœ¨åš generate çš„æ—¶ä¼šé‡‡ç”¨ top\_p = 1.0, top\_k = -1 çš„é‡‡æ ·æ–¹å¼ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¢ç´¢ã€‚ä½¿ç”¨ greedy ç­–ç•¥ç”Ÿæˆå¥å­çš„å¾—åˆ†åšä¸º baslineï¼Œè¿™ä¹‹æ‰€ä»¥èƒ½å¤Ÿé™ä½æ–¹å·®ï¼Œæ˜¯é»˜è®¤è®¤ä¸ºé€šå¸¸ SFT æ¨¡å‹å·²ç»ç»è¿‡ä¸€éƒ¨åˆ†å¯¹é½ï¼Œå¯¹äºåŒä¸€ä¸ª prompt æ¨¡å‹ä¸å¤ªä¼šè¾“å‡ºå·®å¼‚æ€§è¿‡å¤§çš„ç­”æ¡ˆ
>
> > **å› ä¸ºæ¯ä¸€ä¸ªè¡Œä¸ºï¼ˆå¯¹åº”ç”Ÿæˆå¥å­ä¸­çš„æ¯ä¸€ä¸ª tokenï¼‰éƒ½æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼ŒN ä¸ªéšæœºå˜é‡åŠ åœ¨ä¸€èµ·ï¼Œæ–¹å·®å°±ä¼šéå¸¸å·¨å¤§ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ•´ä¸ª RL è®­ç»ƒå´©æ‰ã€‚**

![](images/image-11.png)

# **3.3.9 RLOO**

> **è®ºæ–‡ï¼šBack to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs&#x20;**
>
> **REINFORCE Leave-One-Out**
>
> æœ‰äº†å¯¹ReMaxçš„åˆ†æä¹‹åï¼ŒRLOOçš„åˆ†æå°±æ›´ç®€å•äº†ï¼Œä»åå­—å¯ä»¥çœ‹å‡ºï¼Œä¹Ÿæ˜¯åŸºäºREINFORCEç®—æ³•çš„æ”¹è¿›ï¼ŒåŒæ ·æ˜¯å¯¹åŸºçº¿å€¼è¿›è¡Œäº†æ”¹åŠ¨:
>
> $$b_{\theta}(x^{i},y^i_j) = \frac{1}{M-1}\sum_{k\neq j}r(x^i,y^i_k) ,j\in M$$
>
> **é€šè¿‡å¯¹`prompt` $$x^i$$é‡‡æ · $$M$$æ¡`response`ï¼Œç„¶åæ¯ä¸€ä¸ª`response`å¯¹åº”çš„åŸºçº¿å€¼è®¾ç½®ä¸ºè‡ªå·±çš„å¥–åŠ±å‡å»æ‰€æœ‰å…¶ä»–`response`å¥–åŠ±çš„å‡å€¼**ï¼Œæœ€åå¯¹æ‰€æœ‰çš„prompt $$x^i$$çš„ç­–ç•¥æ¢¯åº¦å¦‚ä¸‹ï¼š
>
> $$g=\frac{1}{N}\sum_{i=1}^N\frac{1}{M}\sum^M_{j=0}\left[\left(r(x^i,y^i_j)-\frac{1}{M-1}\sum_{k\neq j}r(x^i,y^i_k)\right)\nabla_\theta \log \pi_\theta(y^i_j|x^i)\right]$$

# **3.3.10 REINFORCE++**

> **è®ºæ–‡ï¼šREINFORCE++: A SIMPLE AND EFFICIENT APPROACH FOR ALIGNING LARGE LANGUAGE MODELS&#x20;**
>
> **è¿™éƒ¨åˆ†å…¶å®éƒ½åŒ…å«åœ¨äº† 3.3.4ç« ä¸­[ 3.3 RLHF åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ](https://kcnd4kn8i6ap.feishu.cn/wiki/TQqTwh2uwiSrqYktIPccTQOcn0g?fromScene=spaceOverview#share-IZoBdQPAro3yOJx8VOecVQEPn5g)**

# **3.3.11 DAPO**

> ### **æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡æ˜¯å¼ºåŒ–å­¦ä¹ æ°¸æ’ä¸å˜çš„è¯é¢˜**
>
> **æ¢ç´¢ï¼ˆExplorationï¼‰ä¸åˆ©ç”¨ï¼ˆExploitationï¼‰ä¹‹é—´çš„å¹³è¡¡**æ˜¯ä»ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å¼€å§‹è¯ç”Ÿè‡³ä»Šå°±ä¸€ç›´åœ¨è§£å†³çš„å…³é”®é—®é¢˜ï¼Œå‰è€…é¼“åŠ±æ™ºèƒ½ä½“ï¼ˆç­–ç•¥ï¼‰å¯¹ç¯å¢ƒçŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´è¿›è¡Œæ›´å¤šçš„æ¢ç´¢ï¼Œä»¥æœŸå¯»å¾—æ›´ä¼˜çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œåè€…å¼ºè°ƒå¯¹äºç›®å‰çš„æ¯”è¾ƒå¥½çš„ç­–ç•¥çš„åˆ©ç”¨ï¼Œä»¥æŒç»­äº§ç”Ÿå¥½çš„è½¨è¿¹æ ·æœ¬ä»¥ä¾›å­¦ä¹ å’Œä½¿ç”¨ã€‚**å¥–åŠ±è®¾è®¡ï¼ˆReward Designï¼‰åˆ™å¯ä»¥è¯´ä¸€ç›´æ˜¯å¼ºåŒ–å­¦ä¹ çš„çµé­‚**ï¼Œå¥½çš„ä»»åŠ¡ç›¸å…³çš„å¥–åŠ±è®¾è®¡å¾€å¾€ä¼šé™ä½ç­–ç•¥å­¦ä¹ æ‰€éœ€è¦çš„æˆæœ¬ï¼Œä½†æ˜¯è¿™é€šå¸¸éœ€è¦ä¸“å®¶ç»éªŒå’Œç²¾åŠ›ï¼Œæ‰€ä»¥æ‰æœ‰äº†RLHFæ–¹æ³•ï¼Œä»å¯¹æ¯”åå¥½å¯¹ä¸­å­¦ä¹ ä¸€ä¸ªå¥–åŠ±æ¨¡å‹å‡ºæ¥ï¼Œä½†æ˜¯DeepSeekR1ä¸ºä»£è¡¨ï¼Œå¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ åˆå›å½’äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ è®¾è®¡æ ‡é‡å¥–åŠ±çš„æ¨¡å¼
>
> å­—èŠ‚å¼€æºçš„æŠ€æœ¯æŠ¥å‘Š **DAPO: an Open-Source LLM Reinforcement Learning System at Scale**ï¼Œåˆ™æ˜¯åœ¨DeepSeek GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„åŸºç¡€ä¸Šï¼Œå›´ç»•æ¢ç´¢åˆ©ç”¨å¹³è¡¡ä»¥åŠå¥–åŠ±è®¾è®¡æå‡ºäº†**DAPOç®—æ³•ï¼ˆDecoupled Clip and Dynamic sAmpling Policy Optimizationï¼‰**ï¼Œä¸»è¦åŒ…å«äº†å››ä¸ªç›¸å…³çš„å…³é”®æŠ€æœ¯æ”¹è¿›ï¼Œå…ˆæ¥**å›é¡¾ä¸€ä¸‹GRPOçš„å…¬å¼ï¼š**
>
> $$\begin{aligned}\mathcal{J}_{\mathrm{GRPO}}(\theta)&=\mathbb{E}_{(q,a)\thicksim\mathcal{D},\{o_i\}_{i=1}^G\thicksim\pi_{\theta_{\mathrm{old}}}(\cdot|q)}\\&\left[\frac1G\sum_{i=1}^G\frac1{|o_i|}\sum_{t=1}^{|o_i|}\left(\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\right.\right.\operatorname{clip}\left(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right)-\beta D_{\operatorname{KL}}(\pi_\theta||\pi_{\operatorname{ref}})\Bigg)\Bigg],\end{aligned}$$
>
> $$\begin{aligned}\mathrm{where}\\r_{i,t}(\theta)&=\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,<t})}{\pi_{\theta_{\mathrm{old}}}(o_{i,t}\mid q,o_{i,<t})},\quad\hat{A}_{i,t}=\frac{R_{i}-\mathrm{mean}(\{R_{i}\}_{i=1}^{G})}{\mathrm{std}(\{R_{i}\}_{i=1}^{G})}.\end{aligned}$$
>
> DAPOå¯¹GRPOåšçš„**ç¬¬ä¸€ä»¶äº‹å…¶å®æ˜¯å»æ‰KLæ•£åº¦çº¦æŸé¡¹**ï¼ŒKLçº¦æŸé€šå¸¸å‡ºç°åœ¨offline RLä»¥åŠå¤§æ¨¡å‹RLHFå½“ä¸­ï¼Œç”¨æ¥çº¦æŸç­–ç•¥ä¸è¦åç¦»è¡Œä¸ºç­–ç•¥æˆ–è€…åˆå§‹ç­–ç•¥ï¼ˆSFTï¼‰å¤ªè¿œï¼Œä½†æ˜¯åœ¨è®­ç»ƒlong-CoTçš„æ¨ç†æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç­–ç•¥ä¼šæ˜¾è‘—ä¸åŒäºåˆå§‹ç­–ç•¥ï¼Œæ‰€ä»¥KLçº¦æŸæ˜¾å¾—æ²¡é‚£ä¹ˆæœ‰æ„ä¹‰å’Œå¿…éœ€ï¼Œç„¶åå†åŠ ä¸Šåç»­çš„æ”¹è¿›å¾—åˆ°**DAPOçš„å…¬å¼å¦‚ä¸‹ï¼š**
>
> $$\begin{aligned}\mathcal{J}_{\mathrm{DAPO}}(\theta)&=\quad\mathbb{E}_{(q,a)\sim\mathcal{D},\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\mathrm{old}}}(\cdot|q)}\\&\left[\frac{1}{\sum_{i=1}^{G}|o_{i}|}\sum_{i=1}^{G}\sum_{t=1}^{|o_{i}|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\operatorname{clip}\left(r_{i,t}(\theta),1-\varepsilon_{\mathrm{low}},1+\varepsilon_{\mathrm{high}}\right)\hat{A}_{i,t}\right)\right]\end{aligned}$$
>
> $$\mathrm{s.t.}\quad0<\left|\{o_i\mid\text{is equivalent}(a,o_i)\}\right|<G,\mathrm{where}$$
>
> $$r_{i,t}(\theta)=\frac{\pi_\theta(o_{i,t}\mid q,o_{i,<t})}{\pi_{\theta_{\mathrm{old}}}(o_{i,t}\mid q,o_{i,<t})},\quad\hat{A}_{i,t}=\frac{R_i-\mathrm{mean}(\{R_i\}_{i=1}^G)}{\mathrm{std}(\{R_i\}_{i=1}^G)}.$$

## **Clip-Higher**

> å…·ä½“çš„å››ä¸ªæŠ€æœ¯æ”¹è¿›ï¼Œç¬¬ä¸€ä¸ªæ˜¯Clip-Higherã€‚é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯**æŠŠåŸå…ˆçš„å¯¹äºé‡è¦æ€§æƒé‡ $$r_{i,t}(\theta)$$çš„å‘ä¸Šè£å‰ªçš„é˜ˆå€¼æé«˜äº†**
>
> ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼ŒæŒ‰ç…§åŸæœ¬çš„è£å‰ªï¼Œå–$$\epsilon=0.2$$ï¼Œå¦‚æœå½“å‰ä¼˜åŠ¿å€¼ $$A_{i,t}$$æ˜¯æ­£ï¼Œæ„å‘³ç€æˆ‘ä»¬è¦æé«˜å½“å‰responseä¸­tokençš„ç”Ÿæˆæ¦‚ç‡ï¼Œä½†æ˜¯å¯¹äºæœ¬èº«æ¦‚ç‡å°±æ¯”è¾ƒä½çš„tokenæ¥è¯´ï¼Œä¾‹å¦‚ $$\pi_{\theta_{old}}(o_i|q) = 0.01$$ï¼Œé‚£ä¹ˆæ ¹æ®è£åˆ‡ï¼Œå½“å‰ç­–ç•¥æ¦‚ç‡åªèƒ½åˆ° $$\pi_{\theta}(o_i|q)=0.012$$ï¼Œå‡ ä¹æ²¡æœ‰å¢é•¿ï¼Œè¿™å¯¹äºå­¦ä¹ é•¿æ¨ç†è¿‡ç¨‹ä»¥åŠå­¦ä¹ æ–°çš„æ¨ç†èŒƒå¼éƒ½æ˜¯é™åˆ¶ï¼Œé™åˆ¶äº†å¯¹ä½æ¦‚ç‡tokençš„æ¢ç´¢ã€‚æ‰€ä»¥DAPOè°ƒæ•´äº†å‘ä¸Šè£å‰ªçš„é˜ˆå€¼ï¼Œå…·ä½“ä¸º$$\epsilon_{low}=0.2,\epsilon_{high}=0.28$$

![](images/image-12.png)

![](images/image-13.png)

## **Dynamic Sampling**

> ç¬¬äºŒä¸ªæ˜¯åŠ¨æ€é‡‡æ ·ï¼Œå³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ**æ¯æ¬¡è®­ç»ƒå‰éƒ½è¿›è¡Œè¿‡é‡‡æ ·ï¼Œå¹¶è¿‡æ»¤æ‰å‡†ç¡®ç‡ä¸º1å’Œ0çš„prompts**ï¼Œç›´åˆ°æ•´ä¸ªbatchéƒ½æ˜¯å‡†ç¡®ç‡ä»‹äº0ï½1ä¹‹é—´çš„prompts
>
> ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœGRPOçš„ä¸€ä¸ªgroupå†…æ‰€æœ‰çš„outputså‡†ç¡®ç‡éƒ½æ˜¯1ï¼Œé‚£ä¹ˆæ ¹æ®ä¼˜åŠ¿è®¡ç®—å…¬å¼ï¼Œè¯¥ç»„å†…çš„ä¼˜åŠ¿å°±æ˜¯0ï¼Œè¿™æ ·ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œç„¶è€Œéšç€è®­ç»ƒæ­¥æ•°çš„å¢åŠ ï¼Œå‡†ç¡®ç‡ä¸º1çš„å›ç­”æ˜¯åœ¨é€æ­¥å¢åŠ çš„ï¼Œä¸ºäº†é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œå¢åŠ è®­ç»ƒç¨³å®šæ€§ï¼ŒDAPOé‡‡å–äº†åŠ¨æ€é‡‡æ ·ï¼Œè¿‡æ»¤æ‰å‡†ç¡®ç‡ä¸º1å’Œ0çš„prompts

![](images/image-14.png)

![](images/image-15.png)

![](images/image-16.png)

## **Token-Level Loss**



ç¬¬ä¸‰ä¸ªæ˜¯tokençº§lossï¼Œå…·ä½“å°±æ˜¯**å¯¹groupå†…çš„æ‰€æœ‰outputsçš„æ‰€æœ‰tokenè®¡ç®—losså‡å€¼**

GRPOåœ¨è®¡ç®—lossçš„æ—¶å€™æ˜¯å…ˆå¯¹å•ä¸ªoutputè®¡ç®—æ‰€æœ‰token lossçš„å‡å€¼ï¼Œç„¶åå†è®¡ç®—outputsä¹‹é—´çš„å‡å€¼ï¼š

![](images/image-17.png)



è¿™æ ·ä¼šå¯¼è‡´**ç»„å†…outputså¯¹äºlossè´¡çŒ®çš„ä¸å‡è¡¡ï¼Œè¾ƒé•¿çš„responseä¸­çš„tokenå¯¹æ€»lossçš„è´¡çŒ®åä½**ï¼Œè¿™æ ·å¯èƒ½å¯¼è‡´ä¸¤ä¸ªé—®é¢˜ï¼š

* å¯¹äºé«˜è´¨é‡çš„é•¿æ ·æœ¬ï¼Œè¿™æ ·çš„lossè®¡ç®—æ–¹å¼ä¼šå¦¨ç¢ç­–ç•¥å­¦ä¹ å…¶ä¸­çš„å…³é”®æ¨ç†æ¨¡å¼

* å¯¹äºè¿‡é•¿çš„ä½è´¨é‡æ ·æœ¬ï¼Œå…¶ä¸­åŒ…å«èƒ¡è¨€ä¹±è¯­å’Œé‡å¤ç°è±¡ï¼Œè¿™ç§æ ·æœ¬å¯¹äºlossçš„è´¡çŒ®ä¹Ÿåä½ï¼ˆä¼˜åŠ¿ä¸ºè´Ÿï¼Œè´¡çŒ®ä½ä¼šå¯¼è‡´ç­–ç•¥åç¦»è¿™ç§å›ç­”æ ·å¼çš„ç¨‹åº¦ä¸å¤Ÿï¼‰

æ‰€ä»¥DAPOå°†sample-levelçš„lossè®¡ç®—æ”¹ä¸ºäº†token-levelçš„lossè®¡ç®—ï¼Œå¯ä»¥æœ‰æ•ˆçš„è®©é•¿å›ç­”æ›´å¤šçš„å½±å“æœ€åçš„æ¢¯åº¦

![](images/image-18.png)

ä»æœ€åçš„ç»“æœä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œtoken-level lossçš„è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®šï¼Œè€Œä¸”æ§åˆ¶entropyä¸ä¼šè¿‡é«˜ï¼ˆè¿‡é«˜ç­–ç•¥åå‘éšæœºï¼Œè¿‡ä½æ¢ç´¢ä¸è¶³ï¼Œclip-highè§£å†³è¿‡ä½é—®é¢˜ï¼‰

![](images/image-19.png)

## **Overlong Reward Shaping**

> ç¬¬å››ä¸ªæ˜¯**å¯¹è¿‡é•¿å›ç­”çš„å¥–åŠ±ä¿®æ”¹**ï¼Œå…·ä½“æ˜¯**ç”¨ä¸€ç§soft punishmentçš„æ–¹æ³•å¯¹overlong reponseè¿›è¡Œæƒ©ç½š**ï¼Œ**å¯¹äºå¤„äºè¿‡é•¿åŒºé—´å†…çš„å›ç­”è¿›è¡Œæƒ©ç½šï¼Œç„¶åå åŠ åˆ°å‡†ç¡®ç‡å¥–åŠ±ä¸Š**ã€‚è¿™é‡Œå…·ä½“çš„ $$L_{max}=20480, L_{cache}=4096$$ï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæå‡æ€§èƒ½
>
> $$R_{\mathrm{length}}(y)=\begin{cases}0,&|y|\leq L_{\mathrm{max}}-L_{\mathrm{cache}}\\\frac{(L_{\mathrm{max}}-L_{\mathrm{cache}})-|y|}{L_{\mathrm{cache}}},&L_{\mathrm{max}}-L_{\mathrm{cache}}<|y|\leq L_{\mathrm{max}}\\-1,&L_{\mathrm{max}}<|y|&\end{cases}$$
>
> æœ€åçš„å®éªŒç»“æœæ˜¯åœ¨ Qwen2.5-32Bæ¨¡å‹ä¸Šè¿›è¡Œçš„ï¼Œåœ¨æ•°å­¦ä»»åŠ¡AIME2024ä¸Šçš„æ•ˆæœï¼Œä»…ç”¨äº†50%è®­ç»ƒæ­¥æ•°å°±è¶…è¿‡äº†GRPO

![](images/image-20.png)

![](images/image-21.png)

# **3.3.12 VAPO**

> **è®ºæ–‡ï¼šVAPO: Efficient and Reliable Reinforcement Learning forAdvanced Reasoning Tasks**
>
> **é“¾æ¥:** **https://arxiv.org/pdf/2504.05118**
>
> æå‡ºäº† **Value-model-based Augmented PPOï¼ˆVAPOï¼‰**&#x7B97;æ³•æ¥è®­ç»ƒ reasoning æ¨¡å‹ï¼ŒåŸºäº Qwen 32B base model è®­ç»ƒçš„æ¨¡å‹åœ¨ AIME 2024 dataset ä¸Šè¾¾åˆ°äº† SOTA çš„60.4åˆ†ï¼Œè¶…è¶Šäº† Deepseek-R1-Zero-Qwen-32B å’Œ å­—èŠ‚è‡ªå·±ä¹‹å‰çš„ DAPO ç®—æ³•ã€‚**VAPO è®­ç»ƒç¨³å®šä¸”é«˜æ•ˆï¼Œåœ¨ 5000 steps å†…å°±è¾¾åˆ°äº†SOTA**ã€‚ä¸»è¦ç ”ç©¶äº†å¦‚ä½•**é€šè¿‡ Value-model-based æ¡†æ¶å»è¿›è¡Œ Long-CoT æ¨ç†**ï¼Œå¹¶æå‡ºäº†ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š**ä»·å€¼æ¨¡å‹åå·®ã€å¼‚æ„åºåˆ—é•¿åº¦ä»¥åŠç¨€ç–å¥–åŠ±é—®é¢˜**

> ### **é—®é¢˜èƒŒæ™¯**
>
> #### **Value-model-based V.S. Value-model-free**
>
> **Value-model-free çš„æ–¹æ³•æ¯”å¦‚GRPOã€DAPO**ç­‰åœ¨å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹é¢è¡¨ç°å‡ºå‡ºè‰²çš„æœ‰æ•ˆæ€§ï¼Œè¿™äº›æ–¹æ³•é€šè¿‡**è®¡ç®—ä¸€ä¸ªç»„å†…å¤šæ¬¡é‡‡æ ·å¾—åˆ°çš„è½¨è¿¹çš„å¹³å‡å¥–åŠ±æ¥è®¡ç®—ä¼˜åŠ¿ï¼Œå»é™¤äº†æ˜¾å¼çš„ä»·å€¼å‡½æ•°ï¼ˆä»·å€¼æ¨¡å‹ï¼‰çš„éœ€æ±‚ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ–¹æ³•ä¼šè¡¨ç°å‡ºä¸ç¨³å®šçš„æ€§è´¨ï¼ˆMCä¼°è®¡çš„é«˜æ–¹å·®ï¼‰**
>
> VAPOæå‡ºä¸€ä¸ªè§‚ç‚¹ï¼Œå³**å¦‚æœ Value Model è®­ç»ƒçš„å‡†ç¡®å¾—å½“ï¼ŒValue-model-based æ–¹æ³•çš„ä¸Šé™æ˜¯æ›´é«˜çš„**ï¼ˆè¿™é‡Œå…¶å®ä¹Ÿå’Œ Reward Model çš„å›°å¢ƒä¸€æ ·ï¼Œå¦‚æœRMè®­ç»ƒçš„è¶³å¤Ÿå‡†ç¡®ï¼Œé‚£ä¹ˆä¸Šé™è‚¯å®šæ˜¯è¦æ¯”rule-basedé«˜çš„ï¼Œå› ä¸ºå¯ä»¥æä¾›è¶³å¤Ÿç»†ç²’åº¦çš„å¥–åŠ±ï¼‰ï¼š
>
> * **Value Model å¯ä»¥å‡†ç¡®çš„ä¼°è®¡æ¯ä¸ªæ‰§è¡Œçš„ Action å¯¹åç»­è·å¾—çš„æ”¶ç›Š Return çš„å½±å“ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„æ•ˆç”¨åˆ†é…ï¼ˆCredit Assignmentï¼‰ï¼Œä»¥åŠç»†ç²’åº¦çš„ç­–ç•¥ä¼˜åŒ–**ã€‚è¿™å¯¹äºå¤æ‚çš„æ¨ç†ä»»åŠ¡å°¤å…¶é‡è¦ï¼Œæ¯”å¦‚å•ä¸ªæ¨ç†æ­¥éª¤ä¸­çš„ç»†å¾®é”™è¯¯é€šå¸¸ä¼šå¯¼è‡´ç¾éš¾æ€§å¤±è´¥ï¼Œä»è€Œå½±å“ç­–ç•¥ä¼˜åŒ–
>
> * è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡ï¼Œä¹Ÿå°±æ˜¯GRPOã€DAPOç”¨åˆ°çš„ä¼°è®¡ä¼˜åŠ¿çš„æ–¹æ³•ï¼Œé€šå¸¸ä¼´éšç€é«˜æ–¹å·®ï¼Œ&#x800C;**&#x20;Value Model åˆ™å¯ä»¥ç»™æ¯ä¸ª token éƒ½ç”Ÿæˆä½æ–¹å·®çš„ä»·å€¼ä¼°è®¡ï¼Œä»è€Œå¢å¼ºè®­ç»ƒç¨³å®šæ€§**
>
> * è®­ç»ƒå‡†ç¡®çš„ **Value Model å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¢ç´¢è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å„ç§è½¨è¿¹ sampleï¼Œæå‡ RL çš„ä¸Šé™**

> #### **Value Model çš„æŒ‘æˆ˜**
>
> è®­ç»ƒä¸€ä¸ªå®Œç¾çš„ LongCoT ä»»åŠ¡çš„ Value Model å¸¦æ¥äº†é‡å¤§çš„æŒ‘æˆ˜ï¼š
>
> * **é•¿åºåˆ—ä¸­Value Modelçš„åå·®ï¼š**&#x9488;å¯¹é•¿è½¨è¿¹åºåˆ—**é€šè¿‡ bootstrappedï¼ˆè‡ªä¸¾ï¼‰çš„æ–¹æ³•ï¼ˆå°±æ˜¯æ—¶åºå·®åˆ†é‚£ä¸€å¥—ï¼‰å­¦ä¹ ä¸€ä¸ªä½åå·®çš„ Value Model æ˜¯å¾ˆå›°éš¾çš„**ï¼ˆMCä¼°è®¡æ˜¯æ— åä½†æ˜¯é«˜æ–¹å·®ï¼Œæ—¶åºå·®åˆ†æ˜¯æœ‰åä½†æ˜¯ä½æ–¹å·®ï¼Œå› ä¸ºä¸€èˆ¬å°±è‡ªä¸¾ä¸€æ­¥ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒVC-PPO è®ºæ–‡ä¸­æåˆ°ï¼Œ**ä½¿ç”¨ Reward Model åˆå§‹åŒ– Value Model ä¼šäº§ç”Ÿåå·®**ï¼Œå› ä¸ºRMæ˜¯è®­ç»ƒåœ¨ \<EOS> tokenå¤„ç»™å‡ºä¸€ä¸ªå¥–åŠ±ï¼Œå…¶ä»–ä½ç½®çš„ token ä¼šå› ä¸ºä¸å®Œæ•´çš„ä¸Šä¸‹æ–‡å¾—åˆ°è¾ƒä½çš„å¥–åŠ±ï¼Œä½†æ˜¯ Value Modelåº”è¯¥å¯¹æ¯ä¸€ä¸ª token éƒ½ç»™å‡ºåç»­çš„å¥–åŠ±æœŸæœ›çš„ä¼°è®¡ï¼Œæ‰€ä»¥ä½¿ç”¨RMåˆå§‹åŒ–åœ¨è®­ç»ƒå¼€å§‹é˜¶æ®µçš„ GAE è®¡ç®—ä¼šäº§ç”Ÿç´¯ç§¯çš„åå·®
>
> * **è®­ç»ƒè¿‡ç¨‹ä¸­å¼‚æ„çš„åºåˆ—é•¿åº¦ï¼šåŒæ—¶å¤„ç† short response å’Œ long response å¾ˆæœ‰æŒ‘æˆ˜æ€§**ï¼Œå…¶å¯¹åå·®-æ–¹å·®çš„å¹³è¡¡ä¼˜åŒ–å½±å“ä¸åŒã€‚å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œæ¨¡å‹é€šå¸¸ä¸ºäº†å¾—å‡ºæ­£ç¡®ç­”æ¡ˆè€Œç”Ÿæˆé•¿åº¦é«˜åº¦å¯å˜çš„responseï¼Œè€Œä»¥å¾€**è®¡ç®— GAE çš„æ—¶å€™ä½¿ç”¨çš„å›ºå®š$$\lambda$$å‚æ•°å°±ä¸é€‚ç”¨å˜é•¿çš„åºåˆ—äº†**ï¼Œå¯¹äºçŸ­å›ç­”ï¼ŒGAEä¼°è®¡çš„æ–¹å·®å¾€å¾€è¿‡é«˜ï¼Œè€Œå¯¹äºé•¿å›ç­”ï¼ŒGAEä¼°è®¡çš„åå·®å¾€å¾€æ›´ä¸¥é‡ï¼ˆè‡ªä¸¾çš„é—®é¢˜ï¼‰
>
> * **Verifier-based ä»»åŠ¡ä¸­çš„å¥–åŠ±ç¨€ç–æ€§ï¼š**&#x56;erifiers æä¾›çš„**å¥–åŠ±çš„ç¨€ç–æ€§è¢« LongCoTåŠ å‰§äº†ï¼Œè¿™å°±éœ€è¦æ›´å¥½åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨çš„å¹³è¡¡ã€‚&#x20;**&#x56;erifier-based RM æä¾›çš„ä¸€èˆ¬æ˜¯äºŒè¿›åˆ¶åé¦ˆå¥–åŠ±ï¼Œä¸”éšç€å›ç­”çš„å˜é•¿ï¼Œè®¡ç®—æ—¶é—´å¢åŠ ä¸”éé›¶å¥–åŠ±å€¼çš„æ¥æ”¶é¢‘ç‡ä¸‹é™ï¼Œç„¶ååœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­å¸¦æœ‰æ­£ç¡®ç­”æ¡ˆçš„response ä¼šå˜å¾—å¾ˆå°‘ã€‚ä½†æ˜¯ä¸ºäº†é‡‡æ ·åˆ°å„ç§responseï¼Œå¿…é¡»ä¿æŒæ¢ç´¢ï¼Œå¦ä¸€æ–¹é¢ç®—æ³•åˆè¦æœ‰æ•ˆçš„åˆ©ç”¨æ­£ç¡®çš„ responseï¼Œå³æ¢ç´¢å’Œåˆ©ç”¨çš„å¹³è¡¡

**å…·ä½“æ–¹æ³•**

> ### **ç¼“è§£é•¿åºåˆ—ä»·å€¼æ¨¡å‹çš„åå·®**
>
> é‡‡ç”¨ Value-Pretraining å’Œ Decoupled-GAEæ¥è§£å†³ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½åœ¨ VC-PPO ä¸­ä»‹ç»è¿‡
>
> * **Value-Pretrainingï¼š**&#x53EF;ä»¥ç†è§£ä¸ºå¯¹ Value Modelåšäº†ä¸€ä¸ªå……åˆ†çš„warm-up
>
>   * é¦–å…ˆ**ä»ä¸€ä¸ªå›ºå®šç­–ç•¥ï¼Œæ¯”å¦‚$$\pi_{sft}$$ï¼Œä¸æ–­åœ°é‡‡æ ·å›ç­”ï¼Œç„¶åç”¨è’™ç‰¹å¡æ´›returnè®­ç»ƒ Value Modelç›´åˆ°value losså’Œæ–¹å·®è¶³å¤Ÿçš„ä½**
>
>   * ç„¶åä¿å­˜è¿™ä¸ªValue Modelç”¨äºåç»­çš„è®­ç»ƒ
>
> * **Decoupled-GAEï¼š**&#x4E3B;è¦æ€æƒ³æ˜¯å°† value å’Œ policy æ›´æ–°æ—¶å€™é‡‡ç”¨ä¸åŒçš„$$\lambda$$å€¼
>
>   * å…·ä½“æ¥è¯´åœ¨æ›´æ–° value çš„æ—¶å€™ï¼Œ**å¯¹äº value target çš„è®¡ç®—ä½¿ç”¨$$\lambda_{critic}=1$$ï¼Œ**&#x5373;**ä½¿ç”¨çº¯ç²¹çš„ç´¯ç§¯å¥–åŠ±æ¥æ›´æ–° Value Modelï¼ˆ**&#x8FD9;é‡Œå·²ç»é»˜è®¤æŠ˜æ‰£å› å­$$\gamma=1$$**ï¼‰**
>
>     $$V^{target}(s_t) = GAE_{\lambda=1} + V(s_t) = \sum_{l=0}^{T-t+1} r_{t+l}$$
>
>     è¿™æ ·å°±å¯ä»¥å¾ˆå¥½çš„è§£å†³ LongCoT ä»»åŠ¡ä¸­çš„å¥–åŠ±æ¶ˆå‡é—®é¢˜
>
>   * ç„¶åå°±æ˜¯**æ›´æ–° policy çš„æ—¶å€™è®¡ç®— GAE é‡‡ç”¨ $$\lambda_{policy} = 0.95$$**ï¼Œä½¿ç”¨ä¸€ä¸ªè¾ƒä½çš„$$Î»$$å€¼å¯ä»¥å¾ˆå¥½çš„**åŠ é€Ÿç­–ç•¥æ”¶æ•›**

> ### **ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¼‚æ„åºåˆ—é•¿åº¦**
>
> ä¸»è¦ä½¿ç”¨é•¿åº¦è‡ªé€‚åº” GAE è®¡ç®—å’Œ token-level policy lossæ¥è§£å†³åºåˆ—é•¿åº¦å¼‚æ„é—®é¢˜
>
> * **Length-Adaptive GAEï¼š**&#x5BF9;äºé•¿åº¦å¤§äº 100 çš„æ›´é•¿åºåˆ—ä¸­ï¼Œä¸å¥–åŠ±ç›¸å¯¹åº”çš„ TD-error ç³»æ•°ä¸º $$0.95^{100}â‰ˆ0.006$$ï¼Œå®é™…ä¸Šä¸ºé›¶ï¼Œæ‰€ä»¥**å›ºå®šçš„$$\lambda_{policy}$$ä¸æ˜¯å¤„ç†æé•¿è¾“å‡ºåºåˆ—çš„æœ€ä½³é€‰æ‹©**ï¼Œæ‰€ä»¥ VAPO é€‰æ‹©äº†é•¿åº¦è‡ªé€‚åº”ï¼Œå³ $$\lambda_{policy} = 1- \frac{1}{\alpha l}$$ï¼Œå…¶ä¸­$$\alpha$$ä¸ºè¶…å‚
>
> * **Token-level Policy Gradient Lossï¼š**&#x8FD9;ä¸€ç‚¹ä¸ DAPO ç›¸åŒï¼Œè¿™é‡Œä¸è¿‡å¤šèµ˜è¿°

> **å¤„ç† Verifier-based ä»»åŠ¡ä¸­çš„å¥–åŠ±ä¿¡å·ç¨€ç–æ€§**
>
> æå‡RLè®­ç»ƒä¸­æ¢ç´¢-åˆ©ç”¨å¹³è¡¡çš„æ•ˆç‡æå…·æŒ‘æˆ˜æ€§ï¼ŒVAPOé‡‡ç”¨äº†ï¼šClip-Higherã€æ­£ä¾‹æŸå¤±ï¼ˆPositive Example LM Lossï¼‰å’Œåˆ†ç»„é‡‡æ ·ï¼ˆGroup-Samplingï¼‰ä¸‰ç§æ–¹æ³•
>
> * **Clip-Higherï¼š**&#x44;APOä¸­ä¹Ÿä½¿ç”¨äº†ï¼Œä¸è¿‡å¤šé˜è¿°
>
> * **Positive Example LM Lossï¼š**&#x5229;ç”¨RLè®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬ï¼Œè®¡ç®—å…¶ NLL lossï¼Œé¢å¤–åŠ å…¥PPOlossä¸­ï¼Œå¢å¼ºå¯¹æ­£æ ·æœ¬çš„æ¨¡ä»¿å­¦ä¹ ï¼š
>
>   $$\mathcal{L}_{\text{NLL}}(\theta)=-\frac{1}{\sum_{o_i\in\mathcal{T}}|o_i|}\sum_{o_i\in\mathcal{T}}\sum_{t = 1}^{|o_i|}\log\pi_{\theta}(a_t|s_t)$$
>
>   $$\mathcal{L}(\theta)=\mathcal{L}_{\text{PPO}}(\theta)+\mu*\mathcal{L}_{\text{NLL}}(\theta)$$
>
> * **Group-Samplingï¼š**&#x8FD9;ä¸€ç‚¹å…¶å®å°±æ˜¯å¯¹åŒä¸€ä¸ª prompt é‡‡æ ·å¤šä¸ªå›ç­”ï¼Œ**ç›¸æ¯”åŒæ ·è®¡ç®—èµ„æºä¸‹å°½å¯èƒ½å¤šçš„æ”¶é›†promptï¼Œæ¯ä¸€ä¸ªprompté‡‡æ ·ä¸€ä¸ªå›ç­”ï¼ŒGroup Samplingçš„æ–¹æ³•å¯ä»¥æ›´å¥½çš„å¼•å…¥ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·ï¼Œä»è€Œå¢å¼ºpolicyçš„å­¦ä¹ **

# **3.3.13 TTRL**

> **è®ºæ–‡ï¼šTTRL: Test-Time Reinforcement Learning**
>
> **é“¾æ¥ï¼šhttps://arxiv.org/pdf/2504.16084**
>
> ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨**æµ‹è¯•æ—¶æ‰©å±•ï¼ˆTest-Time Scalingï¼ŒTTSï¼‰äº§ç”Ÿçš„æ— æ ‡æ³¨æ•°æ®ï¼ˆæ¯”å¦‚æµ‹è¯•æ•°æ®ä¸ç»™ground truthï¼‰è¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼ˆMajority Voteï¼‰è‡ªåŠ¨è®¡ç®—å¥–åŠ±æ¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–**ï¼Œé©±åŠ¨æ¨¡å‹çš„è‡ªæå‡

![](images/image-22.png)

> ### **å…·ä½“æµç¨‹**
>
> 1. æ ¹æ®æµ‹è¯•æ•°æ®ä¸­çš„é—®é¢˜ï¼Œ**ç”¨ LLM å¤šæ¬¡é‡‡æ ·ï¼Œå¾—åˆ°ä¸€ç»„å›ç­”ï¼ˆæ¸©åº¦è®¾ç½®ä¸º1.0å¢å¼ºå¤šæ ·æ€§ï¼ŒN=64ï¼‰**
>
> 2. å¯¹è¿™ç»„å›ç­”**è¿›è¡Œç­”æ¡ˆæå–ï¼Œå¹¶è¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼ˆå¤šæ•°æŠ•ç¥¨æˆ‘ä»¬åœ¨1.6.2ç« [ 1.6 Structure & Decoding ç»“æ„å’Œè§£ç ](https://kcnd4kn8i6ap.feishu.cn/wiki/Os2swulnTiJuqEkVoarcSZKFnfh?fromScene=spaceOverview#share-KTjcdMxnvoAsxtxmjbJcWxkkn1s)è®²è¿‡ï¼‰ï¼Œé€‰æ‹©æœ€å¤šæ•°çš„ç­”æ¡ˆä½œä¸ºä¼ªæ ‡ç­¾ï¼Œç„¶ååŸºäºä¼ªæ ‡ç­¾å¯¹å…¶ä»–å›ç­”è¿›è¡Œå¥–åŠ±è®¡ç®—ã€‚å…·ä½“è®¡ç®—é€»è¾‘ä¸ºç­”æ¡ˆä¸€è‡´å¥–åŠ±ä¸º1ï¼Œç­”æ¡ˆä¸ä¸€è‡´å¥–åŠ±ä¸º0**
>
> 3. æœ€åå°±æ˜¯åˆ©ç”¨è¿™äº›å¥–åŠ±ã€é—®é¢˜å’Œå›ç­”**å¯¹ LLM è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæœ¬æ–‡ä¸»è¦ç”¨çš„ GRPO**ï¼Œä¹Ÿåšäº†PPOçš„å¯¹æ¯”å®éªŒ
>
> æ•´ä¸ªè¿‡ç¨‹ä¸­**å¹¶æ²¡ç”¨ç”¨åˆ° Ground Truth å»è®¡ç®—å¥–åŠ±ï¼Œæ‰€ä»¥æ˜¯åˆ©ç”¨äº†æ— æ ‡æ³¨æ•°æ®**

![](images/image-23.png)

![](images/image-24.png)

**æµç¨‹ä¼ªä»£ç **

![](images/image-25.png)

**ä¸»è¦å®éªŒç»“æœ**

> ### **ä¸ºä»€ä¹ˆTTRLæœ‰æ•ˆï¼Ÿ**
>
> #### **æ ‡ç­¾ä¼°è®¡**
>
> TTRLä¸æ ‡å‡†å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„åŒºåˆ«åœ¨äºï¼Œ**TTRLçš„å¥–åŠ±æ˜¯ä¸å‡†ç¡®çš„ï¼ˆå› ä¸ºåªæ˜¯é€šè¿‡å¤šæ•°æŠ•ç¥¨ä¼°è®¡å‡ºæ¥çš„ï¼Œç±»ä¼¼äºå¤§æ¨¡å‹è‡ªå·±å¯¹è‡ªå·±çš„ç½®ä¿¡åº¦ä¼°è®¡ï¼‰ï¼Œä½†æ˜¯ä»ç„¶æœ‰æ•ˆï¼š**
>
> * å·²æœ‰ç ”ç©¶è¡¨æ˜ï¼Œ**å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿå®¹å¿ä¸€å®šç¨‹åº¦çš„å¥–åŠ±ä¸å‡†ç¡®ï¼Œå¥–åŠ±é€šå¸¸æ˜¯æ¨¡ç³Šçš„ï¼Œä¸»è¦ä½œä¸ºæ¢ç´¢çš„æ–¹å‘æ€§ä¿¡å·ï¼Œè¿™ä½¿å¾—å¼ºåŒ–å­¦ä¹ å¯¹å¥–åŠ±å™ªå£°å…·æœ‰é²æ£’æ€§**
>
> * å…ˆå‰çš„ç ”ç©¶ä¹Ÿä»ä¼˜åŒ–çš„è§’åº¦è€ƒå¯Ÿäº†ä»€ä¹ˆæ ·çš„å¥–åŠ±æ¨¡å‹æ‰æ˜¯å¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œç»“æœè¡¨æ˜ï¼Œ**æ›´å‡†ç¡®çš„å¥–åŠ±æ¨¡å‹å¹¶ä¸ä¸€å®šå°±æ˜¯æ›´å¥½çš„ â€œè€å¸ˆâ€**ã€‚å› æ­¤ï¼Œç”±**ç­–ç•¥æ¨¡å‹è‡ªèº«ä¼°è®¡çš„å¥–åŠ±ä¿¡å·å¯èƒ½ä¼šä¸ºå­¦ä¹ æä¾›æ›´åˆé€‚çš„æŒ‡å¯¼**
>
> ### **å¥–åŠ±è®¾è®¡**
>
> æ ¹æ® TTRL çš„å¥–åŠ±è®¾è®¡æœºåˆ¶ï¼Œä¸å¤šæ•°æŠ•ç¥¨ç»“æœä¸€è‡´çš„å¥–åŠ±ä¸º1ï¼Œä¸ä¸€è‡´ä¸º0ï¼Œè¿™æ ·åœ¨æ—©æœŸè®­ç»ƒï¼ˆä¸€å¼€å§‹æ˜¯ä»base modelç›´æ¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„ï¼‰**å¤šæ•°æŠ•ç¥¨é¢„æµ‹æ ‡ç­¾è¿˜ä¸å¤Ÿå‡†ç¡®çš„æ—¶å€™ï¼Œå¥–åŠ±çš„å‡†ç¡®åº¦å´æœ‰ä¸€å®šçš„ä¿è¯**ã€‚æ¯”å¦‚ä¸‹æ–¹è¿™ä¸ªä¾‹å­ï¼Œæ ‡ç­¾é¢„æµ‹é”™äº†ï¼Œä½†æ˜¯å¤§å¤šæ•°å›ç­”çš„å¥–åŠ±æ˜¯æ­£ç¡®çš„ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿è¯äº†å‰æœŸçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æœ‰æ•ˆæ€§

![](images/image-26.png)

> ### **TTRLä»€ä¹ˆæ—¶å€™å¤±æ•ˆï¼Ÿ**
>
> #### **å¼ºåŒ–å­¦ä¹ å‚æ•°è®¾ç½®**
>
> æ¯”è¾ƒä½çš„æ¸©åº¦ç³»æ•°ï¼Œæˆ–è€…éš¾åº¦é«˜çš„é—®é¢˜è®­ç»ƒepisodeså¤ªå°‘
>
> #### **ç¼ºä¹ä»»åŠ¡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†æˆ–è€…è¯´æ¨¡å‹å¤ªå°çš„æ—¶å€™**
>
> è¿™ä¸ªå¾ˆå¥½ç†è§£ï¼Œå› ä¸º **TTRL ç›¸å½“äºæ˜¯åŸºäºè‡ªèº«çš„å†…åœ¨å¥–åŠ±ï¼ˆå³é€‰å–ç½®ä¿¡åº¦é«˜ã€å‡ºç°æ¬¡æ•°å¤šçš„å›ç­”ç»™äºˆé«˜å¥–åŠ±ï¼‰ï¼Œè¿›è¡Œå¼ºåŒ–å­¦ä¹ æ¢ç´¢**ï¼Œé‚£ä¹ˆå¦‚æœæœ¬èº«**æ¨¡å‹èƒ½åŠ›æ¯”è¾ƒå·®ï¼ˆæˆ–è€…ä»»åŠ¡éš¾åº¦è¶…å‡ºæ¨¡å‹èƒ½å¤„ç†çš„èŒƒå›´ï¼‰çš„è¯ï¼Œå¾—åˆ°çš„å¥–åŠ±æŒ‡å¯¼æ–¹å‘å°±ä¼šåç¦»ä»»åŠ¡ç›®æ ‡**ï¼Œè®­ç»ƒæ•ˆæœä¹Ÿå°±å¥½ä¸åˆ°å“ªé‡Œå»ã€‚å‰é¢çš„å®éªŒç»“æœä¹Ÿå¯ä»¥çœ‹åˆ° Qwen2.5-Math-1.5B å’Œ LLaMA-3.1-8B-Instruct çš„æ•ˆæœéƒ½ä¸å¦‚ Qwen2.5-Math-7B
>
> è¿™é‡Œä¹Ÿå¯ä»¥ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œåˆ©ç”¨ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“åœ¨è¿·å®«ç¯å¢ƒä¸­æ‰¾å®ç®±ï¼Œç°åœ¨æ™ºèƒ½ä½“èµ°åˆ°äº†è·¯å£ï¼Œå®ƒæœ‰ä¸¤ä¸ªåŠ¨ä½œé€‰æ‹©ç»§ç»­ç›´èµ°ï¼Œæˆ–è€…æ‹å¼¯ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æ˜¯è®¾å®šå¥½çš„ï¼Œå¯¹åº”å¤§æ¨¡å‹ RL çš„æ—¶å€™æœ‰æ ‡æ³¨çš„ground truth å¥–åŠ±ï¼Œç„¶åç»è¿‡å¤šæ¬¡é‡‡æ ·ä¼šå¾—åˆ°å‘ä¸‹æ‹æœ‰å¥–åŠ±ï¼Œç›´èµ°æ²¡æœ‰çš„è½¨è¿¹è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚ä½†æ˜¯ **TTRL çš„åšæ³•æ˜¯å®Œå…¨ç›¸ä¿¡æ¨¡å‹è‡ªå·±ï¼ŒåŠ å…¥ç°åœ¨çš„ agent ç­–ç•¥å¾ˆåˆå§‹åŒ–ï¼Œå®ƒå¹¶ä¸çŸ¥é“å‰é¢åªèƒ½çœ‹åˆ°å¢™è¿‡å»æ˜¯ä¸å¯èƒ½æ‰¾åˆ°å®ç®±çš„è¿™ä¸€å…ˆéªŒï¼Œæ‰€ä»¥è¿›è¡Œå¤šæ•°æŠ•ç¥¨å¾ˆæœ‰å¯èƒ½æŠ•å‡ºæ¥ç›´èµ°çš„åŠ¨ä½œæœ‰å¥–åŠ±ï¼Œå‘ä¸‹æ‹æ²¡æœ‰**

![](images/diagram-9.png)

# **3.3.14 GSPO**

> **è®ºæ–‡ï¼šGroup Sequence Policy Optimization**
>
> **é“¾æ¥: https://arxiv.org/pdf/2507.18071**
>
> **Qwen å›¢é˜Ÿ**å‡ºå“å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç®—æ³• **GSPOï¼ŒGroup Sequence Policy Optimization**
>
> å­—é¢æ„æ€å°±æ˜¯ **ç»„åºåˆ—ç­–ç•¥ä¼˜åŒ–**ï¼Œå¾ˆæ˜æ˜¾æ˜¯è¦æŠŠä¹‹å‰ PPO / GRPO çš„ token level ä¼˜åŒ–ï¼Œå˜æˆåºåˆ—ä¸ºå•ä½çš„ä¼˜åŒ–

## **RL for LLM ä¸­çš„ Action å»ºæ¨¡é—®é¢˜**

å…ˆçœ‹ä¸€ä¸‹åœ¨æ­¤ä¹‹å‰çš„ RLHF/RLVR å¸¸ç”¨ç®—æ³• PPO/GRPO åœ¨å¤§æ¨¡å‹åœºæ™¯ä¸‹å¯¹ RL çš„å»ºæ¨¡

![](images/diagram-10.png)

å¯ä»¥çœ‹åˆ° action æ˜¯ä»¥ token ä¸ºå•ä½çš„ï¼Œå³åŠ¨ä½œå®šä¹‰ä¸ºå¤§æ¨¡å‹ä»¥å†å² token åºåˆ—ä¸º condition è¾“å‡ºä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡ $$\pi(a_t|s_t)$$ï¼Œä½†æ˜¯ reward å´æ˜¯ä»¥æ•´ä¸ª sequence ä¸ºå•ä½çš„ï¼Œå³å¥–åŠ±æ˜¯èµ‹äºˆæ•´ä¸ª prompt + response å¯¹çš„ $$r(x,y)$$å…¶ä¸­ $$x$$æ˜¯ promptï¼Œ$$y$$æ˜¯ response ï¼ˆè¿™é‡Œä¸è€ƒè™‘ KL çº¦æŸâ€œå¥–åŠ±â€ï¼‰

**å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªæ˜æ˜¾çš„é—®é¢˜å°±æ˜¯ï¼Œaction å’Œ reward çš„å•ä½å¹¶ä¸åŒ¹é…**

* åœ¨ PPO ä¸­ï¼Œå…¶æ˜¯å°†ä¸­é—´ token çš„ reward å½“ä½œ0å¤„ç†çš„ï¼ˆè¿™é‡Œä¸è€ƒè™‘ KL çº¦æŸâ€œå¥–åŠ±â€ï¼‰ï¼Œå³å°†å…¶å˜æˆäº†ä¸€ä¸ª sparse reward é—®é¢˜ï¼Œå…¶ä¼˜åŠ¿å‡½æ•°ä¼°è®¡$$\hat A_t$$æ˜¯åˆ©ç”¨ value model å’Œ reward åœ¨æ¯ä¸€ä¸ª token å¤„è®¡ç®—çš„ï¼Œä¼˜åŒ–ç›®æ ‡ä¹Ÿæ˜¯ token çº§çš„ç­–ç•¥ï¼Œæ‰€ä»¥ä¼˜åŒ–ç›®æ ‡å•ä½ä¸å¥–åŠ±/ä¼˜åŠ¿å‡½æ•°æ˜¯ä¸€è‡´çš„

![](images/image-27.png)

* åœ¨ GRPO ä¸­ï¼Œåˆæ˜¯å‡½æ•°ä¼°è®¡ $$ \hat{A}_{i, t}=\hat{A}_{i}=\frac{r\left(x, y_{i}\right)-mean\left(\left\{r\left(x, y_{i}\right)\right\}_{i=1}^{G}\right)}{std\left(\left\{r\left(x, y_{i}\right)\right\}_{i=1}^{G}\right)},$$å»æ‰äº†å¯¹ value model çš„éœ€æ±‚ï¼Œä½†æ˜¯å…¶ä½¿ç”¨åºåˆ—çº§åˆ«çš„ reward ä¼°è®¡çš„ä¼˜åŠ¿å‡½æ•°ï¼Œåˆå°†å…¶å¹¿æ’­åˆ°æ¯ä¸€ä¸ª token å¤„ä¼˜åŒ– token çº§çš„ç­–ç•¥ï¼Œ**ä¼˜åŒ–ç›®æ ‡çš„å•ä½ä¸å¥–åŠ±/ä¼˜åŠ¿å‡½æ•°çš„å•ä½å¹¶ä¸åŒ¹é…**

![](images/image-28.png)

## **GRPO å­˜åœ¨çš„é—®é¢˜**

ä¸‹é¢åˆ†æä¸€ä¸‹ï¼Œä¸Šé¢çš„ä¼˜åŒ–ç›®æ ‡çš„å•ä½ä¸å¥–åŠ±/ä¼˜åŠ¿å‡½æ•°çš„å•ä½ä¸åŒ¹é…å¸¦æ¥çš„é—®é¢˜ï¼Œè¿™é‡Œçš„é—®é¢˜ä¸»è¦æ¥æº&#x4E8E;**&#x20;GRPO å¯¹é‡è¦æ€§é‡‡æ ·æƒé‡çš„è¯¯ç”¨**

å…ˆå›é¡¾ä¸€ä¸‹é‡è¦æ€§é‡‡æ ·çš„åŸç†ï¼š

![](images/image-29.png)

ä¸Šå¼ä¸­ï¼Œé€šè¿‡å¯¹ä»è¡Œä¸ºåˆ†å¸ƒ$$\pi_{beh}$$ä¸­æŠ½å–çš„æ ·æœ¬è¿›è¡Œé‡åŠ æƒï¼Œæ¥ä¼°è®¡ç›®æ ‡åˆ†å¸ƒ$$\pi_{tar}$$ä¸‹å‡½æ•°$$f$$çš„æœŸæœ›ï¼Œä¾èµ–äºå¯¹ä»è¡Œä¸ºåˆ†å¸ƒ$$\pi_{beh}$$ä¸­æŠ½å–çš„å¤šä¸ªæ ·æœ¬ï¼ˆ $$N\gg 1$$ï¼‰è¿›è¡Œå¹³å‡ï¼Œç„¶åé‡è¦æ€§æƒé‡ $$\frac{\pi_{btar}}{\pi_{beh}}$$æ‰èƒ½å¤Ÿæœ‰æ•ˆæ ¡æ­£åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜

ä½†æ˜¯ GRPO çš„ token çº§é‡è¦æ€§æƒé‡ $$w_{i, t}(\theta)=\frac{\pi_{\theta}\left(y_{i, t} | x, y_{i,<t}\right)}{\pi_{\theta_{old }}\left(y_{i, t} | x, y_{i,<t}\right)}$$ï¼Œå¯¹åº”çš„ä»è¡Œä¸ºåˆ†å¸ƒ$$\pi_{\theta_{old}}$$æŠ½å–çš„æ ·æœ¬åªæœ‰$$\pi_{\theta_{old }}\left(y_{i, t} | x, y_{i,<t}\right)$$ ä¸€ä¸ª

è¿™é‡Œè§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆåªæœ‰ä¸€ä¸ªï¼Œæˆ‘ä»¬çœ‹è™½ç„¶ GRPO ä¼˜åŒ–ç›®æ ‡å¤–é¢æœ‰æœŸæœ›ï¼Œä»$$\pi_{\theta_{old}}$$é‡‡æ ·äº†ä¸€ä¸ª group çš„å›å¤ $$\{y_i\}_{i=1}^G$$ï¼Œä½†æ˜¯æ¯ä¸€ä¸ª$$y_i$$éƒ½ä¸ä¸€æ ·ï¼Œæ‰€ä»¥åœ¨é‡è¦æ€§é‡‡æ ·è¿™é‡Œ $$z=y_{i, t} | x, y_{i,<t}$$æ˜¯åªæœ‰ä¸€ä¸ªçš„ã€‚ä»”ç»†åˆ†æä¸€ä¸‹ï¼ŒPPO ä¹Ÿæ˜¯å¦‚æ­¤ï¼ŒæœŸæœ›ä¸­é‡‡æ ·çš„$$y$$æ˜¯ä¸ä¸€æ ·çš„ï¼Œæ‰€ä»¥$$z=y_{ t} | x, y_{<t}$$ä¹Ÿæ˜¯åªæœ‰ä¸€ä¸ª

**å³ PPO/GRPO å¹¶æ²¡æœ‰å¯¹å½“å‰ token å¤„è¿›è¡Œä»è¡Œä¸ºåˆ†å¸ƒä¸­å¤šæ¬¡é‡‡æ ·çš„æ“ä½œï¼Œå°±è¿™æ ·ä½¿ç”¨ token çº§çš„é‡è¦æ€§é‡‡æ ·æƒé‡æ— æ³•å®ç°æœ‰æ•ˆçš„åˆ†å¸ƒæ ¡æ­£ï¼Œè¿˜ä¼šå› ä¸ºæ ·æœ¬æ•°ä¸è¶³å¼•å…¥é«˜æ–¹å·®å™ªå£°åˆ°è®­ç»ƒæ¢¯åº¦ä¸­ï¼Œåœ¨é•¿åºåˆ—ä¸Šä¸æ–­ç´¯ç§¯ï¼Œå¹¶ä¸”ä¼šè¢«è£å‰ªæœºåˆ¶è¿›ä¸€æ­¥æ”¾å¤§**

è¿›ä¸€æ­¥è§£é‡Š GRPO ä¸­ token çº§é‡è¦æ€§æƒé‡çš„é«˜æ–¹å·®åœ¨é•¿åºåˆ—ä»¥åŠ clip ä½œç”¨ä¸‹çš„å™ªå£°ç´¯ç§¯å’Œæ”¾å¤§ï¼Œæ ¸å¿ƒåŸå› åœ¨äº**å…¶token çº§è£å‰ªä¸é«˜æ–¹å·®æƒé‡çš„ä¸åŒ¹é…æ€§ï¼š**

* **token çº§è£å‰ªå¯¹é«˜æ–¹å·®æƒé‡çš„è¯¯å·®ç´¯ç§¯æ•ˆåº”ï¼š**

  * GRPO çš„å•ä¸ª token çš„æƒé‡æœ¬èº«å°±å› ä¸ºåŸºäºå•ä¸€æ ·æœ¬ä¼°è®¡å­˜åœ¨é«˜æ–¹å·®ï¼ˆä¾‹å¦‚ï¼ŒæŸäº› token çš„æƒé‡å¯èƒ½å¼‚å¸¸å¤§æˆ–å¼‚å¸¸å°ï¼Œåç¦»åˆç†èŒƒå›´ï¼‰ã€‚è£å‰ªæœºåˆ¶çš„åˆè¡·æ˜¯é™åˆ¶æç«¯æƒé‡ï¼ˆå¦‚ clip åˆ° $$[1-\varepsilon, 1+\varepsilon]$$ï¼‰ï¼Œä½†åœ¨é«˜æ–¹å·®åœºæ™¯ä¸‹ï¼Œå¤§é‡ token çš„æƒé‡ä¼šé¢‘ç¹è§¦å‘è£å‰ªã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªé•¿åºåˆ—ä¸­å¯èƒ½æœ‰ 10% çš„ token æƒé‡è¶…å‡ºè£å‰ªèŒƒå›´ï¼Œè¿™äº› token çš„æƒé‡è¢«å¼ºåˆ¶ â€œæ‹‰å›â€ åˆ°è£å‰ªè¾¹ç•Œï¼Œå¯¼è‡´å…¶æ¢¯åº¦è´¡çŒ®ä¸çœŸå®æƒé‡äº§ç”Ÿåå·®

  * å¯¹äºé•¿åºåˆ—ï¼Œè¿™ç§å•ä¸ª token çš„è£å‰ªè¯¯å·®ä¼šéšåºåˆ—é•¿åº¦ç´¯ç§¯ï¼šæ¯ä¸ªè¢«è£å‰ªçš„ token éƒ½ä¼šå¼•å…¥ä¸€ç‚¹è¯¯å·®ï¼Œè€Œåºåˆ—è¶Šé•¿ï¼Œè¢«è£å‰ªçš„ token æ•°é‡è¶Šå¤šï¼Œæ€»è¯¯å·®å°±è¶Šå¤§ã€‚æœ€ç»ˆï¼Œè¿™äº›ç´¯ç§¯çš„è¯¯å·®ä¼šè¢«çº³å…¥æ¢¯åº¦è®¡ç®—ï¼Œæ”¾å¤§æ•´ä½“è®­ç»ƒå™ªå£°

* **è£å‰ªæœºåˆ¶å¯¹å¼‚å¸¸æƒé‡çš„é€‰æ‹©æ€§æ”¾å¤§ï¼Œåœ¨ GRPO ä¸­ï¼Œtoken çº§æƒé‡çš„é«˜æ–¹å·®å¯èƒ½å¯¼è‡´ä¸¤ç§æç«¯æƒ…å†µï¼š**

  * éƒ¨åˆ† token çš„æƒé‡å¼‚å¸¸å¤§ï¼ˆè¿œå¤§äº $$1+\varepsilon$$ï¼‰ï¼Œè¢«è£å‰ªåå¼ºåˆ¶è®¾ä¸º $$1+\varepsilon$$

  * éƒ¨åˆ† token çš„æƒé‡å¼‚å¸¸å°ï¼ˆè¿œå°äº $$1-\varepsilon$$ï¼‰ï¼Œè¢«è£å‰ªåå¼ºåˆ¶è®¾ä¸º $$1-\varepsilon$$

  è¿™ä¸¤ç§æƒ…å†µéƒ½ä¼šæ‰­æ›²æ¢¯åº¦æ–¹å‘ï¼š

  * å¼‚å¸¸å¤§çš„æƒé‡æœ¬åº”åæ˜ è¯¥ token å¯¹ç­–ç•¥ä¼˜åŒ–çš„é‡è¦è´¡çŒ®ï¼Œä½†è£å‰ªåå…¶è´¡çŒ®è¢«ä½ä¼°

  * å¼‚å¸¸å°çš„æƒé‡æœ¬åº”åæ˜ è¯¥ token çš„è´Ÿå‘å½±å“ï¼Œä½†è£å‰ªåå…¶è´Ÿå‘è´¡çŒ®è¢«å‰Šå¼±

  æ›´å…³é”®çš„æ˜¯ï¼Œé•¿åºåˆ—ä¸­è¿™ç±»å¼‚å¸¸ token çš„æ•°é‡ä¼šéšé•¿åº¦å¢åŠ è€Œå¢å¤šï¼ˆæ¦‚ç‡ä¸Šæ›´æ˜“å‡ºç°æç«¯å€¼ï¼‰ï¼Œè£å‰ªæœºåˆ¶å¯¹å®ƒä»¬çš„æ‰­æ›²ä¹Ÿä¼šæˆæ¯”ä¾‹å¢åŠ ï¼Œæœ€ç»ˆå¯¼è‡´æ¢¯åº¦æ–¹å‘ä¸¥é‡åç¦»æœ€ä¼˜æ–¹å‘ï¼Œé«˜æ–¹å·®å™ªå£°è¢«è¿›ä¸€æ­¥æ”¾å¤§

Qwenå›¢é˜Ÿé€šè¿‡å®éªŒè§‚å¯Ÿåˆ°ï¼Œé«˜æ–¹å·®å™ªå£°ä¼šè¿›å…¥è®­ç»ƒæ¢¯åº¦ä¸­ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹å´©æºƒï¼Œè€Œè¿™ç§å´©æºƒå¾€å¾€æ˜¯ä¸å¯é€†çš„ã€‚ä¸€æ—¦å‘ç”Ÿå´©æºƒï¼Œå³ä½¿æ¢å¤åˆ°ä¹‹å‰çš„æ£€æŸ¥ç‚¹ã€ç²¾å¿ƒè°ƒæ•´è¶…å‚æ•°ï¼ˆä¾‹å¦‚è£å‰ªèŒƒå›´ï¼‰ã€å»¶é•¿ç”Ÿæˆé•¿åº¦æˆ–åˆ‡æ¢å¼ºåŒ–å­¦ä¹ çš„æŸ¥è¯¢å†…å®¹ï¼Œæ¢å¤è®­ç»ƒä¹Ÿæ— æµäºäº‹

## **GSPO æ–¹æ³•è®¾è®¡**

**é‡ç‚¹ï¼šç›´æ¥åœ¨åºåˆ—çº§ä½¿ç”¨é‡è¦æ€§æƒé‡å¹¶è¿›è¡Œä¼˜åŒ–**

![](images/image-30.png)

é‡è¦æ€§æƒé‡ç›´æ¥ä½¿ç”¨åºåˆ—çº§å•ä½ï¼Œå¯¹é½å¥–åŠ±/ä¼˜åŠ¿å‡½æ•°çš„å•ä½ï¼Œå³å°† action å»ºæ¨¡ä¸ºåºåˆ—çº§ï¼Œå¤§æ¨¡å‹ä»¥ prompt ä¸ºconditionï¼Œè¾“å‡ºæ•´ä¸ª response çš„æ¦‚ç‡$$\pi(y_i|x)$$ï¼Œå…¶ä¸­ $$\pi(y_i|x)=\prod_{t=1}^{|y_i|}\pi(y_{i,t}|x,y_{i,<t})$$ã€‚ä¸åŒçš„ç‚¹åœ¨äºï¼Œå…¶**è®¡ç®—çš„é‡è¦æ€§æƒé‡ä¸ºç­–ç•¥æ¯”å€¼åœ¨ response é•¿åº¦ä¸Šçš„å‡ ä½•å¹³å‡å€¼ï¼Œç›¸å½“äºä¸€ç§å½’ä¸€åŒ–**

**GSPOä¸GRPOçš„æ¢¯åº¦åˆ†æï¼š**

![](images/image-31.png)

![](images/image-32.png)

* GRPO çš„æ¢¯åº¦ä¸­ï¼Œä¸åŒ token æƒé‡ä¸ç­‰ï¼Œè¿™äº›ä¸ç­‰æƒé‡çš„å½±å“éšç€è®­ç»ƒçš„è¿›è¡Œä¼šä¸æ–­ç´¯ç§¯ï¼Œè¿›è€Œå¯¼è‡´ä¸å¯é¢„æµ‹çš„ç»“æœ

* **GSPO å¯¹ä¸€ä¸ª response ä¸­çš„æ‰€æœ‰ token èµ‹äºˆåŒç­‰æƒé‡ï¼Œä»è€Œæ¶ˆé™¤äº† GRPO çš„è¿™ä¸€ä¸ç¨³å®šæ€§å› ç´ **

# **3.3.15 ASPO**

> **è®ºæ–‡ï¼šASPO: Asymmetric Importance Sampling Policy Optimization**
>
> **é“¾æ¥ï¼šhttps://arxiv.org/pdf/2510.06062**
>
> å¿«æ‰‹ä¸æ¸…åè”åˆå‡ºå“ï¼Œå­—é¢æ„ä¹‰ç†è§£ä¸º **éå¯¹ç§°é‡è¦æ€§é‡‡æ ·** ç­–ç•¥ä¼˜åŒ–ï¼Œé€šè¿‡å¯¹ä¼˜åŠ¿å€¼ä¸ºæ­£çš„ token çš„é‡è¦æ€§æƒé‡è¿›è¡Œä¸Šä¸‹ç¿»è½¬ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œç¼“è§£ **ç†µåç¼©** ç°è±¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆå€¾å‘

### **å›é¡¾ï¼šä»€ä¹ˆæ˜¯é‡è¦æ€§é‡‡æ ·ï¼Ÿ**

**é‡è¦æ€§é‡‡æ ·æ˜¯ä¸€ç§ç»Ÿè®¡æŠ€å·§ï¼Œè°ƒæ•´ç›®æ ‡åˆ†å¸ƒ$$p(x)$$ä¸é‡‡æ ·åˆ†å¸ƒ$$q(x)$$ä¸ä¸€è‡´æƒ…å†µä¸‹çš„é‡‡æ ·æƒé‡ï¼Œä»è€Œå¯ä»¥ç”¨$$q(x)$$ä¸­é‡‡æ ·çš„æ•°æ®æ¥ä¼°è®¡$$p(x)$$çš„æœŸæœ›ã€‚å‡è®¾å˜é‡$$x$$æœä»åˆ†å¸ƒ$$p(x)$$ï¼Œè®¡ç®—ä»¥$$x$$ä¸ºå˜é‡çš„å‡½æ•°$$f(x)$$çš„æœŸæœ›å¦‚ä¸‹ï¼š**

**$$\mathbb{E}_{x\sim p(x)} [f(x)] = \int f(x) p(x) dx \\ =\int f(x)\frac{p(x)}{q(x)}q(x)dx =\mathbb{E}_{x\sim q(x)} [f(x)\frac{p(x)}{q(x)}]
$$**

**å…¶ä¸­$$p(x)$$éš¾ä»¥é‡‡æ ·ï¼Œ$$q(x)$$è¾ƒå®¹æ˜“é‡‡æ ·ï¼Œ$$\frac{p(x)}{q(x)}$$ä¸ºé‡è¦æ€§æƒé‡(IS)**

![](images/Figure_1.png)

### **å›é¡¾ï¼šä¸ºä»€ä¹ˆéœ€è¦é‡è¦æ€§é‡‡æ ·ï¼Ÿ**

**å¼ºåŒ–å­¦ä¹ ç®—æ³• PPO çš„ç­–ç•¥ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼š**

**$$\mathbb{E}_{a \sim \pi_{\theta_k}(\cdot \mid s)} \left[ \min \left( \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_k}(a \mid s)} A^{\pi_{\theta_k}}(s, a), \text{clip} \left( \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_k}(a \mid s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}}(s, a) \right) \right]
$$**

**å…¶ä¸­$$r_{\theta}=\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_k}(a \mid s)} $$ä¸ºé‡è¦æ€§æƒé‡ã€‚å…ˆå¿½ç•¥clipï¼Œç„¶åå»æ‰é‡è¦æ€§æƒé‡ï¼Œçœ‹ä¸€ä¸‹æœ€ç®€åŒ–çš„ PPO ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼š**

**$$\mathbb{E}_{ a \sim \pi_{\theta}(\cdot \mid s)} \left[ {\pi_{\theta}(a \mid s)} A^{\pi_{\theta}}(s, a)  \right]$$**

**è¿™ä¸ªç›®æ ‡å°±æ˜¯ä½¿ç”¨å½“å‰ç­–ç•¥$$\pi_{\theta}$$é‡‡æ ·çš„æ•°æ®æ¥æ ¹æ®ä¼˜åŠ¿å‡½æ•°æŒ‡å¯¼å½“å‰ç­–ç•¥æ›´æ–°ï¼Œæ˜¯å®Œå…¨çš„ on-policyï¼Œå³è¡Œä¸ºç­–ç•¥ï¼ˆé‡‡æ ·ç­–ç•¥ï¼‰å’Œç›®æ ‡ç­–ç•¥ï¼ˆæ›´æ–°ç­–ç•¥ï¼‰æ˜¯ä¸€æ ·çš„ã€‚å®Œå…¨çš„ on-policy å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ï¼Œå› ä¸ºåªèƒ½ç”¨å½“å‰ç­–ç•¥é‡‡é›†çš„æ ·æœ¬æ›´æ–°å½“å‰ç­–ç•¥ï¼Œæ›´æ–°å®Œæ ·æœ¬å°±å¾—æ‰”æ‰**

**åŠ å…¥é‡è¦æ€§é‡‡æ ·åçš„ç®€åŒ–ä¼˜åŒ–ç›®æ ‡ï¼š**

**$$\mathbb{E}_{ a \sim \pi_{\theta}(\cdot | s)} \left[ \pi_{\theta}(a|s)A^{\pi_{\theta}}(s, a)  \right] =\mathbb{E}_{ a \sim \pi_{\theta^k}(\cdot | s)} \left[ \frac{\pi_{\theta}(a | s)}{\pi_{\theta_k}(a | s)}  A^{\pi_{\theta^k}}(s, a)  \right]$$**

**ç›®æ ‡åˆ†å¸ƒ$$p(x)=\pi_{\theta}$$ï¼Œé‡‡æ ·åˆ†å¸ƒ $$q(x)=\pi_{\theta_k}$$ï¼Œ$$f(x)=A^{\pi}(s, a)$$**

> **æ‰€ä»¥é‡è¦æ€§é‡‡æ ·çš„ä½œç”¨ä¸»è¦æ˜¯ï¼š**
>
> * **ä¿®æ­£åˆ†å¸ƒï¼Œå‡å°‘ off-policy æ•°æ®ä¸‹çš„ä¼°è®¡åå·®ã€‚æ•°æ®æ˜¯æ—§ç­–ç•¥é‡‡çš„ï¼Œä½†æ˜¯éœ€è¦æ›´æ–°çš„æ˜¯å½“å‰ç­–ç•¥**
>
> * **æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡ã€‚ç”±äºæœ‰äº†é‡è¦æ€§æƒé‡ï¼Œæ‰€ä»¥å¯ä»¥åœ¨å¤šæ¬¡ epoch ä¸­åå¤ç”¨åŒä¸€æ‰¹ trajectoryï¼Œè€Œä¸æ˜¯æ¯ä¸€æ­¥éƒ½å¿…é¡»é‡æ–°é‡‡æ ·ä¸¥æ ¼ on-policy çš„æ•°æ®**ï¼ˆ**å…³äºPPOåˆ°åº•æ˜¯ä¸æ˜¯ on-policy å¯ä»¥å›é¡¾ 10.4.50 [ 10.4 å…«è‚¡é¢ç»ï¼ˆå«ç­”æ¡ˆï¼‰](https://kcnd4kn8i6ap.feishu.cn/wiki/S5DLwwDbOiKhXfkBgXfchVSCnle#share-R2qAdHSfto5RU6xc11DcEiIknBc)**ï¼‰

### **ä¸åŒçš„ clip ç­–ç•¥çš„æ•ˆæœ**

#### **PPO/GRPO çš„ clip ç­–ç•¥**

**GRPO å’Œ PPO çš„ clip æœºåˆ¶æ˜¯ä¸€è„‰ç›¸æ‰¿çš„ï¼Œä¸è€ƒè™‘ KL æ•£åº¦ï¼Œçœç•¥æ±‚å’Œæ±‚å¹³å‡ï¼Œç›®æ ‡å‡½æ•°å¯ä»¥ç®€åŒ–å¦‚ä¸‹å¼ï¼Œç¤ºæ„å›¾ä¹Ÿå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š**

$$J(Î¸)=\mathbb{E} \left[\min(r_{i,t}(Î¸) \hat{A}_{i,t}, \text{clip}(r_{i,t}(Î¸),1âˆ’Ïµ,1+Ïµ)\hat{A}_{i,t})\right]$$

![](images/Figure_2.png)

* **$$A>0$$ï¼Œé™åˆ¶å½“å‰ç­–ç•¥æ¦‚ç‡å·²ç»å¾ˆé«˜çš„å¥½ token æ¦‚ç‡ä¸è¦å†å¢åŠ **

* **$$A<0$$ï¼Œé™åˆ¶å½“å‰ç­–ç•¥æ¦‚ç‡å·²ç»å¾ˆä½çš„å token æ¦‚ç‡ä¸è¦å†é™ä½**

**clip ä¼šè¿æ¢¯åº¦ä¹Ÿè£å‰ªæ‰ï¼Œå¯¼è‡´åœ¨åŸºåº§æ¨¡å‹é‡Œæœ¬æ¥å°±æ¦‚ç‡å¾ˆä½çš„åæ€ç±» tokenï¼ˆ â€œAhaâ€ç­‰ï¼‰ï¼ŒRL ç»™è¿™ç±» token é«˜å¥–åŠ±åï¼Œæ›´æ–°å‡ æ­¥åä¼šæŠŠå®ƒä»¬çš„æ¦‚ç‡æ‹‰å¾—å¾ˆé«˜ï¼Œè§¦å‘ä¸Šè£å‰ªï¼Œå…³é”®tokenæ¢¯åº¦éƒ½è¢«æ‰”æ‰æµªè´¹äº†**

* **å½“$$r(\theta)$$å¤„äºæ–œçº¿éƒ¨åˆ†ï¼Œ$$âˆ‡_Î¸J=\frac{âˆ‚r(\theta)A}{âˆ‚\theta}=A\cdotâˆ‡_Î¸r(Î¸)$$**

* **å½“$$r(\theta)$$å¤„äºå¹³ç›´éƒ¨åˆ†ï¼Œ$$âˆ‡_Î¸J=\frac{âˆ‚(1Â±\epsilon)A}{âˆ‚\theta}=0\cdotâˆ‡_Î¸r(Î¸)=0$$**

### **ä¸åŒçš„ clip ç­–ç•¥çš„æ•ˆæœ**

#### **Dual-clip PPO çš„ clip ç­–ç•¥**

**PPO-clip ç›¸å½“äºåªåœ¨å¾€æå‡ç›®æ ‡çš„æ–¹å‘ä¸Šè£å‰ªï¼Œå³å¥½åŠ¨ä½œä¸èƒ½è¢«æ¨å¾—è¿‡çŒ›ï¼ŒååŠ¨ä½œä¸èƒ½è¢«å‹å¾—è¿‡çŒ›ã€‚è¿™æ ·è¿˜æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼Œå¦‚æœæ˜¯å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œè½¨è¿¹ä»ä¸åŒæ—¶é—´ã€ç‰ˆæœ¬çš„ç­–ç•¥é‡‡æ ·å›æ¥ï¼Œéå¸¸çš„ off**

**äºæ˜¯ä¼šå‡ºç°è¿™æ ·çš„æ ·æœ¬ï¼š**

* **æ—§ç­–ç•¥æ¦‚ç‡å¾ˆå°ï¼Œä½†åŒæ—¶æ–°ç­–ç•¥å¾ˆå–œæ¬¢ï¼Œæ¦‚ç‡å¾ˆå¤§ï¼ŒISå€¼å¾ˆé«˜**

* **ä½†æ˜¯æ ·æœ¬ä¼˜åŠ¿ä¸ºè´Ÿï¼Œé‚£ä¹ˆæ ‡å‡† clip ä¸‹$$rA$$æ˜¯å¾ˆå¤§çš„è´Ÿæ•°ï¼Œå¯¹åº”çš„æ¢¯åº¦ç³»æ•°ä¹Ÿå·¨å¤§ï¼Œæ–¹å·®ç‚¸è£‚ï¼Œå¯¼è‡´è®­ç»ƒæä¸ç¨³å®š**

**æ‰€ä»¥ Dual-clip PPO å¯¹äºæ­¤æƒ…å†µå¤šåŠ äº†ä¸€ä¸ªè£å‰ªè¾¹ç•Œï¼Œå…¬å¼å¦‚ä¸‹ï¼š**

$$\mathbb{E} \left[ \max \left( \underbrace{\min(r_t A_t, \text{clip}(r_t, 1 - \varepsilon, 1 + \varepsilon) A_t)}_{\text{æ ‡å‡† PPO éƒ¨åˆ†}}, c A_t \right) \right], \quad A_t < 0$$

![](images/image-33.png)

### **ä¸åŒçš„ clip ç­–ç•¥çš„æ•ˆæœ**

#### **CISPO çš„ soft clip ç­–ç•¥**

**å‰é¢æåˆ° PPO/GRPO çš„ clip ä¼šè¿åŒæ¢¯åº¦ä¸€èµ·è£å‰ªæ‰ï¼Œå¯¼è‡´å…³é”® tokenæ¢¯åº¦ä¸å‚ä¸åç»­å¤š epoch çš„ off-policy è®­ç»ƒï¼Œæ ·æœ¬æ•ˆç‡é™ä½**

**Minimax-M1 ä¸­æå‡ºçš„ CISPO æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§ soft clip ç­–ç•¥æ¥ä¿ç•™è¢«è£å‰ªçš„æ ·æœ¬çš„æ¢¯åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿç»§ç»­ç»™è®­ç»ƒè¿‡ç¨‹è´¡çŒ®**

**CISPO åšäº†å‡ ç‚¹ï¼Œé¦–å…ˆç›®æ ‡å‡½æ•°ä» PPO å½¢å¼æ”¹å› REINFORCEï¼š**

$$J_{\text{REINFORCE}}(\theta) = \mathbb{E}\left[ \text{sg}(r_{i,t}(\theta)) \hat{A}_{i,t} \log \pi_\theta(o_{i,t}|\cdot) \right]$$

* **ä½¿ç”¨$$\text{sg}(\cdot)$$ï¼ˆstop gradientï¼‰æ“ä½œæŠŠ IS çœ‹ä½œå¸¸æ•°æƒé‡**

* **ä½¿ç”¨ REINFORCE çš„ç›®æ ‡ï¼Œå¯¹å…¶æ±‚å¯¼æ¢¯åº¦ç”±&#x20;**$$\log\pi_{\theta}$$é¡¹æä¾›

* **ä¸ç”¨$$\min$$æ“ä½œï¼Œç›´æ¥å¯¹æƒé‡æœ¬èº«clip**

$$r=\text{clip}(r,1âˆ’Ïµ_{low},1+Ïµ_{high})$$

**æ‰€ä»¥å…¶æ¢¯åº¦ä¸ºï¼š**

$$\nabla_\theta J_{\text{CISPO}} = \mathbb{E}\left[ \hat{r}_{i,t} \hat{A}_{i,t} \nabla_\theta \log \pi_\theta(\cdot) \right]$$

* **$$r$$å°±ç®—è¢« clip äº†ï¼Œä¹Ÿåªæ˜¯å½“ä½œä¸€ä¸ªæœ‰é™çš„ç³»æ•°ï¼Œä¸ä¼šå˜æˆ 0**

* **ä¸å­˜åœ¨ r è¶…ç•Œå¯¼è‡´ loss å¯¹$$Î¸ $$æ²¡æœ‰ä¾èµ–ï¼Œæ¢¯åº¦ä¸º 0 çš„åŒºåŸŸ**

* **CISPO æ˜¯åªé™åˆ¶æƒé‡å¤§å°ï¼Œä¸æŠŠ token ä»æ¢¯åº¦é‡Œè¸¢å‡ºå»ï¼Œæ‰€ä»¥å«soft clip è½¯è£å‰ª**

### **ASPO æå‡ºçš„é—®é¢˜**

**å®é™… PPO åº”ç”¨çš„æ—¶å€™å¤§å¤šä¼šæŠŠ A<0 çš„ dual-clip åŠ ä¸Šï¼Œå¦‚ä¸‹å›¾ï¼š**

* **å›¾(a) token å½“å‰æ¦‚ç‡ä½çš„æ—¶å€™ï¼Œé‡è¦æ€§æƒé‡ä¹Ÿä½ï¼Œè®­ç»ƒä¸å……åˆ†**

* **å›¾(c)å¯¹äº A<0 çš„æ ·æœ¬ï¼Œæƒé‡åˆ†å¸ƒç¬¦åˆé¢„æœŸï¼Œä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’é€æ¸å‡å°ï¼Œå³$$r$$å¢å¤§æ—¶ï¼Œå¦‚æœç°åœ¨çš„ç­–ç•¥å·²ç»æŠŠä¸€ä¸ªâ€œååŠ¨ä½œâ€æ¦‚ç‡æé«˜äº†ï¼ˆr>1ï¼‰ï¼Œå°±ä¼šç»™å®ƒæ›´å¤§çš„è´Ÿæ¢¯åº¦ï¼Œå¼ºçƒˆå¾€å›æ‹‰ï¼›å¦‚æœç°åœ¨æ¦‚ç‡ä¸é«˜ï¼ˆr æ¥è¿‘ 1 æˆ–æ›´å°ï¼‰ï¼Œè´Ÿæ¢¯åº¦æ¯”è¾ƒæ¸©å’Œ**

* **å›¾(b)å¯¹äº A>0 çš„æ ·æœ¬ï¼Œå·¦ä¸Šçš„ token åœ¨å½“å‰ç­–ç•¥ä¸‹çš„æ¦‚ç‡å·²ç»è¿œå¤§äºåœ¨æ—§ç­–ç•¥ä¸‹çš„æ¦‚ç‡ï¼Œå´è¢«èµ‹äºˆäº†æ›´å¤§çš„æƒé‡ï¼›è€Œå³ä¸‹è§’åŒºåŸŸçš„ token å…·æœ‰è¾ƒä½çš„å½“å‰æ¦‚ç‡ï¼Œå´è¢«åˆ†é…äº†æå°çš„æƒé‡**

![](images/image-34.png)

**å¯¹äºä¼˜åŠ¿ä¸ºæ­£çš„ tokenï¼Œå­˜åœ¨æƒé‡åˆ†é…åçš„é—®é¢˜ï¼š**

* **å·²ç»å¾ˆâ€œè‡ªä¿¡â€çš„æ­£ä¼˜åŠ¿ token å¾—åˆ°æ›´å¤§çš„æ­£å‘æ›´æ–°ï¼Œå½¢æˆè‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€ç†µå´©æºƒå’Œè¾“å‡ºé‡å¤å¢å¤šç­‰ç°è±¡**

* **æ­£ä¼˜åŠ¿ä½†æ¦‚ç‡å¾ˆä½çš„ tokenè¢«èµ‹äºˆæå°çš„æƒé‡è€Œè¢«è¿›ä¸€æ­¥æŠ‘åˆ¶ï¼ˆæ ·æœ¬é‡æœ‰é™æ—¶å‡ ä¹æ²¡æœ‰æ¢¯åº¦ï¼‰ï¼Œå¯¼è‡´æ›´æ–°ä¿¡å·å¾®å¼±ï¼Œå­¦ä¹ ä¸å……åˆ†**

### **ASPO çš„ç¿»è½¬æƒé‡åŠå®ç°ç»†èŠ‚**

**åŸºäºå‰é¢çš„åˆ†æï¼ŒASPO æå‡ºéå¯¹ç§°é‡è¦æ€§é‡‡æ ·ï¼ˆAISï¼‰æ–¹æ³•ï¼Œå¯¹æ­£æ ·æœ¬çš„æƒé‡è¿›è¡Œåè½¬ï¼Œä½¿å…¶æ›´æ–°è¡Œä¸ºä¸è´Ÿæ ·æœ¬ä¿æŒä¸€è‡´ï¼Œå½“å‰ç­–ç•¥æ¦‚ç‡ä½äºæ—§ç­–ç•¥æ¦‚ç‡çš„ token åº”è¢«èµ‹äºˆæ›´é«˜çš„å­¦ä¹ æƒé‡ï¼Œè€Œæ¦‚ç‡è¾ƒé«˜çš„æ ‡è®°åˆ™åº”è·å¾—è¾ƒä½çš„æƒé‡**

**å…·ä½“å®ç°åˆ†ä¸‰æ­¥ï¼š**

1. **å¯¹$$\ r_t^i(\theta) < 1 - \varepsilon_{\text{low}}\ (\hat{A}_t^i < 0)\ \text{or}\ \ r_t^i(\theta) > 1 - \varepsilon_{\text{high}}\ (\hat{A}_t^i > 0)$$çš„ tokenï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢å›¾éé˜´å½±éƒ¨åˆ†ï¼Œç›´æ¥ mask æ‰ï¼Œhard clip**

2. **å¯¹ä¼˜åŠ¿ä¸ºæ­£çš„ token ç¿»è½¬ IS æƒé‡**

**$$\hat{r}_t^i = \frac{\textcolor{red}{\pi_{\theta_{\text{old}}}(o_t^i \mid q, o_{<t}^i)}\pi_{\theta}(o_t^i \mid q, o_{<t}^i)}{\textcolor{red}{\text{sg}(\pi_{\theta}^2(o_t^i \mid q, o_{<t}^i))}}$$**

* **å¯¹ä¼˜åŠ¿ä¸ºè´Ÿçš„ token é‡‡ç”¨æ­£å¸¸çš„ dual-clipï¼Œå¯¹ä¼˜åŠ¿ä¸ºæ­£çš„tokenï¼Œç”±äºæƒé‡ç¿»è½¬ï¼Œæ‰€ä»¥æç«¯æƒ…å†µä»å·¦ä¸Šå˜åˆ°å³ä¸‹ï¼Œæ‰€ä»¥éœ€è¦å¯¹ä¼˜åŠ¿ä¸ºæ­£çš„ token ä¹Ÿè¿›è¡Œ dual-clipï¼Œç„¶å dual-clip éƒ½ç”¨ CISPO çš„ soft clip å®ç°**

### **ASPO çš„å®éªŒç»“æœ**

![](images/image-35.png)

![](images/image-36.png)



![](images/Figure_3.png)

**ASPOçš„è£å‰ªç¤ºæ„å›¾**
