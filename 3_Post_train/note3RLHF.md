![](./note3RLHF.assets/diagram.png)

LLM可以视为上图, 给定一个prompt，大模型会**在 $$t$$时刻生成一个token，然后下一个时刻根据prompt+上一时刻的token再去生成下一个token**，进行自回归的生成，所以可以定义**强化学习**中的各个概念为：

* **动作$$a_t$$：**&#x751F;成的 `token`，动作空间就是整个词表，动作空间大小就是词表大小$$|V|$$

* **策略$$\pi(a_t|s_t)$$：**&#x6839;据当前状态$$s_t$$生成动作`token`$$a_t$$的概率&#x20;

* **状态$$s_t$$：**&#x4E0A;文以及$$t$$时刻前生成的**所有 token concat的 token 序列，**&#x521D;始状态 $$s_0$$就是prompt的token序列

* **状态转移：**&#x8FD9;里的强化学习状态转移是确定性的，定义为**当前状态和动作的concat的token序列为下一个状态**，即$$s_{t+1}=[s_t,a_t]$$

* **奖励$$r_t$$和价值 $$V_t$$：**&#x8FD9;里的定义就是一般强化学习的定义，**即时奖励以及状态价值函数**

## [RLHF](https://arxiv.org/pdf/1706.03741)

### 问题

* 强化学习在许多任务中面临**目标复杂、难以定义奖励函数**的问题，导致**难以将人类实际目标传达给智能体**

* 不正确的、有偏的奖励函数会导致**智能体过分利用(exploit)奖励函数**，产生**reward hacking**问题，即**实际学到的行为与人类期望不符合，甚至有害**

* 奖励函数的设计工程需要**大量的专业人士的精力**

* 现有方法如**逆强化学习和模仿学习在处理复杂行为时存在局限性，直接使用人类反馈作为奖励函数成本过高**

### 目标

用于**解决没有明确定义奖励函数的强化学习**问题，需要满足以下几点：

* 能够解决那些人类**只能识别期望行为**，但**不一定能提供demonstration的任务**

* 允许**非专家用户对智能体进行教导**

* 能够**扩展到大型问题**

* 在**用户反馈方面经济高效

### 方法

将**奖励函数与人类偏好进行拟合**，同时**用RL算法训练一个策略来优化当前预测的奖励函数**。**给人类提供两个智能体行为轨迹的片段(一般来说是视频、动图)，让人给出自己的偏好标签(就是那个片段更好)，而不是提供绝对数值分数**。

![](./note3RLHF.assets/image.png)



* **对比标签：**&#x5BF9;于智能体轨迹片段 $$\sigma^1$$和 $$\sigma^2$$来说，下面的式子表示 $$\sigma^1$$比 $$\sigma^2$$更被人偏好，得到的标签 $$y$$也可以表示如下，0.5代表同等偏好程度。**&#x20;$$s,a$$分别表示智能体的观测/状态和动作**

**&#x20;**$$\sigma^1\succ\sigma^2=
\left(\left(s_{0}^{1}, a_{0}^{1}\right), \ldots,\left(s_{k - 1}^{1}, a_{k - 1}^{1}\right)\right) \succ\left(\left(s_{0}^{2}, a_{0}^{2}\right), \ldots,\left(s_{k - 1}^{2}, a_{k - 1}^{2}\right)\right) 
\\ \\
y = \{0,1,0.5\} \text{  if  } \{\sigma^1\succ\sigma^2, \sigma^2\succ\sigma^1, \sigma^1=\sigma^2\}$$

* **偏好建模：**&#x7531;于RLHF的一个目标是将**奖励函数与人类偏好进行拟合，**&#x5C31;是**利用人类的比较偏好标签来学出一个reward model**，那就涉及到了奖励函数和偏好之间的关联问题，这里给出的方法是，将奖励函数视为解释人类判断的潜在因素，并**假设人类偏好一个片段的概率与潜在奖励在该片段长度上的总和呈指数相关**，基于 **Bradley-Terry 模型**，可以给出**人类偏好片段 $$\sigma^1$$超过 $$\sigma^2$$的概率**：

$$\hat{P}[\sigma^{1} \succ \sigma^{2}] = \frac{\exp \sum \hat{r}(s_{t}^{1}, a_{t}^{1})}{\exp \sum \hat{r}(s_{t}^{1}, a_{t}^{1}) + \exp \sum \hat{r}(s_{t}^{2}, a_{t}^{2})}$$

* **奖励学习：**&#x5F97;到这个偏好建模以及收集到的人类偏好标签之后，就可以简单的使用**二分类的思路来隐式的学习我们的奖励函数**了，损失函数是分类常用的**交叉熵，**&#x7136;后利用这个loss训练优化得到最后的符合人类偏好的奖励函数

 $$ \mathrm{loss}(\hat{r}) = - \mathbb{E}_{(\sigma^{1}, \sigma^{2}, y) \in \mathcal{D}} \left[y(\sigma^1\succ\sigma^2) \log \hat{P}[\sigma^{1} \succ \sigma^{2}] + y(\sigma^2\succ\sigma^1) \log \hat{P}[\sigma^{2} \succ \sigma^{1}]\right]$$

  如果将正样本（被偏好）和负样本（不被偏好）记为 $$\sigma^+,\sigma^-$$，则上述loss可以写成：


 $$ \mathrm{loss}(\hat{r}) = - \mathbb{E}_{(\sigma^{+}, \sigma^{-}, y) \in \mathcal{D}} \left[ \log \hat{P}[\sigma^{+} \succ \sigma^{-}] \right] = \\ - \mathbb{E}_{(\sigma^{+}, \sigma^{-}, y) \in \mathcal{D}} \left[ \log \frac{\exp \sum \hat{r}(s_{t}^{+}, a_{t}^{+})}{\exp \sum \hat{r}(s_{t}^{+}, a_{t}^{+}) + \exp \sum \hat{r}(s_{t}^{-}, a_{t}^{-})}\right]$$

* **策略学习：**&#x5F97;到奖励函数之后就可以应用任何一个强化学习算法去**最大化奖励用来产出相应的策略**了

* **在线学习：**&#x8FD9;篇文章提出的方法是在线RLHF(Online RLHF)，就是**奖励函数和策略学习是交替同时进行的**，伴随着智能体不断的和环境交互产生新的轨迹数据用来给人类打反馈标签

## 研究**总结**

* **算法原理：**算法通过将奖励函数与人类偏好进行拟合，**使智能体的行为朝着符合人类期望的方向发展**。在训练过程中，同时优化策略以最大化预测的奖励，从而在没有明确奖励函数的情况下，让智能体学会做出符合人类偏好的决策

* **反馈方式优势：**

  * **易于提供：**&#x76F8;比提供绝对数值分数，**人类更容易对智能体轨迹片段进行比较**，降低了反馈的难度，使得非专家用户也能更轻松地参与到智能体的训练过程中

  * **信息丰富：**&#x667A;能体轨迹片段包含了一定的行为序列信息，**比单个状态更能反映智能体的行为特点和趋势**，因此在学习人类偏好方面更有帮助，能够**为奖励函数的拟合提供更有价值的信息**

  * **在线反馈的好处：**&#x5728;线收集反馈意味着系统可以**实时获取人类的偏好信息**，并**根据新的反馈及时调整策略和奖励函数**。这样可以**避免系统过度依赖之前学习到的奖励函数**，**防止因奖励函数的不准确性或局限性而导致的不良行为**，从而持续提高系统的性能，使其更好地适应复杂多变的任务环境；缺点就是**实时收集人类偏好标签成本很高**，所以之后在LLM中应用的时候，很多工作都在研究**自动化偏好标签，比如RLAIF，利用大模型代替人类给偏好**



[LLM对齐中的RLHF+PPO](https://github.com/wlll123456/study_rlhf)

这是使用gpt2进行RLHF的教程,具体[代码部分]()