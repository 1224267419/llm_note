# 强化学习

这部分内容主要参考 [动手学强化学习](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)，可以通过该书详细入门强化学习，这里做一些**简要但是也比较全面**的介绍。**主要是想通过对强化学习有一定了解之后，看大模型强化学习会得心应手很多**

## 1. 强化学习问题、流程以及独特性

### **强化学习解决的问题**

在机器学习领域，有一类重要的任务和人生选择很相似，即序贯决策（sequential decision making）任务。决策和预测任务不同，**决策往往会带来“后果”，因此决策者需要为未来负责**，在未来的时间点做出进一步的决策**。实现序贯决策的机器学习方法就是强化学习（reinforcement learning），**预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变

强化学习（Reinforcement Learning，RL）是一种机器学习方法，用于解决需要在一定环境下**通过与环境交互来学习最优行为策略的问题**。其核心思想是**通过试错（Trial and Error）和奖励机制来指导智能体（Agent）学习如何在不同情境下采取行动，以最大化长期累积奖励**

**应用场景：**控制问题、游戏、资源管理优化、金融风险控制、推荐算法

### 强化学习流程

![](note2.assets/image.png)

智能体和环境之间具体的交互方式如图所示。在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移，智能体则在下一轮交互中感知到新的环境状态直到到达终止状态

智能体在这个过程中学习，它的最终目标是：**找到一个策略，这个策略根据当前观测到的环境状态和奖励反馈，来选择最佳的动作**

**强化学习的独特性**

对于一般的有监督学习任务，**目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数**。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差，用简要的公式可以概括为：

$$最优模型 = \arg\min_{模型} \mathbb{E}_{(特征, 标签)\sim 数据分布}[损失函数(标签, 模型(特征))]$$

相比之下，强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。**策略的价值可以等价转换成奖励函数在策略的占用度量（这里简单理解策略的占用度量是策略的分布即可）上的期望**，即：

$$最优策略 = \arg\max_{策略} \mathbb{E}_{(状态, 动作)\sim 策略的占用度量}[奖励函数(状态, 动作)]$$

- 有监督学习和强化学习的优化目标相似，即都是在**优化某个数据分布下的一个分数值的期望**。
- 二者优化的途径是不同的，**有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变**；**强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变**

综上所述，一般有监督学习和强化学习的范式之间的区别为：

- 一般的**有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小**
- **强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望**

#### **强化学习与有监督学习的其他区别**

强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器**不但可以感知周围的环境信息，还可以通过做决策来直接改变环境**，而不只是给出一些预测信号

> 强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号

##### **数据类型与来源**

- 有监督学习：依赖于**标注好的数据集**，每个数据**样本**都**带有明确的标签**（目标值）
- **强化学习：不依赖预先标注的数据集，而是通过与环境交互产生数据**。智能体在每一步行动后会得到环境的反馈（奖励或惩罚），这个反馈用于指导学习

##### **学习方式**

- **有监督学习：**基于**静态数据集进行训练，学习过程通常是一次性的，即通过一个固定的数据集训练完模型**
- **强化学习：**基于动态数据进行训练，学习过程是持续的，**通过与环境不断交互、试错来更新策略****

##### **反馈机制**

- **有监督学习：**每个训练样本都有明确的标签，模型可以直接计算误差
- **强化学习：**没有明确的标签，模型通过从环境中获得的奖励信号来评估行动的好坏。奖励通常是延迟的，**不是每个行动都能立即得到反馈**

后面再补充,现在先放一下